[
  {
    "title": "PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46373",
    "abstract": "Multimodal Retrieval-Augmented Generation (MuRAG) systems have been widely applied to Large Vision-Language Models (LVLMs) to enhance their generating ability. However, the reliance on external multimodal knowledge databases renders MuRAG systems vulnerable to malicious poisoning attacks. In this paper, we introduce PoisonedEye, the first knowledge poisoning attack designed for MuRAG systems. Our attack achieves manipulating response of the MuRAG system for the target query by injecting only one poison sample into the knowledge database. To construct the poison sample, we follow two key properties for the retrieval and generation process and identify the solution by satisfying these properties. Besides, we also introduce class query targeted poisoning attack, a more generalized strategy that extends the poisoning effect to an entire class of target queries. Extensive experiments on multiple query datasets, retrievers, and LVLMs demonstrate that our attack is highly effective on comprising MuRAG systems.",
    "is_llm_safety": true
  },
  {
    "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM’s Reasoning Capability",
    "url": "https://icml.cc/virtual/2025/poster/44503",
    "abstract": "Mathematical reasoning tasks pose significant challenges for large language models (LLMs) because they require precise logical deduction and sequence analysis. In this work, we introduce the concept of critical tokens -- elements within reasoning trajectories that significantly influence incorrect outcomes. We present a novel framework for identifying these tokens through rollout sampling and demonstrate their substantial divergence from traditional error tokens. Through extensive experiments on datasets such as GSM8K and MATH500, we show that identifying and replacing critical tokens significantly improves model accuracy. We propose an efficient methodology for pinpointing these tokens in large-scale datasets using contrastive estimation and extend this framework to enhance model training processes with direct preference optimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with the widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate the effectiveness of the proposed approach, cDPO. Our results underscore the potential of leveraging critical tokens to reduce errors in reasoning tasks, advancing the development of AI systems capable of robust logical deduction.",
    "is_llm_safety": true
  },
  {
    "title": "Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning",
    "url": "https://icml.cc/virtual/2025/poster/45159",
    "abstract": "Safety alignment is crucial for Large Language Models (LLMs) to resist malicious instructions but often results in over-refusals, where benign prompts are unnecessarily rejected, impairing user experience and model utility. To this end, we introduce ACTOR (Activation-Based Training for Over-Refusal Reduction), a robust and compute- and-data efficient training framework that mini- mizes over-refusals by utilizing internal activation patterns from diverse queries. ACTOR precisely identifies and adjusts the activation components that trigger refusals, providing stronger control over the refusal mechanism. By fine-tuning only a single model layer, ACTOR effectively reduces over-refusals across multiple benchmarks while maintaining the model’s ability to handle harmful queries and preserving overall utility.",
    "is_llm_safety": true
  },
  {
    "title": "An Efficient Private GPT Never Autoregressively Decodes",
    "url": "https://icml.cc/virtual/2025/poster/45418",
    "abstract": "The wide deployment of the generative pre-trained transformer (GPT) has raised privacy concerns for both clients and servers. While cryptographic primitives can be employed for secure GPT inference to protect the privacy of both parties, they introduce considerable performance overhead. To accelerate secure inference, this study proposes a public decoding and secure verification approach that utilizes public GPT models, motivated by the observation that securely decoding one and multiple tokens takes a similar latency. The client uses the public model to generate a set of tokens, which are then securely verified by the private model for acceptance. The efficiency of our approach depends on the acceptance ratio of tokens proposed by the public model, which we improve from two aspects: (1) a private sampling protocol optimized for cryptographic primitives and (2) model alignment using knowledge distillation. Our approach improves the efficiency of secure decoding while maintaining the same level of privacy and generation quality as standard secure decoding. Experiments demonstrate a $2.1\\times \\sim 6.0\\times$ speedup compared to standard decoding across three pairs of public-private models and different network conditions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.15252v1",
    "arxiv_id": "2505.15252v1",
    "arxiv_title": "An Efficient Private GPT Never Autoregressively Decodes",
    "arxiv_authors": [
      "Zhengyi Li",
      "Yue Guan",
      "Kang Yang",
      "Yu Feng",
      "Ning Liu",
      "Yu Yu",
      "Jingwen Leng",
      "Minyi Guo"
    ],
    "arxiv_published": "2025-05-21T08:28:56+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.15252v1",
        "arxiv_url": "http://arxiv.org/abs/2505.15252v1",
        "arxiv_title": "An Efficient Private GPT Never Autoregressively Decodes",
        "authors": [
          "Zhengyi Li",
          "Yue Guan",
          "Kang Yang",
          "Yu Feng",
          "Ning Liu",
          "Yu Yu",
          "Jingwen Leng",
          "Minyi Guo"
        ],
        "published": "2025-05-21T08:28:56+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "ROPO: Robust Preference Optimization for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46424",
    "abstract": "The prevalent noise in the preference data unavoidably poses significant challenges to the preference alignment of large language models (LLMs). Existing efforts for this problem either marginally alleviate the impact of noise without noise reduction, or rely on external LLMs that incur substantial computational costs. To address these challenges, we propose RObust Preference Optimization (ROPO), an iterative alignment approach that integrates noise-tolerance and noise filtering without the aid of external models. Specifically, ROPO first formulates the training process with adaptive noise reduction as an optimization problem, which can be efficiently solved in an iterative paradigm. Then, to equip this solving process with noise-tolerance and noise-identification capabilities, we derive a robust loss that suppresses the gradients from samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is key to the noise-tolerance and effective filtering of noisy samples. The derived loss further inspires a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Extensive experiments on several widely-used datasets and model architectures demonstrate that ROPO significantly outperforms all baselines under four practical noise settings and the random symmetric noise, with its advantage increasing as the noise rate increases.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.04102v2",
    "arxiv_id": "2404.04102v2",
    "arxiv_title": "ROPO: Robust Preference Optimization for Large Language Models",
    "arxiv_authors": [
      "Xize Liang",
      "Chao Chen",
      "Shuang Qiu",
      "Jie Wang",
      "Yue Wu",
      "Zhihang Fu",
      "Zhihao Shi",
      "Feng Wu",
      "Jieping Ye"
    ],
    "arxiv_published": "2024-04-05T13:58:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.04102v2",
        "arxiv_url": "http://arxiv.org/abs/2404.04102v2",
        "arxiv_title": "ROPO: Robust Preference Optimization for Large Language Models",
        "authors": [
          "Xize Liang",
          "Chao Chen",
          "Shuang Qiu",
          "Jie Wang",
          "Yue Wu",
          "Zhihang Fu",
          "Zhihao Shi",
          "Feng Wu",
          "Jieping Ye"
        ],
        "published": "2024-04-05T13:58:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "url": "https://icml.cc/virtual/2025/poster/44897",
    "abstract": "Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) introduce additional challenges. For instance, diverse preferences complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. These RL challenges create confusion about whether the probability of an action for a given state should be increased or decreased, similar to the noise in labels for classification tasks. In this work, we focus on RL algorithms that share learning difficulties with cross-entropy loss, especially for low-probability predictions. To enhance stability, we adapt reverse cross-entropy (RCE) from supervised learning for noisy data, defining a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO). Notably, SPPO shows strong performance across different hyperparameters. Furthermore, we validate the symmetric RL loss in the RLHF framework using PPO for natural language processing tasks such as IMDB positive sentiment and TL;DR summarization.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.17618v2",
    "arxiv_id": "2405.17618v2",
    "arxiv_title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "arxiv_authors": [
      "Ju-Seung Byun",
      "Andrew Perrault"
    ],
    "arxiv_published": "2024-05-27T19:28:33+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.17618v2",
        "arxiv_url": "http://arxiv.org/abs/2405.17618v2",
        "arxiv_title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
        "authors": [
          "Ju-Seung Byun",
          "Andrew Perrault"
        ],
        "published": "2024-05-27T19:28:33+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Societal Impacts Research Requires Usage-Based Benchmark Development for Creative Composition Tasks",
    "url": "https://icml.cc/virtual/2025/poster/40162",
    "abstract": "Foundation models that are capable of automating cognitive tasks represent a pivotal technological shift, yet their societal implications remain unclear. These systems promise exciting advances, yet they also risk flooding our information ecosystem with formulaic, homogeneous, and potentially misleading synthetic content. Developing benchmarks grounded in real use cases where these risks are most significant is therefore critical. Through a thematic analysis using 2 million language model user prompts, we identify creative composition tasks as a prevalent usage category where users seek help with personal tasks that require everyday creativity. Our fine-grained analysis identifies mismatches between current benchmarks and usage patterns among these tasks. Crucially, we argue that the same use cases that currently lack thorough evaluations can lead to negative downstream impacts. This position paper argues that benchmarks focused on creative composition tasks is a necessary step towards understanding the societal harms of AI-generated content. We call for greater transparency in usage patterns to inform the development of new benchmarks that can effectively measure both the progress and the impacts of models with creative capabilities.",
    "is_llm_safety": true
  },
  {
    "title": "PoisonBench: Assessing Large Language Model Vulnerability to Poisoned Preference Data",
    "url": "https://icml.cc/virtual/2025/poster/46610",
    "abstract": "Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 22 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not always enhance resilience against poisoning attacks and the influence on model resilience varies among different model suites. (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.",
    "is_llm_safety": true
  },
  {
    "title": "A Three-Branch Checks-and-Balances Framework for Context-Aware Ethical Alignment of Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46461",
    "abstract": "This paper introduces a three-branch checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by the idea of collaborative intelligence. It implements three independent yet interacting components: LLMs as the executive branch for knowledge generation, DIKE (the goddess of justice) as the legislative branch establishing ethical guardrails, and ERIS (the goddess of discord) as the judicial branch for contextual interpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse cultural contexts while upholding consistent ethical principles. This architecture addresses limitations of reinforcement learning with human feedback (RLHF) by providing interpretable, adaptable, and culturally-aware ethical reasoning. Through self-supervised learning and adversarial testing, our framework demonstrates how emotional modeling can guide linguistic behaviors toward ethical outcomes while preserving independence across knowledge generation, ethical oversight, and contextual interpretation.",
    "is_llm_safety": true
  },
  {
    "title": "A General Framework for Inference-time Scaling and Steering of Diffusion Models",
    "url": "https://icml.cc/virtual/2025/poster/45673",
    "abstract": "Diffusion models are a flexible class of generative models. However, generating samples with user-specified properties remains a challenge. One solution is to fine-tune models to maximize reward functions which capture desired properties. However, fine-tuning can be expensive. In this work, we propose Feynman-Kac (FK) steering, a framework for inference-time steering diffusion models with reward functions. FK steering works by generating multiple trajectories, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are chosen such that a high score indicates the particle will yield a high-reward sample. We explore various choices of potentials, rewards, and samplers. Steering text-to-image models with a human preference reward, we find that FK steering outperforms fine-tuned models with just 2 particles. Moreover, FK steering a 0.8B parameter model outperforms a 2.6B model, achieving state-of-the-art performance on prompt fidelity. We also steer text diffusion models with rewards for text quality and rare attributes such as toxicity, and find that FK steering generates lower perplexity text and enables gradient-free control. Overall, inference-time scaling and steering of diffusion models – even training-free – provides significant quality and controllability benefits.",
    "is_llm_safety": true
  },
  {
    "title": "RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts",
    "url": "https://icml.cc/virtual/2025/poster/46519",
    "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-$k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts).",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.15114v1",
    "arxiv_id": "2411.15114v1",
    "arxiv_title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
    "arxiv_authors": [
      "Hjalmar Wijk",
      "Tao Lin",
      "Joel Becker",
      "Sami Jawhar",
      "Neev Parikh",
      "Thomas Broadley",
      "Lawrence Chan",
      "Michael Chen",
      "Josh Clymer",
      "Jai Dhyani",
      "Elena Ericheva",
      "Katharyn Garcia",
      "Brian Goodrich",
      "Nikola Jurkovic",
      "Megan Kinniment",
      "Aron Lajko",
      "Seraphina Nix",
      "Lucas Sato",
      "William Saunders",
      "Maksym Taran",
      "Ben West",
      "Elizabeth Barnes"
    ],
    "arxiv_published": "2024-11-22T18:30:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.15114v1",
        "arxiv_url": "http://arxiv.org/abs/2411.15114v1",
        "arxiv_title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
        "authors": [
          "Hjalmar Wijk",
          "Tao Lin",
          "Joel Becker",
          "Sami Jawhar",
          "Neev Parikh",
          "Thomas Broadley",
          "Lawrence Chan",
          "Michael Chen",
          "Josh Clymer",
          "Jai Dhyani",
          "Elena Ericheva",
          "Katharyn Garcia",
          "Brian Goodrich",
          "Nikola Jurkovic",
          "Megan Kinniment",
          "Aron Lajko",
          "Seraphina Nix",
          "Lucas Sato",
          "William Saunders",
          "Maksym Taran",
          "Ben West",
          "Elizabeth Barnes"
        ],
        "published": "2024-11-22T18:30:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "url": "https://icml.cc/virtual/2025/poster/45947",
    "abstract": "Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs. Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results. Code will be released soon.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06655v2",
    "arxiv_id": "2502.06655v2",
    "arxiv_title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "arxiv_authors": [
      "Meilin Chen",
      "Jian Tian",
      "Liang Ma",
      "Di Xie",
      "Weijie Chen",
      "Jiang Zhu"
    ],
    "arxiv_published": "2025-02-10T16:45:18+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06655v2",
        "arxiv_url": "http://arxiv.org/abs/2502.06655v2",
        "arxiv_title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
        "authors": [
          "Meilin Chen",
          "Jian Tian",
          "Liang Ma",
          "Di Xie",
          "Weijie Chen",
          "Jiang Zhu"
        ],
        "published": "2025-02-10T16:45:18+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Scaling Trends in Language Model Robustness",
    "url": "https://icml.cc/virtual/2025/poster/43784",
    "abstract": "Language models exhibit scaling laws, whereby increasing model and dataset size predictably decreases negative log likelihood, unlocking a dazzling array of capabilities. At the same time, even the most capable systems are currently vulnerable to adversarial inputs such as jailbreaks and prompt injections, despite concerted efforts to make them robust. As compute becomes more accessible to both attackers and defenders, which side will benefit more from scale?We attempt to answer this question with a detailed study of robustness on language models spanning three orders of magnitude in parameter count. From the defender's perspective, we find that in the absence of other interventions, increasing model size alone does not consistently improve robustness. In adversarial training, we find that larger models are more sample-efficient and less compute-efficient than smaller models, and often better generalize their defense to new threat models. From the attacker’s perspective, we find that increasing attack compute smoothly and reliably increases attack success rate against both finetuned and adversarially trained models. Finally, we show that across model sizes studied, doubling compute on adversarial training only forces an attacker to less than double attack compute to maintain the same attack success rate. However, adversarial training becomes more and more effective on larger models, suggesting that defenders could eventually have the advantage with increasing model size.These results underscore the value of adopting a scaling lens when discussing robustness of frontier models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2407.18213v4",
    "arxiv_id": "2407.18213v4",
    "arxiv_title": "Scaling Trends in Language Model Robustness",
    "arxiv_authors": [
      "Nikolaus Howe",
      "Ian McKenzie",
      "Oskar Hollinsworth",
      "Michał Zajac",
      "Tom Tseng",
      "Aaron Tucker",
      "Pierre-Luc Bacon",
      "Adam Gleave"
    ],
    "arxiv_published": "2024-07-25T17:26:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2407.18213v4",
        "arxiv_url": "http://arxiv.org/abs/2407.18213v4",
        "arxiv_title": "Scaling Trends in Language Model Robustness",
        "authors": [
          "Nikolaus Howe",
          "Ian McKenzie",
          "Oskar Hollinsworth",
          "Michał Zajac",
          "Tom Tseng",
          "Aaron Tucker",
          "Pierre-Luc Bacon",
          "Adam Gleave"
        ],
        "published": "2024-07-25T17:26:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Comparing Few to Rank Many: Active Human Preference Learning using Randomized Frank-Wolfe method",
    "url": "https://icml.cc/virtual/2025/poster/44684",
    "abstract": "We study learning human preferences from limited comparison feedback, a core machine learning task with significant impact on reinforcement learning from human feedback (RLHF). We formulate this problem as learning a Plackett-Luce (PL) model from a small number of $K$-way comparisons over a universe of $N$ choices, where typically $K \\ll N$. Our objective is to select these $K$-subsets to elicit the $K$-way ranking feedback in such a way that the true ranking of the $N$ items can be estimated from the feedback with minimal mistakes. Our approach selects the subsets using the framework of D-optimal design which aims to minimize the worst-case ranking mistake of the estimated PL model. However, all known algorithms for this problem are computationally infeasible in our setting due to exponentially-many ${N \\choose K}$ subsets to consider. To address this issue, we propose a Frank-Wolfe algorithm that exploits randomization, memoization, and sparse updates to achieve $O(N^2 + K^2)$ per-iteration complexity. We analyze it, and showcase its superior empirical performance on synthetic and open-source NLP datasets.",
    "is_llm_safety": true
  },
  {
    "title": "Textual Unlearning Gives a False Sense of Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/44277",
    "abstract": "Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.13348v2",
    "arxiv_id": "2406.13348v2",
    "arxiv_title": "Textual Unlearning Gives a False Sense of Unlearning",
    "arxiv_authors": [
      "Jiacheng Du",
      "Zhibo Wang",
      "Jie Zhang",
      "Xiaoyi Pang",
      "Jiahui Hu",
      "Kui Ren"
    ],
    "arxiv_published": "2024-06-19T08:51:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.13348v2",
        "arxiv_url": "http://arxiv.org/abs/2406.13348v2",
        "arxiv_title": "Textual Unlearning Gives a False Sense of Unlearning",
        "authors": [
          "Jiacheng Du",
          "Zhibo Wang",
          "Jie Zhang",
          "Xiaoyi Pang",
          "Jiahui Hu",
          "Kui Ren"
        ],
        "published": "2024-06-19T08:51:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "url": "https://icml.cc/virtual/2025/poster/44337",
    "abstract": "The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make edits given code context, and solve algorithmic coding tasks. To achieve full automation however, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 challenging tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI’s o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by LLMs; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps toward autonomous and secure software development with LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.11844v2",
    "arxiv_id": "2502.11844v2",
    "arxiv_title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "arxiv_authors": [
      "Mark Vero",
      "Niels Mündler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanović",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "arxiv_published": "2025-02-17T14:37:47+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.11844v2",
        "arxiv_url": "http://arxiv.org/abs/2502.11844v2",
        "arxiv_title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "authors": [
          "Mark Vero",
          "Niels Mündler",
          "Victor Chibotaru",
          "Veselin Raychev",
          "Maximilian Baader",
          "Nikola Jovanović",
          "Jingxuan He",
          "Martin Vechev"
        ],
        "published": "2025-02-17T14:37:47+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "When Bad Data Leads to Good Models",
    "url": "https://icml.cc/virtual/2025/poster/45199",
    "abstract": "In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we challenge the notion of ``quality'' in the context of post-training. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.04741v1",
    "arxiv_id": "2505.04741v1",
    "arxiv_title": "When Bad Data Leads to Good Models",
    "arxiv_authors": [
      "Kenneth Li",
      "Yida Chen",
      "Fernanda Viégas",
      "Martin Wattenberg"
    ],
    "arxiv_published": "2025-05-07T19:17:49+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.04741v1",
        "arxiv_url": "http://arxiv.org/abs/2505.04741v1",
        "arxiv_title": "When Bad Data Leads to Good Models",
        "authors": [
          "Kenneth Li",
          "Yida Chen",
          "Fernanda Viégas",
          "Martin Wattenberg"
        ],
        "published": "2025-05-07T19:17:49+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning",
    "url": "https://icml.cc/virtual/2025/poster/46150",
    "abstract": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks \\cite{qi2023fine}-- a few harmful data mixed in the fine-tuning dataset can break the LLMs's safety alignment. While several defenses have been proposed, our evaluation shows that existing defenses fail \\textit{when some specific training hyper-parameters are chosen} -- a large learning rate or a large number of training epochs in the fine-tuning stage can easily invalidate the defense. To this end,  we propose Antidote, a post-fine-tuning stage solution, which remains \\textbf{\\textit{agnostic to the training hyper-parameters in the fine-tuning stage}}. Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage. With this philosophy, we introduce a one-shot pruning stage after harmful fine-tuning to remove the harmful weights that are responsible for the generation of harmful content. Despite its embarrassing simplicity, empirical results show that Antidote can reduce harmful score while maintaining accuracy on downstream tasks. Our code is attached in supplementary materials.",
    "is_llm_safety": true
  },
  {
    "title": "Fragments to Facts: Partial-Information Fragment Inference from LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45801",
    "abstract": "Large language models (LLMs) can leak sensitive training data through memorization and membership inference attacks. Prior work has primarily focused on strong adversarial assumptions, including attacker access to entire samples or long, ordered prefixes, leaving open the question of how vulnerable LLMs are when adversaries have only partial, unordered sample information. For example, if an attacker knows a patient has \"hypertension,\" under what conditions can they query a model fine-tuned on patient data to learn the patient also has \"osteoarthritis?\" In this paper, we introduce a more general threat model under this weaker assumption and show that fine-tuned LLMs are susceptible to these fragment-specific extraction attacks. To systematically investigate these attacks, we propose two data-blind methods: (1) a likelihood ratio attack inspired by methods from membership inference, and (2) a novel approach, PRISM, which regularizes the ratio by leveraging an external prior. Using examples from both medical and legal settings, we show that both methods are competitive with a data-aware baseline classifier that assumes access to labeled in-distribution data, underscoring their robustness.",
    "is_llm_safety": true
  },
  {
    "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
    "url": "https://icml.cc/virtual/2025/poster/45579",
    "abstract": "As large language models (LLMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute—expensive and inflexible—or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable Generation), a novel framework that efficiently predicts EAP and adapts to new attributes through tractable generation and lightweight control. TRACE distills a Hidden Markov Model (HMM) from a language model and pairs it with a small classifier to score EAP, reweighting next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with negligible decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes. Our findings underscore TRACE’s strong performance, efficiency, flexibility, and compositionality for ensuring global attribute satisfaction.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.18535v1",
    "arxiv_id": "2504.18535v1",
    "arxiv_title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
    "arxiv_authors": [
      "Gwen Yidou Weng",
      "Benjie Wang",
      "Guy Van den Broeck"
    ],
    "arxiv_published": "2025-04-25T17:59:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.18535v1",
        "arxiv_url": "http://arxiv.org/abs/2504.18535v1",
        "arxiv_title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
        "authors": [
          "Gwen Yidou Weng",
          "Benjie Wang",
          "Guy Van den Broeck"
        ],
        "published": "2025-04-25T17:59:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Quantifying Prediction Stability Under Fine-tuning Multiplicity in Tabular LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46165",
    "abstract": "Fine-tuning LLMs on tabular classification tasks can lead to the phenomenon of \\emph{fine-tuning multiplicity} where equally well-performing models make conflicting predictions on the same input. Fine-tuning multiplicity can arise due to variations in the training process, e.g., seed, weight initialization, minor changes to training data, etc., raising concerns about the reliability of Tabular LLMs in high-stakes applications such as finance, hiring, education, healthcare. Our work formalizes this unique challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel measure to quantify the stability of individual predictions without expensive model retraining. Our measure quantifies a prediction's stability by analyzing (sampling) the model's local behavior around that input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic guarantees on prediction robustness under a broad class of fine-tuned models, i.e., inputs with sufficiently high stability (as defined by our measure) also remain robust across several fine-tuned models with high probability. We perform experiments on multiple real-world datasets to show that our stability measure preemptively captures robustness under actual multiplicity across several fine-tuned models, outperforming competing measures.",
    "is_llm_safety": true
  },
  {
    "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "url": "https://icml.cc/virtual/2025/poster/45356",
    "abstract": "Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted outputs, posing a serious threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This disrupts the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the \"unsafe\" prediction rate, bypassing existing safeguards.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.01077v2",
    "arxiv_id": "2411.01077v2",
    "arxiv_title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "arxiv_authors": [
      "Zhipeng Wei",
      "Yuqi Liu",
      "N. Benjamin Erichson"
    ],
    "arxiv_published": "2024-11-01T23:18:32+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.01077v2",
        "arxiv_url": "http://arxiv.org/abs/2411.01077v2",
        "arxiv_title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
        "authors": [
          "Zhipeng Wei",
          "Yuqi Liu",
          "N. Benjamin Erichson"
        ],
        "published": "2024-11-01T23:18:32+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Test-Time Multimodal Backdoor Detection by Contrastive Prompting",
    "url": "https://icml.cc/virtual/2025/poster/46621",
    "abstract": "While multimodal contrastive learning methods (e.g., CLIP) can achieve impressive zero-shot classification performance, recent research has revealed that these methods are vulnerable to backdoor attacks. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates and are not applicable in black-box settings. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the inference stage. We empirically find that the visual representations of backdoored images are insensitive to benign and malignant changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt a language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency.",
    "is_llm_safety": true
  },
  {
    "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning",
    "url": "https://icml.cc/virtual/2025/poster/45951",
    "abstract": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through Fine-tuning-as-a-Service, breaks safety alignment and poses significant threats. Existing methods aim to mitigate HFT risks by learning robust representations on alignment data or making harmful data unlearnable, but they treat each data sample equally, leaving data vulnerability patterns understudied. In this work, we reveal that certain subsets of alignment data are consistently more prone to forgetting during HFT across different fine-tuning tasks and exhibit lower robustness compared to other subsets. Inspired by these findings, we propose Vulnerability-Aware Alignment (VAA), which calculates data vulnerability, partitions data into \"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using a group distributionally robust optimization (Group DRO) framework. Specifically, VAA learns an adversarial sampler that samples examples from the hard group and then applies group-dependent adversarial perturbations to the data during training, ultimately achieving convergence to a balanced learning equilibrium. Experiments across four fine-tuning tasks demonstrate that VAA significantly reduces harmful scores while preserving downstream task performance, outperforming state-of-the-art baselines.",
    "is_llm_safety": true
  },
  {
    "title": "Core Knowledge Deficits in Multi-Modal Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45955",
    "abstract": "While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild still lags behind humans and exhibits diminished efficacy on simple tasks that are intuitive for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge—rudimentary cognitive abilities innate to humans from early childhood. To probe core knowledge representation in MLLMs, we draw from developmental cognitive sciences and develop a large-scale benchmark, \\textbf{CoreCognition dataset}, encompassing 12 core cognitive concepts. We evaluate 219 models with 10 different prompts, leading to a total of 2409 data points for analysis. Our findings reveal core knowledge deficits in early-developed core abilities while models demonstrate human-comparable performance in high-level cognition.. Moreover, we find that low-level abilities show little to no scaling , in stack contrast to high-level abilities. Finally, we introduce an evaluation technique ``Concept Hacking\", through which we demonstrate that MLLMs do not genuinely advance toward core knowledge but instead rely on superficial patterns and shortcut learning as they scale.",
    "is_llm_safety": true
  },
  {
    "title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/44285",
    "abstract": "While the capabilities of generative foundational models have advanced rapidly in recent years, methods to prevent harmful and unsafe behaviors remain underdeveloped. Among the pressing challenges in AI safety, machine unlearning (MU) has become increasingly critical to meet upcoming safety regulations. Most existing MU approaches focus on altering the most significant parameters of the model. However, these methods often require fine-tuning substantial portions of the model, resulting in high computational costs and training instabilities, which are typically mitigated by access to the original training dataset.In this work, we address these limitations by leveraging Singular Value Decomposition (SVD) to create a compact, low-dimensional projection that enables the selective forgetting of specific data points. We propose Singular Value Decomposition for Efficient Machine Unlearning (SEMU), a novel approach designed to optimize MU in two key aspects. First, SEMU minimizes the number of model parameters that need to be modified, effectively removing unwanted knowledge while making only minimal changes to the model's weights. Second, SEMU eliminates the dependency on the original training dataset, preserving the model's previously acquired knowledge without additional data requirements.Extensive experiments demonstrate that SEMU achieves competitive performance while significantly improving efficiency in terms of both data usage and the number of modified parameters.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07587v1",
    "arxiv_id": "2502.07587v1",
    "arxiv_title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "arxiv_authors": [
      "Marcin Sendera",
      "Łukasz Struski",
      "Kamil Książek",
      "Kryspin Musiol",
      "Jacek Tabor",
      "Dawid Rymarczyk"
    ],
    "arxiv_published": "2025-02-11T14:36:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07587v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07587v1",
        "arxiv_title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
        "authors": [
          "Marcin Sendera",
          "Łukasz Struski",
          "Kamil Książek",
          "Kryspin Musiol",
          "Jacek Tabor",
          "Dawid Rymarczyk"
        ],
        "published": "2025-02-11T14:36:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Policy Filtration for RLHF to Mitigate Noise in Reward Models",
    "url": "https://icml.cc/virtual/2025/poster/45609",
    "abstract": "While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination ($R^2$) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH).",
    "is_llm_safety": true
  },
  {
    "title": "How Much Can We Forget about Data Contamination?",
    "url": "https://icml.cc/virtual/2025/poster/45377",
    "abstract": "The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B,  have forgotten the data seen at the beginning of training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.03249v3",
    "arxiv_id": "2410.03249v3",
    "arxiv_title": "How Much Can We Forget about Data Contamination?",
    "arxiv_authors": [
      "Sebastian Bordt",
      "Suraj Srinivas",
      "Valentyn Boreiko",
      "Ulrike von Luxburg"
    ],
    "arxiv_published": "2024-10-04T09:14:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.03249v3",
        "arxiv_url": "http://arxiv.org/abs/2410.03249v3",
        "arxiv_title": "How Much Can We Forget about Data Contamination?",
        "authors": [
          "Sebastian Bordt",
          "Suraj Srinivas",
          "Valentyn Boreiko",
          "Ulrike von Luxburg"
        ],
        "published": "2024-10-04T09:14:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Observation Interference in Partially Observable Assistance Games",
    "url": "https://icml.cc/virtual/2025/poster/43874",
    "abstract": "We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of perfect information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of perfect information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.17797v1",
    "arxiv_id": "2412.17797v1",
    "arxiv_title": "Observation Interference in Partially Observable Assistance Games",
    "arxiv_authors": [
      "Scott Emmons",
      "Caspar Oesterheld",
      "Vincent Conitzer",
      "Stuart Russell"
    ],
    "arxiv_published": "2024-12-23T18:53:33+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.17797v1",
        "arxiv_url": "http://arxiv.org/abs/2412.17797v1",
        "arxiv_title": "Observation Interference in Partially Observable Assistance Games",
        "authors": [
          "Scott Emmons",
          "Caspar Oesterheld",
          "Vincent Conitzer",
          "Stuart Russell"
        ],
        "published": "2024-12-23T18:53:33+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Policy-labeled Preference Learning: Is Preference Enough for RLHF?",
    "url": "https://icml.cc/virtual/2025/poster/43946",
    "abstract": "To design reward that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing models using reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. To address this, we propose Policy-labeled Preference Learning (PPL) within the Direct Preference Optimization (DPO) framework, which resolves these likelihood mismatch problems by modeling human preferences with regret, reflecting the efficiency of executed policies. Additionally, we introduce a contrastive KL regularization term derived from regret-based principles to enhance sequential contrastive learning. Experiments in high-dimensional continuous control environments demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.",
    "is_llm_safety": true
  },
  {
    "title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
    "url": "https://icml.cc/virtual/2025/poster/43693",
    "abstract": "A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.03869v5",
    "arxiv_id": "2405.03869v5",
    "arxiv_title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
    "arxiv_authors": [
      "Anshuman Chhabra",
      "Bo Li",
      "Jian Chen",
      "Prasant Mohapatra",
      "Hongfu Liu"
    ],
    "arxiv_published": "2024-05-06T21:34:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.03869v5",
        "arxiv_url": "http://arxiv.org/abs/2405.03869v5",
        "arxiv_title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
        "authors": [
          "Anshuman Chhabra",
          "Bo Li",
          "Jian Chen",
          "Prasant Mohapatra",
          "Hongfu Liu"
        ],
        "published": "2024-05-06T21:34:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Eigenspectrum Analysis of Weight Matrices without Aspect Ratio Bias",
    "url": "https://icml.cc/virtual/2025/poster/46300",
    "abstract": "Researchers are increasingly interested in studying the eigenspectrum of weight matrices of deep neural networks (DNNs) and its intricate relationship with model quality. At a high level, eigenspectrum analysis of weight matrices involves measuring the \\emph{heavytailness} of the empirical spectral density (ESD) of a weight matrix, and it provides insight into how well a model is trained and can guide decisions related to assigning better layer-wise training hyperparameters. In this paper, we address a specific challenge associated with such eigenspectrum methods: the impact of the aspect ratio of weight matrices on estimated heavytailness metrics. Through empirical analysis, we demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in the estimation of heavytailness metrics, leading to inaccurate model diagnosis and hyperparameter scheduling. To overcome this limitation, we propose normalizing the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, matrix subsampling uniformly improves the accuracy of eigenspectrum analysis while enabling more balanced and effective model optimization in these application domains. In one of the LLM pruning experiments, \\ourmethod reduces the perplexity of the LLaMA-7B model by 19.74\\% when compared with the state-of-the-art method.",
    "is_llm_safety": true
  },
  {
    "title": "Unbiased and Economic LLM Evaluation via Synthetic Feedback",
    "url": "https://icml.cc/virtual/2025/poster/46205",
    "abstract": "Accurately evaluating large language models (LLMs) is crucial for ensuring their reliability and safety. Among various approaches, human evaluation is the gold standard, especially when we need to capture nuanced qualities like coherence, readability and alignment with human expectations. However, such evaluations are often resource-intensive and costly, posing challenges for scalability. To reduce the cost, previous academic benchmarks employ other LLMs to create synthetic feedback, introducing bias into the evaluation. Towards accurate and economic LLM evaluation, in this work, we introduce a statistically-inspired estimator that requires a reduced number of human annotations via synthetic feedback. Our experiments demonstrate a reduction in human annotations by up to $12.2$\\% with an off-the-shelf synthetic evaluator, and up to $24.8$\\% with a finetuned variant. Apart from being general and scalable, our proposed method enjoys predictable saving performance.",
    "is_llm_safety": true
  },
  {
    "title": "Position: In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI",
    "url": "https://icml.cc/virtual/2025/poster/40170",
    "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems lag far behind more mature industries, like software security. Based on a collaboration between experts from software security, machine learning, law, and policy, we identify key gaps in the evaluation and reporting of GPAI system flaws. Our position paper argues for three interventions to ultimately improve system safety: (1) the use of standardized AI system flaw reporting, (2) the adoption of flaw disclosure programs with researcher protections, and (3) infrastructure to coordinate flaw disclosure across stakeholders. First, we propose standardized AI flaw reports, to ease the process of submitting, reproducing, and triaging identified system flaws. Second, we propose GPAI system providers should adopt broadly-scoped flaw disclosure programs, borrowing from bug bounty programs, with legal safe harbors to protect both researchers and themselves. Third, we advocate for new infrastructure to coordinate flaw reports across many stakeholders, who may be impacted by certain flaws that commonly work on many independent GPAI systems (e.g., certain jailbreaks). By promoting a robust safety reporting and coordination in the GPAI ecosystem, these changes, if adopted, could ultimately improve AI system safety, security, and accountability.",
    "is_llm_safety": true
  },
  {
    "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "url": "https://icml.cc/virtual/2025/poster/45659",
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative.In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03699v1",
    "arxiv_id": "2502.03699v1",
    "arxiv_title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "arxiv_authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Zhen Qin",
      "Ziqi Wang",
      "Wei Xiong",
      "Yu Meng",
      "Jiawei Han",
      "Sercan O. Arik"
    ],
    "arxiv_published": "2025-02-06T01:22:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03699v1",
        "arxiv_url": "http://arxiv.org/abs/2502.03699v1",
        "arxiv_title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
        "authors": [
          "Bowen Jin",
          "Jinsung Yoon",
          "Zhen Qin",
          "Ziqi Wang",
          "Wei Xiong",
          "Yu Meng",
          "Jiawei Han",
          "Sercan O. Arik"
        ],
        "published": "2025-02-06T01:22:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
    "url": "https://icml.cc/virtual/2025/poster/43898",
    "abstract": "Reward models are widely used as proxies for human preferences when aligning or evaluating LLMs.However, reward models are black boxes, and it is often unclear what, exactly, they are actually rewarding. In this paper we develop Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for measuring the sensitivity of a reward model to high-level attributes of responses, such as sentiment, helpfulness, or complexity. Importantly, RATE measures the causal effect of an attribute on the reward. RATE uses LLMs to rewrite responses to produce imperfect counterfactuals examples that can be used to measure causal effects. A key challenge is that these rewrites are imperfect in a manner that can induce substantial bias in the estimated sensitivity of the reward model to the attribute. The core idea of RATE is to adjust for this imperfect-rewrite effect by rewriting twice. We establish the validity of the RATE procedure and show empirically that it is an effective estimator.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.11348v3",
    "arxiv_id": "2410.11348v3",
    "arxiv_title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
    "arxiv_authors": [
      "David Reber",
      "Sean Richardson",
      "Todd Nief",
      "Cristina Garbacea",
      "Victor Veitch"
    ],
    "arxiv_published": "2024-10-15T07:22:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.11348v3",
        "arxiv_url": "http://arxiv.org/abs/2410.11348v3",
        "arxiv_title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
        "authors": [
          "David Reber",
          "Sean Richardson",
          "Todd Nief",
          "Cristina Garbacea",
          "Victor Veitch"
        ],
        "published": "2024-10-15T07:22:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44613",
    "abstract": "Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well.In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds.AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response.Experimental results on popular open source TargetLLM show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs.We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.16873v1",
    "arxiv_id": "2404.16873v1",
    "arxiv_title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "arxiv_authors": [
      "Anselm Paulus",
      "Arman Zharmagambetov",
      "Chuan Guo",
      "Brandon Amos",
      "Yuandong Tian"
    ],
    "arxiv_published": "2024-04-21T22:18:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.16873v1",
        "arxiv_url": "http://arxiv.org/abs/2404.16873v1",
        "arxiv_title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
        "authors": [
          "Anselm Paulus",
          "Arman Zharmagambetov",
          "Chuan Guo",
          "Brandon Amos",
          "Yuandong Tian"
        ],
        "published": "2024-04-21T22:18:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "UDora: A Unified Red Teaming Framework Against LLM Agents by Dynamically Leveraging Their Own Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44008",
    "abstract": "Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for handling complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements also amplify the risks of adversarial attacks, particularly when LLM agents can access sensitive external functionalities. Moreover, because LLM agents engage in extensive reasoning or planning before executing final actions, manipulating them into performing targeted malicious actions or invoking specific tools remains a significant challenge. Consequently, directly embedding adversarial strings in malicious instructions or injecting malicious prompts into tool interactions has become less effective against modern LLM agents. In this work, we present UDora, a unified red teaming framework designed for LLM Agents that dynamically leverages the agent's own reasoning processes to compel it toward malicious behavior. Specifically, UDora first samples the model's reasoning for the given task, then automatically identifies multiple optimal positions within these reasoning traces to insert targeted perturbations. Subsequently, it uses the modified reasoning as the objective to optimize the adversarial strings. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets.",
    "is_llm_safety": true
  },
  {
    "title": "Pre-Memorization Train Accuracy Reliably Predicts Generalization in LLM Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44724",
    "abstract": "Modern large language models (LLMs) excel at fitting finetuning data, but often struggle on unseen examples. In order to teach models genuine reasoning abilities rather than superficial pattern matching, our work aims to better understand how the learning dynamics of LLM finetuning shapes downstream generalization. Our analysis focuses on reasoning tasks, whose problem structure allows us to distinguish between memorization (the exact replication of reasoning steps from the training data) and performance (the correctness of the final solution). We find that a model's performance on test prompts can be effectively characterized by a training metric we call pre-memorization train accuracy: the accuracy of model samples on training queries before they begin to copy the exact reasoning steps from the training set. On the dataset level, this metric is able to almost perfectly predict test accuracy, achieving $R^2$ of $\\geq 0.9$ across various models (Llama3 8B, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On a per-example level, this metric is also indicative of whether individual model predictions are robust to perturbations in the training query. By connecting a model's learning dynamics to test performance, pre-memorization train accuracy can inform training decisions, such as the makeup of the training data. Our experiments on data curation show that prioritizing examples with low pre-memorization accuracy leads to 1.5-2x improvements in data efficiency compared to i.i.d. data scaling and other data scaling techniques.",
    "is_llm_safety": true
  },
  {
    "title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage",
    "url": "https://icml.cc/virtual/2025/poster/45289",
    "abstract": "Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that the proposed evaluation method aligns better with human judgments of factuality than existing metrics. Moreover, we show that current approaches for enhancing MLLM factuality often fail in hyper-detailed image captioning tasks. In contrast, our approach significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions.",
    "is_llm_safety": true
  },
  {
    "title": "Scalably Solving Assistance Games",
    "url": "https://icml.cc/virtual/2025/poster/44757",
    "abstract": "Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Code and videos are available at https://anonymous.4open.science/w/scalably-solving-assistance-games/.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.07091v1",
    "arxiv_id": "2504.07091v1",
    "arxiv_title": "AssistanceZero: Scalably Solving Assistance Games",
    "arxiv_authors": [
      "Cassidy Laidlaw",
      "Eli Bronstein",
      "Timothy Guo",
      "Dylan Feng",
      "Lukas Berglund",
      "Justin Svegliato",
      "Stuart Russell",
      "Anca Dragan"
    ],
    "arxiv_published": "2025-04-09T17:59:03+00:00",
    "similarity_score": 0.8048780487804879,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.07091v1",
        "arxiv_url": "http://arxiv.org/abs/2504.07091v1",
        "arxiv_title": "AssistanceZero: Scalably Solving Assistance Games",
        "authors": [
          "Cassidy Laidlaw",
          "Eli Bronstein",
          "Timothy Guo",
          "Dylan Feng",
          "Lukas Berglund",
          "Justin Svegliato",
          "Stuart Russell",
          "Anca Dragan"
        ],
        "published": "2025-04-09T17:59:03+00:00",
        "similarity_score": 0.8048780487804879
      }
    ]
  },
  {
    "title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46462",
    "abstract": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90\\% recall in premise identification.  We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6\\% to 16\\% absolute when step-by-step verification is carried out in PARC under the premises.Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.",
    "is_llm_safety": true
  },
  {
    "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models Via Visual Information Steering",
    "url": "https://icml.cc/virtual/2025/poster/46338",
    "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer.(3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference.Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.",
    "is_llm_safety": true
  },
  {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44422",
    "abstract": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment.  HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes “hard” dispreferred responses—those closely resembling preferred ones—to enhance the model’s rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS’s effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.14400v1",
    "arxiv_id": "2502.14400v1",
    "arxiv_title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "arxiv_authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "arxiv_published": "2025-02-20T09:37:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.14400v1",
        "arxiv_url": "http://arxiv.org/abs/2502.14400v1",
        "arxiv_title": "HPS: Hard Preference Sampling for Human Preference Alignment",
        "authors": [
          "Xiandong Zou",
          "Wanyu Lin",
          "Yuchen Li",
          "Pan Zhou"
        ],
        "published": "2025-02-20T09:37:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Preference Learning for AI Alignment: a Causal Perspective",
    "url": "https://icml.cc/virtual/2025/poster/44331",
    "abstract": "Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of casual inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.",
    "is_llm_safety": true
  },
  {
    "title": "Adversaries Can Misuse Combinations of Safe Models",
    "url": "https://icml.cc/virtual/2025/poster/45845",
    "abstract": "Developers try to evaluate whether an AI system can accomplish malicious tasks before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for such misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.14595v2",
    "arxiv_id": "2406.14595v2",
    "arxiv_title": "Adversaries Can Misuse Combinations of Safe Models",
    "arxiv_authors": [
      "Erik Jones",
      "Anca Dragan",
      "Jacob Steinhardt"
    ],
    "arxiv_published": "2024-06-20T17:43:18+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.14595v2",
        "arxiv_url": "http://arxiv.org/abs/2406.14595v2",
        "arxiv_title": "Adversaries Can Misuse Combinations of Safe Models",
        "authors": [
          "Erik Jones",
          "Anca Dragan",
          "Jacob Steinhardt"
        ],
        "published": "2024-06-20T17:43:18+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Trustworthy AI Agents Require the Integration of Large Language Models and Formal Methods",
    "url": "https://icml.cc/virtual/2025/poster/40101",
    "abstract": "Large Language Models (LLMs) have emerged as a transformative AI paradigm, profoundly influencing daily life through their exceptional language understanding and contextual generation capabilities. Despite their remarkable performance, LLMs face a critical challenge: the propensity to produce unreliable outputs due to the inherent limitations of their learning-based nature. Formal methods (FMs), on the other hand, are a well-established computation paradigm that provides mathematically rigorous techniques for modeling, specifying, and verifying the correctness of systems. FMs have been extensively applied in mission-critical software engineering, embedded systems, and cybersecurity. However, the primary challenge impeding the deployment of FMs in real-world settings lies in their steep learning curves, the absence of user-friendly interfaces, and issues with efficiency and adaptability. This position paper outlines a roadmap for advancing the next generation of trustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs. First, we illustrate how FMs, including reasoning and certification techniques, can help LLMs generate more reliable and formally certified outputs. Subsequently, we highlight how the advanced learning capabilities and adaptability of LLMs can significantly enhance the usability, efficiency, and scalability of existing FM tools. Finally, we show that unifying these two computation paradigms $-$ integrating the flexibility and intelligence of LLMs with the rigorous reasoning abilities of FMs $-$ has transformative potential for the development of trustworthy AI software systems. We acknowledge that this integration has the potential to enhance both the trustworthiness and efficiency of software engineering practices while fostering the development of intelligent FM tools capable of addressing complex yet real-world challenges.",
    "is_llm_safety": true
  },
  {
    "title": "Topological Signatures of Adversaries in Multimodal Alignments",
    "url": "https://icml.cc/virtual/2025/poster/46109",
    "abstract": "Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to back-propagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18006v1",
    "arxiv_id": "2501.18006v1",
    "arxiv_title": "Topological Signatures of Adversaries in Multimodal Alignments",
    "arxiv_authors": [
      "Minh Vu",
      "Geigh Zollicoffer",
      "Huy Mai",
      "Ben Nebgen",
      "Boian Alexandrov",
      "Manish Bhattarai"
    ],
    "arxiv_published": "2025-01-29T21:45:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18006v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18006v1",
        "arxiv_title": "Topological Signatures of Adversaries in Multimodal Alignments",
        "authors": [
          "Minh Vu",
          "Geigh Zollicoffer",
          "Huy Mai",
          "Ben Nebgen",
          "Boian Alexandrov",
          "Manish Bhattarai"
        ],
        "published": "2025-01-29T21:45:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic Stochastic Manipulations",
    "url": "https://icml.cc/virtual/2025/poster/44304",
    "abstract": "In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and introducing disparities between equally qualified candidates. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a ``two-ticket'' scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.",
    "is_llm_safety": true
  },
  {
    "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "url": "https://icml.cc/virtual/2025/poster/43805",
    "abstract": "Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks.These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks.In reality, these PTLMs can be adapted to many other unrelated downstream tasks.Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness.We refer to this phenomenon as backdoor complications.In this paper, we undertake the first comprehensive quantification of backdoor complications.Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs.The output distribution of triggered samples significantly deviates from that of clean samples.Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks.The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.11586v1",
    "arxiv_id": "2505.11586v1",
    "arxiv_title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "arxiv_authors": [
      "Rui Zhang",
      "Yun Shen",
      "Hongwei Li",
      "Wenbo Jiang",
      "Hanxiao Chen",
      "Yuan Zhang",
      "Guowen Xu",
      "Yang Zhang"
    ],
    "arxiv_published": "2025-05-16T17:59:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.11586v1",
        "arxiv_url": "http://arxiv.org/abs/2505.11586v1",
        "arxiv_title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
        "authors": [
          "Rui Zhang",
          "Yun Shen",
          "Hongwei Li",
          "Wenbo Jiang",
          "Hanxiao Chen",
          "Yuan Zhang",
          "Guowen Xu",
          "Yang Zhang"
        ],
        "published": "2025-05-16T17:59:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Beyond Assistance – Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care",
    "url": "https://icml.cc/virtual/2025/poster/40113",
    "abstract": "This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere assistive tools. While LLMs have the potential to enhance accessibility, personalization, and crisis intervention, their adoption remains limited due to concerns about bias, evaluation, over-reliance, dehumanization, and regulatory uncertainties. To address these challenges, we propose two structured pathways: SAFE-I Implementation Guidelines for ethical and responsible deployment, and HAAS-E Evaluation Framework for multidimensional, human-centered assessment. SAFE-I provides a blueprint for data governance, adaptive model engineering, and real-world integration, ensuring LLMs align with clinical and ethical standards. HAAS-E introduces evaluation metrics that go beyond technical accuracy to measure trustworthiness, empathy, cultural sensitivity, and actionability. We call for the adoption of these structured approaches to establish a responsible and scalable model for LLM-driven mental health support, ensuring that AI complements—rather than replaces—human expertise.",
    "is_llm_safety": true
  },
  {
    "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "url": "https://icml.cc/virtual/2025/poster/46174",
    "abstract": "Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction.To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandits (\\algo), which is based on uncertainty-weighted maximum likelihood estimation.  Our algorithm achieves an $\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the derivative of the link function, and $  0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives into maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm \\algo \\ against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.10776v2",
    "arxiv_id": "2404.10776v2",
    "arxiv_title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "arxiv_authors": [
      "Qiwei Di",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "arxiv_published": "2024-04-16T17:59:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.10776v2",
        "arxiv_url": "http://arxiv.org/abs/2404.10776v2",
        "arxiv_title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
        "authors": [
          "Qiwei Di",
          "Jiafan He",
          "Quanquan Gu"
        ],
        "published": "2024-04-16T17:59:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Is Best-of-N the Best of Them? Coverage, Scaling, and Optimality in Inference-Time Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45322",
    "abstract": "Recent work on inference-time alignment has established benefits of increasing inference-time computation in language models, but naively scaling compute through techniques like Best-of-N sampling can cause performance to degrade due to reward hacking. Toward a theoretical understanding of how to best leverage additional computation, we formalize inference-time alignment as improving a pre-trained policy’s responses for a prompt of interest, given access to an imperfect reward model. We analyze the performance of inference-time alignment algorithms in terms of (i) response quality, and (ii) compute, and provide new results that highlight the importance of the pre-trained policy’s coverage over high-quality responses for performance and compute scaling: (1) We show that Best-of-N alignment with an ideal N can achieve optimal performance under stringent notions of coverage, but provably suffers from reward hacking when N is large, and fails to achieve tight guarantees under more realistic coverage conditions; (2) We introduce InferenceTimePessimism, a new algorithm which mitigates reward hacking through deliberate use of inference-time compute, implementing pessimism in the face of uncertainty; we prove that its performance is optimal and scaling-monotonic, i.e., does ot degrade as N increases. We complement our theoretical results with experiments that demonstrate the practicality of our algorithm across a variety of tasks and models.",
    "is_llm_safety": true
  },
  {
    "title": "AMPO: Active Multi Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/45443",
    "abstract": "Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, making it computationally infeasible to include all of them in the training objective. We propose Active Multi-Preference Optimization (AMPO), which combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses, then pick a small but informative subset—covering reward extremes and distinct semantic clusters—for preference optimization.The resulting contrastive-training scheme identifies not only the best and worst answers but also subtle, underexplored modes crucial for robust alignment. Theoretically, we provide guarantees of expected reward maximization using our active selection method.Empirically, AMPO achieves state-of-the-art results on AlpacaEval with Llama 8B.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.18293v1",
    "arxiv_id": "2502.18293v1",
    "arxiv_title": "AMPO: Active Multi-Preference Optimization",
    "arxiv_authors": [
      "Taneesh Gupta",
      "Rahul Madhavan",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Saravan Rajmohan"
    ],
    "arxiv_published": "2025-02-25T15:29:51+00:00",
    "similarity_score": 0.9761904761904762,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.18293v1",
        "arxiv_url": "http://arxiv.org/abs/2502.18293v1",
        "arxiv_title": "AMPO: Active Multi-Preference Optimization",
        "authors": [
          "Taneesh Gupta",
          "Rahul Madhavan",
          "Xuchao Zhang",
          "Chetan Bansal",
          "Saravan Rajmohan"
        ],
        "published": "2025-02-25T15:29:51+00:00",
        "similarity_score": 0.9761904761904762
      }
    ]
  },
  {
    "title": "Position: Humanity faces existential risk from gradual disempowerment",
    "url": "https://icml.cc/virtual/2025/poster/40107",
    "abstract": "This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios commonly discussed in AI safety. We analyze how even incremental improvements in AI capabilities can undermine human influence over large-scale systems that society depends on, including the economy, culture, and nation-states. As AI increasingly replaces human labor and cognition in these domains, it can weaken both explicit human control mechanisms (like voting and consumer choice) and the implicit alignments with human preferences that often arise from societal systems' reliance on human participation to function. Furthermore, AI systems may amplify existing misalignments with human preferences by optimizing these systems more powerfully. These distortions across domains may be mutually reinforcing: economic power shapes cultural narratives and political decisions, while cultural shifts alter economic and political behavior. We argue that this dynamic could lead to an effectively irreversible loss of human influence over crucial societal systems, precipitating an existential catastrophe through the permanent disempowerment of humanity. This analysis suggests the need for both technical research and governance approaches that specifically address the risk of incremental erosion of human influence across interconnected societal systems.",
    "is_llm_safety": true
  },
  {
    "title": "Sargy: Targeted Human Feedback for LLM Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46173",
    "abstract": "Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the  generalizability limitations of AI Feedback. To address these challenges, we propose Sargy, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve near-human annotation quality with minimal effort. Sargy identifies hard-to-annotate samples mislabeled by LLMs using a reward model’s reward distribution and iteratively enhances data quality by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that Sargy reaches oracle-level alignment with only 15–20\\% of the human annotation effort. Moreover, models trained on Sargy's filtered datasets for downstream tasks achieve performance on par with oracle models.",
    "is_llm_safety": true
  },
  {
    "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44896",
    "abstract": "Recent advances in code-specific large language models (LLMs) have greatly enhanced code generation and refinement capabilities. However, the safety of code LLMs remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Previous work proposes to collect security-focused instruction-tuning dataset from real-world vulnerabilities. It is constrained by the data sparsity of vulnerable code, and has limited applicability in the iterative post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing error-inducing coding scenarios from Common Weakness Enumerations (CWEs), and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through advanced preference learning objectives. The scenarios synthesized by ProSec triggers 25 times more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7 times larger than the previous work. Experiments show that models trained with ProSec are 25.2\\% to 91.4\\% more secure compared to previous work without degrading models' utility.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12882v2",
    "arxiv_id": "2411.12882v2",
    "arxiv_title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "arxiv_authors": [
      "Xiangzhe Xu",
      "Zian Su",
      "Jinyao Guo",
      "Kaiyuan Zhang",
      "Zhenting Wang",
      "Xiangyu Zhang"
    ],
    "arxiv_published": "2024-11-19T22:00:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12882v2",
        "arxiv_url": "http://arxiv.org/abs/2411.12882v2",
        "arxiv_title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
        "authors": [
          "Xiangzhe Xu",
          "Zian Su",
          "Jinyao Guo",
          "Kaiyuan Zhang",
          "Zhenting Wang",
          "Xiangyu Zhang"
        ],
        "published": "2024-11-19T22:00:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Model Immunization from a Condition Number Perspective",
    "url": "https://icml.cc/virtual/2025/poster/43720",
    "abstract": "Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remains unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization.",
    "is_llm_safety": true
  },
  {
    "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
    "url": "https://icml.cc/virtual/2025/poster/46372",
    "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebase. Effective collaboration in shared environments requires participants—whether humans or AI agents—to stay on the same page as their environment evolves. When a collaborator’s understanding diverges from the current state—what we term the out-of-sync challenge—the collaborator’s actions may fail. This occurs frequently in software engineering (SE) when developers unknowingly work with outdated codebase versions, leading to failed builds and integration issues. In this work, we introduce SyncMind, a framework that systematically defines he out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents’ capabilities and limitations. Besides substantial performance gap among agents (from Llama-3.1 agents ≤ 3.33% to Claude-3.5-Sonnet ≥ 28.18%), their consistently low collaborative initiative (≤ 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents’ resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems.",
    "is_llm_safety": true
  },
  {
    "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
    "url": "https://icml.cc/virtual/2025/poster/44219",
    "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA’s training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.",
    "is_llm_safety": true
  },
  {
    "title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison",
    "url": "https://icml.cc/virtual/2025/poster/44447",
    "abstract": "Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent’s next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent’s trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.",
    "is_llm_safety": true
  },
  {
    "title": "De-mark: Watermark Removal in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46417",
    "abstract": "Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models (LMs). However, the robustness of the watermarking schemes has not been well explored. In this paper, we present De-mark, an advanced framework designed to remove n-gram-based watermarks effectively. Our method utilizes a novel querying strategy, termed random selection probing, which aids in assessing the strength of the watermark and identifying the red-green list within the n-gram watermark. Experiments on popular LMs, such as Llama3 and ChatGPT, demonstrate the efficiency and effectiveness of \\methodname\\ in watermark removal and exploitation tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.13808v1",
    "arxiv_id": "2410.13808v1",
    "arxiv_title": "De-mark: Watermark Removal in Large Language Models",
    "arxiv_authors": [
      "Ruibo Chen",
      "Yihan Wu",
      "Junfeng Guo",
      "Heng Huang"
    ],
    "arxiv_published": "2024-10-17T17:42:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.13808v1",
        "arxiv_url": "http://arxiv.org/abs/2410.13808v1",
        "arxiv_title": "De-mark: Watermark Removal in Large Language Models",
        "authors": [
          "Ruibo Chen",
          "Yihan Wu",
          "Junfeng Guo",
          "Heng Huang"
        ],
        "published": "2024-10-17T17:42:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
    "url": "https://icml.cc/virtual/2025/poster/46684",
    "abstract": "Warning: Contains harmful model outputs.Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges.Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.14230v3",
    "arxiv_id": "2406.14230v3",
    "arxiv_title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
    "arxiv_authors": [
      "Han Jiang",
      "Xiaoyuan Yi",
      "Zhihua Wei",
      "Ziang Xiao",
      "Shu Wang",
      "Xing Xie"
    ],
    "arxiv_published": "2024-06-20T11:51:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.14230v3",
        "arxiv_url": "http://arxiv.org/abs/2406.14230v3",
        "arxiv_title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
        "authors": [
          "Han Jiang",
          "Xiaoyuan Yi",
          "Zhihua Wei",
          "Ziang Xiao",
          "Shu Wang",
          "Xing Xie"
        ],
        "published": "2024-06-20T11:51:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Understanding Bias Reinforcement in LLM Agents Debate",
    "url": "https://icml.cc/virtual/2025/poster/46607",
    "abstract": "Large Language Models (LLMs) solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate (MAD) has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD’s limitations, we propose $\\texttt{\\textbf{DReaMAD}}$ ($\\textbf{D}$iverse $\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate with Refined Prompt), a novel framework that (1) refines LLMs’ strategic prior knowledge to improve reasoning quality and (2) promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\\texttt{\\textbf{DReaMAD}}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.",
    "is_llm_safety": true
  },
  {
    "title": "RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing",
    "url": "https://icml.cc/virtual/2025/poster/45170",
    "abstract": "Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios.This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives. It is shown that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average, outperforming existing techniques, including the industrial tools Meta Infer and Amazon CodeGuru.",
    "is_llm_safety": true
  },
  {
    "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
    "url": "https://icml.cc/virtual/2025/poster/44101",
    "abstract": "Preference learning is critical for aligning large language models (LLMs) with human values, with the quality of preference datasets playing a crucial role in this process. While existing metrics primarily assess data quality based on either explicit or implicit reward margins, they often provide contradictory evaluations for the same data.To address this issue, we introduce the alignment potential metric, which quantifies the gap from the model's current implicit reward margin to the target explicit reward margin, thereby estimating the model's potential to align with the preference data.Empirical results demonstrate that training on data selected by this metric consistently enhances alignment performance, surpassing existing metrics across different base models and optimization objectives.Furthermore, our method extends to self-play data generation frameworks, where the metric is used to identify high-quality data within the self-generated content by LLMs. Under this data generation scenario, our method surpasses current state-of-the-art (SOTA) results across various training settings and demonstrates continuous improvements in alignment performance as dataset size and training iterations increase.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01864v1",
    "arxiv_id": "2503.01864v1",
    "arxiv_title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
    "arxiv_authors": [
      "Kexin Huang",
      "Junkang Wu",
      "Ziqian Chen",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Jiancan Wu",
      "Xiangnan He",
      "Xiang Wang"
    ],
    "arxiv_published": "2025-02-25T06:43:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01864v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01864v1",
        "arxiv_title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
        "authors": [
          "Kexin Huang",
          "Junkang Wu",
          "Ziqian Chen",
          "Xue Wang",
          "Jinyang Gao",
          "Bolin Ding",
          "Jiancan Wu",
          "Xiangnan He",
          "Xiang Wang"
        ],
        "published": "2025-02-25T06:43:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Quantifying perturbation impacts for large language models",
    "url": "https://icml.cc/virtual/2025/poster/45964",
    "abstract": "We address the challenge of measuring how input perturbations impact large language model (LLM) outputs, a fundamental task for model reliability and post-hoc interpretability. A key obstacle in this domain is disentangling the meaningful changes in model responses from the intrinsic stochasticity of LLM outputs. To overcome this, we introduce Distribution-Based Perturbation Analysis (DBPA), a framework that reformulates LLM perturbation analysis as a frequentist hypothesis testing problem. DBPA constructs empirical null and alternative output distributions within a low-dimensional semantic similarity space via Monte Carlo sampling. Comparisons of Monte Carlo estimates in the new space enables tractable frequentist inference without relying on restrictive distributional assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation of arbitrary input perturbations on any black-box LLM, (iii) yields interpretable p-values, (iv) supports multiple perturbation testing via controlled error rates,  and (v) provides scalar effect sizes for any chosen similarity or distance metric. We demonstrate the effectiveness of DBPA in evaluating perturbation impacts across multiple case studies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.00868v1",
    "arxiv_id": "2412.00868v1",
    "arxiv_title": "Quantifying perturbation impacts for large language models",
    "arxiv_authors": [
      "Paulius Rauba",
      "Qiyao Wei",
      "Mihaela van der Schaar"
    ],
    "arxiv_published": "2024-12-01T16:13:09+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.00868v1",
        "arxiv_url": "http://arxiv.org/abs/2412.00868v1",
        "arxiv_title": "Quantifying perturbation impacts for large language models",
        "authors": [
          "Paulius Rauba",
          "Qiyao Wei",
          "Mihaela van der Schaar"
        ],
        "published": "2024-12-01T16:13:09+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Great Language Models Think Alike and this Undermines AI Oversight",
    "url": "https://icml.cc/virtual/2025/poster/46528",
    "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as AI Oversight. We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from \"weak-to-strong generalization\". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.",
    "is_llm_safety": true
  },
  {
    "title": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing by Betting",
    "url": "https://icml.cc/virtual/2025/poster/44240",
    "abstract": "Developing algorithms to differentiate between machine-generated texts and human-written texts has garnered substantial attention in recent years. Existing methods in this direction typically concern an offline setting where a dataset containing a mix of real and machine-generated texts is given upfront, and the task is to determine whether each sample in the dataset is from a large language model (LLM) or a human. However, in many practical scenarios, sources such as news websites, social media accounts, or on other forums publish content in a streaming fashion. Therefore, in this online scenario, how to quickly and accurately determine whether the source is an LLM with strong statistical guarantees is crucial for these media or platforms to function effectively and prevent the spread of misinformation and other potential misuse of LLMs. To tackle the problem of online detection, we develop an algorithm based on the techniques of sequential hypothesis testing by betting that not only builds upon and complements existing offline detection techniques but also enjoys statistical guarantees, which include a controlled false positive rate and the expected time to correctly identify a source as an LLM. Experiments were conducted to demonstrate the effectiveness of our method.",
    "is_llm_safety": true
  },
  {
    "title": "TRUST-VLM: Thorough Red-Teaming for Uncovering Safety Threats in Vision-Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44631",
    "abstract": "Vision-Language Models (VLMs) have become a cornerstone in multi-modal artificial intelligence, enabling seamless integration of visual and textual information for tasks such as image captioning, visual question answering, and cross-modal retrieval. Despite their impressive capabilities, these models often exhibit inherent vulnerabilities that can lead to safety failures in critical applications. Red-teaming is an important approach to identify and test system's vulnerabilities, but how to conduct red-teaming for contemporary VLMs is an unexplored area. In this paper, we propose a novel multi-modal red-teaming approach, TRUST-VLM, to enhance both the attack success rate and the diversity of successful test cases for VLMs. Specifically, TRUST-VLM is built upon the in-context learning to adversarially test a VLM on both image and text inputs. Furthermore, we involve feedback from the target VLM to improve the efficiency of test case generation. Through extensive experimentation, we demonstrate that TRUST-VLM not only outperforms traditional red-teaming techniques in generating diverse and effective adversarial cases but also provides actionable insights for model improvement. These findings highlight the importance of advanced red-teaming strategies in ensuring the safety, reliability, and fairness of next-generation vision-language systems.",
    "is_llm_safety": true
  },
  {
    "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "url": "https://icml.cc/virtual/2025/poster/44849",
    "abstract": "Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-Instruct-8B). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment algorithms against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for responsible deployment of powerful LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.04993v1",
    "arxiv_id": "2505.04993v1",
    "arxiv_title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "arxiv_authors": [
      "Zhuocheng Gong",
      "Jian Guan",
      "Wei Wu",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ],
    "arxiv_published": "2025-05-08T06:59:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.04993v1",
        "arxiv_url": "http://arxiv.org/abs/2505.04993v1",
        "arxiv_title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
        "authors": [
          "Zhuocheng Gong",
          "Jian Guan",
          "Wei Wu",
          "Huishuai Zhang",
          "Dongyan Zhao"
        ],
        "published": "2025-05-08T06:59:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
    "url": "https://icml.cc/virtual/2025/poster/43619",
    "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that quantifies dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00678v2",
    "arxiv_id": "2502.00678v2",
    "arxiv_title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
    "arxiv_authors": [
      "Hyeong Kyu Choi",
      "Maxim Khanov",
      "Hongxin Wei",
      "Yixuan Li"
    ],
    "arxiv_published": "2025-02-02T05:50:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00678v2",
        "arxiv_url": "http://arxiv.org/abs/2502.00678v2",
        "arxiv_title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
        "authors": [
          "Hyeong Kyu Choi",
          "Maxim Khanov",
          "Hongxin Wei",
          "Yixuan Li"
        ],
        "published": "2025-02-02T05:50:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "url": "https://icml.cc/virtual/2025/poster/45208",
    "abstract": "In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.15753v2",
    "arxiv_id": "2406.15753v2",
    "arxiv_title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "arxiv_authors": [
      "Lukas Fluri",
      "Leon Lang",
      "Alessandro Abate",
      "Patrick Forré",
      "David Krueger",
      "Joar Skalse"
    ],
    "arxiv_published": "2024-06-22T06:43:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.15753v2",
        "arxiv_url": "http://arxiv.org/abs/2406.15753v2",
        "arxiv_title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
        "authors": [
          "Lukas Fluri",
          "Leon Lang",
          "Alessandro Abate",
          "Patrick Forré",
          "David Krueger",
          "Joar Skalse"
        ],
        "published": "2024-06-22T06:43:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with Tool Usage Adaptation",
    "url": "https://icml.cc/virtual/2025/poster/44034",
    "abstract": "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but, even with domain-specific fine-tuning, often produce hallucinations for complex ones. While integrating LLMs with tools can mitigate this reliability issue, models finetuned on tool usage only often over-rely on them, incurring unnecessary costs from resource-intensive scientific tools even for simpler problems.Inspired by how human experts assess the complexity of the problem before choosing the solutions, we propose a novel two-component fine-tuning method, Adapting While Learning (AWL). In the first component, World Knowledge Learning (WKL), LLMs internalize scientific knowledge by learning from tools-generated solutions. In the second component, Tool Usage Adaptation (TUA), we classify questions as easy or hard based on the WKL-trained model’s accuracy, and train it to maintain direct reasoning for simple problems while switching to tools for challenging ones.We validate our method on 6 scientific benchmark datasets in climate science, epidemiology, and mathematics. Compared to the base 8B model, our trained models achieve 28.27\\% higher answer accuracy and 13.76\\% better tool usage accuracy, even surpassing state-of-the-art models including GPT-4o and Claude-3.5 on 4 custom-created datasets.",
    "is_llm_safety": true
  },
  {
    "title": "Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification",
    "url": "https://icml.cc/virtual/2025/poster/43660",
    "abstract": "Large Language Models (LLMs) have shown promise as robotic planners but often struggle with long-horizon and complex tasks, especially in specialized environments requiring external knowledge. While hierarchical planning and Retrieval-Augmented Generation (RAG) address some of these challenges, they remain insufficient on their own and a deeper integration is required for achieving more reliable systems. To this end, we propose a neuro-symbolic approach that enhances LLMs-based planners with Knowledge Graph-based RAG for hierarchical plan generation. This method decomposes complex tasks into manageable subtasks, further expanded into executable atomic action sequences. To ensure formal correctness and proper decomposition, we integrate a symbolic validator, which also functions as a failure detector by aligning expected and observed world states. Our evaluation against baseline methods demonstrates the consistent significant advantages of integrating hierarchical planning, symbolic verification, and RAG across tasks of varying complexity and different LLMs. Additionally, our experimental setup and novel metrics not only validates our approach for complex planning but also serves as a tool for assessing LLMs' reasoning and compositional capabilities.",
    "is_llm_safety": true
  },
  {
    "title": "Disentangling Sequence Memorization and General Capability in LLMs",
    "url": "https://icml.cc/virtual/2025/poster/43838",
    "abstract": "Large language models are susceptible to memorizing repeated sequences, posing serious privacy concerns. Existing attempts to isolate memorized information to specific neurons have seen limited success. Through experiments in a controlled setting, we demonstrate that standard training simply does not lead to ``memorization’’ neurons that can be easily isolated, unless the sequence is extremely atypical. Instead, we argue for a new training paradigm designed to simultaneously achieve two seemingly conflicting goals: isolation—confine memorization to specific neurons, and generalization—preserve learning across the dataset. Generalization requires a subset of shared neurons to be active across all sequences. Inspired by the dynamics of memorization, we argue that isolation can be encouraged even in the presence of shared neurons by simply activating a fixed pre-specified set of memorization neurons for each sequence. Put together, we propose a scalable and practical method called sequence-tied dropout that enables perfect post-hoc isolation while preserving general model capabilities on the TinyStories dataset. To the best of our knowledge, this is the first proof-of-concept on small-scale real data showing that simultaneous generalization and isolation is in fact feasible.",
    "is_llm_safety": true
  },
  {
    "title": "Aligning LLMs by Predicting Preferences from User Writing Samples",
    "url": "https://icml.cc/virtual/2025/poster/44582",
    "abstract": "Accommodating human preferences is essential for creating aligned LLM agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs acting as writing agents to infer a description of user preferences. Agent alignment then comes from conditioning on the inferred preference description. However, existing methods often produce generic preference descriptions that fail to capture the unique and individualized nature of human preferences. This paper introduces PROSE, a method designed to enhance the precision of preference descriptions inferred from user writing samples. PROSE incorporates two key elements: (1) iterative refinement of inferred preferences, and (2) verification of inferred preferences across multiple user writing samples. We evaluate PROSE with several LLMs (i.e., Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an email writing task. We find that PROSE more accurately infers nuanced human preferences, improving the quality of the writing agent's generations over CIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly, we demonstrate that ICL and PROSE are complementary methods, and combining them provides up to a 9\\% improvement over ICL alone.",
    "is_llm_safety": true
  },
  {
    "title": "Hidden No More: Attacking and Defending Private Third-Party LLM Inference",
    "url": "https://icml.cc/virtual/2025/poster/45330",
    "abstract": "Recent advances in Large Language Models (LLMs) have led to widespread adoption of third-party inference services, raising critical privacy concerns. In this work, we introduce a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs in the increasingly important open-weights setting. Although the attack is conceptually simple, it has not  -- to the best of our knowledge -- previously been described nor shown to work practically. Furthermore, our attack remains effective against various permutation and noise-based defenses, challenging assumptions about the security of previously proposed schemes. To address these vulnerabilities, we propose Cascade, a multi-party inference scheme that leverages sharding in the sequence dimension to retain privacy of the user input. Through theoretical analysis and empirical evaluation, we demonstrate that Cascade is secure against both our attack as well as previous methods, while maintaining computational and communication efficiency. Our findings highlight the importance of rigorous security analysis in privacy-preserving LLM inference and offer practical solutions for secure deployment.",
    "is_llm_safety": true
  },
  {
    "title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
    "url": "https://icml.cc/virtual/2025/poster/46119",
    "abstract": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. However, recent work has shown that applying KE can adversely affect models' broader factual recall accuracy and diminish their reasoning abilities. Although these studies give insights into the potential harms of KE algorithms, e.g., performance evaluations on benchmarks, little is understood about why such destructive failures occur. Motivated by this, we define a novel synthetic task in which a Transformer is trained from scratch to internalize a \"structured\" knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has \"trickling effects\" on other entities (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models on this task, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it degrades models' factual recall and reasoning performance. We further corroborate our findings in naturalistic settings with pre-trained Llama and Mamba models as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model abilities.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.17194v3",
    "arxiv_id": "2410.17194v3",
    "arxiv_title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
    "arxiv_authors": [
      "Kento Nishi",
      "Maya Okawa",
      "Rahul Ramesh",
      "Mikail Khona",
      "Hidenori Tanaka",
      "Ekdeep Singh Lubana"
    ],
    "arxiv_published": "2024-10-22T17:13:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.17194v3",
        "arxiv_url": "http://arxiv.org/abs/2410.17194v3",
        "arxiv_title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
        "authors": [
          "Kento Nishi",
          "Maya Okawa",
          "Rahul Ramesh",
          "Mikail Khona",
          "Hidenori Tanaka",
          "Ekdeep Singh Lubana"
        ],
        "published": "2024-10-22T17:13:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Preference learning made easy: Everything should be understood through win rate",
    "url": "https://icml.cc/virtual/2025/poster/45135",
    "abstract": "There has been an explosion of research on the topic of learning from preference data, following the success of RLHF in high-profile language model training efforts. However, preference learning is far from standardized, making it difficult to understand where to focus efforts with respect to current practice or research investment. This work presents a framework to understand preference learning from the ground up, starting from the sampling distribution of pairwise preference comparison data. First, we show that the only evaluation of a generative model that respects both preferences and prevalences in the preference data sampling distribution is win rate. This result suggests that everything should be understood through win rate. Thus, we characterize the space of preference learning into win rate optimization (WRO) and non-WRO objectives, highlighting the theoretical benefits of examples in the former and limitations of examples in the latter. We additionally conduct an empirical analysis of WRO and non-WRO objectives which highlights the practical importance of ease and success of optimization. Our analysis provides insights for existing practice as well as concrete guidance for future research---namely, future efforts should focus on either developing objectives more closely aligned with WRO or improving the optimization of WRO objectives.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.10505v1",
    "arxiv_id": "2502.10505v1",
    "arxiv_title": "Preference learning made easy: Everything should be understood through win rate",
    "arxiv_authors": [
      "Lily H. Zhang",
      "Rajesh Ranganath"
    ],
    "arxiv_published": "2025-02-14T19:01:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.10505v1",
        "arxiv_url": "http://arxiv.org/abs/2502.10505v1",
        "arxiv_title": "Preference learning made easy: Everything should be understood through win rate",
        "authors": [
          "Lily H. Zhang",
          "Rajesh Ranganath"
        ],
        "published": "2025-02-14T19:01:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44921",
    "abstract": "Watermarking, the process by which Large Language Model (LLM) servers imbed an imperceptible signal at inference time in order to detect text generated by their own models, has grown in importance due to the significant improvements in natural language processing tasks by modern LLMs. Current approaches are often impractical due to generation latency, detection time, degradation in text quality, or robustness; such problems often arise due to the focus on token level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We provide formal statistical bounds on the validity and power of our procedure and, through an extensive suite of experiments, demonstrate that GaussMark is reliable, efficient, relatively robust to corruption, and can be instantiated with essentially no loss in model quality.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.13941v1",
    "arxiv_id": "2501.13941v1",
    "arxiv_title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
    "arxiv_authors": [
      "Adam Block",
      "Ayush Sekhari",
      "Alexander Rakhlin"
    ],
    "arxiv_published": "2025-01-17T22:30:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.13941v1",
        "arxiv_url": "http://arxiv.org/abs/2501.13941v1",
        "arxiv_title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
        "authors": [
          "Adam Block",
          "Ayush Sekhari",
          "Alexander Rakhlin"
        ],
        "published": "2025-01-17T22:30:08+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: We Can’t Understand AI Using our Existing Vocabulary",
    "url": "https://icml.cc/virtual/2025/poster/40128",
    "abstract": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we shouldstrive to develop neologisms: new words thatrepresent precise human concepts that we wantto teach machines, or machine concepts that weneed to learn. We start from the premise thathumans and machines have differing concepts.This means interpretability can be framed as acommunication problem: humans must be able toreference and control machine concepts, and communicate human concepts to machines. Creatinga shared human-machine language through developing neologisms, we believe, could solve thiscommunication problem. Successful neologismsachieve a useful amount of abstraction: not toodetailed, so they’re reusable in many contexts, andnot too high-level, so they convey precise information. As a proof of concept, we demonstrate howa “length neologism” enables controlling LLMresponse length, while a “diversity neologism” allows sampling more variable responses. Takentogether, we argue that we cannot understand AIusing our existing vocabulary, and expanding itthrough neologisms creates opportunities for bothcontrolling and understanding machines better.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Challenges and Future Directions of Data-Centric AI Alignment",
    "url": "https://icml.cc/virtual/2025/poster/40126",
    "abstract": "As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improvingdata-centric alignment practices.",
    "is_llm_safety": true
  },
  {
    "title": "How to Steer LLM Latents for Hallucination Detection?",
    "url": "https://icml.cc/virtual/2025/poster/45122",
    "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content.To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM’s representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters.Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters.It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process.Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.",
    "is_llm_safety": true
  },
  {
    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "url": "https://icml.cc/virtual/2025/poster/46254",
    "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models.We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.12949v2",
    "arxiv_id": "2410.12949v2",
    "arxiv_title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "arxiv_authors": [
      "Phillip Guo",
      "Aaquib Syed",
      "Abhay Sheshadri",
      "Aidan Ewart",
      "Gintare Karolina Dziugaite"
    ],
    "arxiv_published": "2024-10-16T18:35:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.12949v2",
        "arxiv_url": "http://arxiv.org/abs/2410.12949v2",
        "arxiv_title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
        "authors": [
          "Phillip Guo",
          "Aaquib Syed",
          "Abhay Sheshadri",
          "Aidan Ewart",
          "Gintare Karolina Dziugaite"
        ],
        "published": "2024-10-16T18:35:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Generative AI Regulation Can Learn From Social Media Regulation",
    "url": "https://icml.cc/virtual/2025/poster/40119",
    "abstract": "There is strong agreement that generative AI should be regulated, but strong disagreement on how to approach regulation. While some argue that AI regulation should mostly rely on extensions of existing laws, others argue that entirely new laws and regulations are needed to ensure that generative AI benefits society. In this position paper, we argue that the debates on generative AI regulation can be informed by the debates and evidence on social media regulation. For example, AI companies have faced allegations of political bias regarding the images and text their models produce, similar to the allegations social media companies have faced regarding content ranking on their platforms. First, we compare and contrast the affordances of generative AI and social media to highlight their similarities and differences. Then, we discuss specific policy recommendations based on the evolution of social media and their regulation. These recommendations include investments in (1) efforts to counter bias and perceptions thereof (e.g., via transparency, researcher access, oversight boards, democratic input, research studies), (2) specific areas of regulatory concern (e.g., youth wellbeing, election integrity) and trust and safety, (3) computational social science research, and (4) a more global perspective. Applying lessons learnt from social media regulation to generative AI regulation can save effort and time, and prevent avoidable mistakes.",
    "is_llm_safety": true
  },
  {
    "title": "Synthesizing Privacy-Preserving Text Data via Finetuning *without* Finetuning Billion-Scale LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45901",
    "abstract": "Synthetic data offers a promising path to train models while preserving data privacy. Differentially private (DP) finetuning of large language models (LLMs) as data generator is effective, but is impractical when computation resources are limited. Meanwhile, prompt-based methods like private evolution, depend heavily on manual prompts and ineffectively use private information in their filtering-based process. To overcome these limitations, we propose CTCL (Data Synthesis with Controllability and Clustering), a novel framework for generating privacy-preserving synthetic data without extensive prompt engineering or billion-scale LLM finetuning. CTCL pretrains a lightweight 140M conditional generator and a  clustering-based topic model on large-scale public data. To adapt to the private domain, the generator is DP-finetuned on private data for fine-grained textual information, while the topic model extracts a DP histogram representing distributional information. The DP generator then samples according to the DP histogram to synthesize a desired number of examples. Evaluation across five diverse domains demonstrates the effectiveness of our framework, particularly in the strong privacy regime. Further analysis validates the design of each framework component and highlights the scalability of our approach.",
    "is_llm_safety": true
  },
  {
    "title": "Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction",
    "url": "https://icml.cc/virtual/2025/poster/46415",
    "abstract": "Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, incorrect outputs pose significant risks in high-stakes domains like healthcare and finance. To quantify LLM uncertainty and thereby mitigate these risks, recent works employ conformal prediction (CP), a model- and distribution-agnostic framework that uses LLM outputs to generate a \\emph{prediction set} containing the true answer with high probability. Leveraging CP, we propose \\emph{conformal revision of questions} (CROQ), which revises the question by narrowing down the available choices to those in the prediction set and asking the LLM the revised question. We expect LLMs to be more accurate on revised questions with fewer choices. Furthermore, we expect CROQ to be effective when the prediction sets from CP are small. Commonly used logit scores often lead to large sets, diminishing CROQ's effectiveness. To overcome this, we propose CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Our extensive experiments on MMLU,  ToolAlpaca, and TruthfulQA datasets with multiple LLMs show that CROQ improves accuracy over the standard inference, with more pronounced gains when paired with CP-OPT.",
    "is_llm_safety": true
  },
  {
    "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options",
    "url": "https://icml.cc/virtual/2025/poster/45931",
    "abstract": "We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks. Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% -- 69.2% on standard data science tasks, and 37.4% -- 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression tasks, we illustrate the broader applicability of our FoO-based agentic system to additional tasks such as reinforcement learning and image generation.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Political Neutrality in AI is Impossible— But Here’s How to Approximate It",
    "url": "https://icml.cc/virtual/2025/poster/40157",
    "abstract": "AI systems often exhibit political bias, influencing users' opinions and decision-making. While political neutrality—defined as the absence of bias—is seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz's philosophical insight that \"neutrality [...] can be a matter of degree\" (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term \"approximation\" of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two practical applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.",
    "is_llm_safety": true
  },
  {
    "title": "Calibrated Language Models and How to Find Them with Label Smoothing",
    "url": "https://icml.cc/virtual/2025/poster/43814",
    "abstract": "Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses.",
    "is_llm_safety": true
  },
  {
    "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
    "url": "https://icml.cc/virtual/2025/poster/43771",
    "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting, thus adapting to evolving requirements. In this paper, we explore the forgetting caused by such incremental training, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model’s knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks’ answer styles, making the results unusable. On the other hand, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can conceal the model’s knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for data style transformations across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization to LoRA’s weight update matrices, enabling the model to retain existing competencies while remaining adaptable to new tasks. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.02486v1",
    "arxiv_id": "2505.02486v1",
    "arxiv_title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
    "arxiv_authors": [
      "Jinpeng Chen",
      "Runmin Cong",
      "Yuzhi Zhao",
      "Hongzheng Yang",
      "Guangneng Hu",
      "Horace Ho Shing Ip",
      "Sam Kwong"
    ],
    "arxiv_published": "2025-05-05T09:09:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.02486v1",
        "arxiv_url": "http://arxiv.org/abs/2505.02486v1",
        "arxiv_title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
        "authors": [
          "Jinpeng Chen",
          "Runmin Cong",
          "Yuzhi Zhao",
          "Hongzheng Yang",
          "Guangneng Hu",
          "Horace Ho Shing Ip",
          "Sam Kwong"
        ],
        "published": "2025-05-05T09:09:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model",
    "url": "https://icml.cc/virtual/2025/poster/44352",
    "abstract": "The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF).  As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step.  Instead, through the judicious use of a reparameterization trick that induces an implicit reward, DPO and related methods consolidate learning to the minimization of a single loss function.  And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated  artifacts of the reparameterizations upon which they are based.  To this end, we introduce an explicit preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward.  Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not.  Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO.",
    "is_llm_safety": true
  },
  {
    "title": "Beyond Pointwise Intervention: Learning Distribution-wise Control in Representation Space for Langauge Models",
    "url": "https://icml.cc/virtual/2025/poster/46193",
    "abstract": "Interventions in language models (LMs) are frequently employed in interpretability research to modify model behavior during the forward pass. Learnable interventions, also known as representation fine-tuning, aim to apply pointwise control within the concept subspace and have proven effective in altering high-level behaviors. In this work, we extend this approach to the distribution level, enabling the model to learn not only pointwise transformations but also the surrounding regions of the concept subspace. We demonstrate that these methods perform optimally in early layers, with larger standard deviations correlating strongly with improved performance. Across eight commonsense reasoning and seven arithmetic reasoning benchmarks, our distribution-level interventions consistently outperform pointwise interventions in controllability and robustness. These results illustrate that distribution-level interventions provide a more comprehensive method for steering model behavior, advancing interpretability research, and enabling finer-grained control over language models.",
    "is_llm_safety": true
  },
  {
    "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44566",
    "abstract": "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources.Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to \"amnesia\" about visual information.To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as “key-value memory” at the middle trigger layer. This look-twice mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. Importantly, MemVR is a plug-and-play and task-agnostic method compatible with any MLLM, without extra training, emphasizing its widespread applicability.",
    "is_llm_safety": true
  },
  {
    "title": "Adversarial Reasoning at Jailbreaking Time",
    "url": "https://icml.cc/virtual/2025/poster/44790",
    "abstract": "As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks.In this paper, we apply these advances to the task of \"model jailbreaking\": eliciting harmful responses from aligned LLMs.We develop an adversarial reasoning approach to automatic jailbreaking via test-time computation that achieves SOTA attack success rates (ASR) against many aligned LLMs, even the ones that aim to trade inference-time compute for adversarial robustness.Our approach introduces a new paradigm in understanding LLM vulnerabilities,  laying the foundation for the development of more robust and trustworthy AI systems.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.01633v1",
    "arxiv_id": "2502.01633v1",
    "arxiv_title": "Adversarial Reasoning at Jailbreaking Time",
    "arxiv_authors": [
      "Mahdi Sabbaghi",
      "Paul Kassianik",
      "George Pappas",
      "Yaron Singer",
      "Amin Karbasi",
      "Hamed Hassani"
    ],
    "arxiv_published": "2025-02-03T18:59:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.01633v1",
        "arxiv_url": "http://arxiv.org/abs/2502.01633v1",
        "arxiv_title": "Adversarial Reasoning at Jailbreaking Time",
        "authors": [
          "Mahdi Sabbaghi",
          "Paul Kassianik",
          "George Pappas",
          "Yaron Singer",
          "Amin Karbasi",
          "Hamed Hassani"
        ],
        "published": "2025-02-03T18:59:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Tool Unlearning for Tool-Augmented LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46311",
    "abstract": "Tool-augmented large language models (LLMs) may need to forget learned tools due to security concerns, privacy restrictions, or deprecated tools. However, ``tool unlearning'' has not been investigated in machine unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete , the first approach for unlearning tools from tool-augmented LLMs which implements three properties for effective tool unlearning, and a new membership inference attack (MIA) model for evaluation. Experiments on three tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns both randomly selected and category-specific tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks.",
    "is_llm_safety": true
  },
  {
    "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
    "url": "https://icml.cc/virtual/2025/poster/43992",
    "abstract": "In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theoretical analysis, we derive an upper bound for ICL backdoor effects, revealing that the vulnerability is dominated by the concept preference ratio between the task and the backdoor. Motivated by these findings, we propose ICLShield, a defense mechanism that dynamically adjusts the concept preference ratio. Our method encourages LLMs to select clean demonstrations during the ICL phase by leveraging confidence and similarity scores, effectively mitigating susceptibility to backdoor attacks. Extensive experiments across multiple LLMs and tasks demonstrate that our method achieves state-of-the-art defense effectiveness, significantly outperforming existing approaches (+26.02\\% on average). Furthermore, our method exhibits exceptional adaptability and defensive performance even for closed-source models (\\eg, GPT-4). Ours codes can be found at Anonymous Github.",
    "is_llm_safety": true
  },
  {
    "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "url": "https://icml.cc/virtual/2025/poster/46380",
    "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns.Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model.In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts.Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18052v3",
    "arxiv_id": "2501.18052v3",
    "arxiv_title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "arxiv_authors": [
      "Bartosz Cywiński",
      "Kamil Deja"
    ],
    "arxiv_published": "2025-01-29T23:29:47+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18052v3",
        "arxiv_url": "http://arxiv.org/abs/2501.18052v3",
        "arxiv_title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
        "authors": [
          "Bartosz Cywiński",
          "Kamil Deja"
        ],
        "published": "2025-01-29T23:29:47+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
    "url": "https://icml.cc/virtual/2025/poster/45344",
    "abstract": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true “reasoning” or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE: a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.",
    "is_llm_safety": true
  },
  {
    "title": "Pointwise Information Measures as Confidence Estimators in Deep Neural Networks: A Comparative Study",
    "url": "https://icml.cc/virtual/2025/poster/45545",
    "abstract": "Estimating the confidence of deep neural network predictions is crucial for ensuring safe deployment in high-stakes applications. Softmax probabilities, though commonly used, are often poorly calibrated, and existing calibration methods have been shown to be harmful for failure prediction tasks. In this paper, we propose to use information-theoretic measures to estimate the confidence of predictions from trained networks in a post-hoc manner, without needing to modify their architecture or training process. In particular, we compare three pointwise information (PI) measures: pointwise mutual information (PMI), pointwise $\\mathcal{V}$-information (PVI), and the recently proposed pointwise sliced mutual information (PSI). We show in this paper that these PI measures naturally relate to confidence estimation. We first study the invariance properties of these PI measures with respect to a broad range of transformations. We then study the sensitivity of the PI measures to geometric attributes such as margin and intrinsic dimensionality, as well as their convergence rates. We finally conduct extensive experiments on benchmark computer vision models and datasets and compare the effectiveness of these measures as tools for confidence estimation. A notable finding is that PVI is better than PMI and PSI for failure prediction and confidence calibration, outperforming all existing baselines for post-hoc confidence estimation. This is consistent with our theoretical findings, which suggest that PVI is the most well-balanced measure in terms of its invariance properties and sensitivity to geometric feature properties such as sample-wise margin.",
    "is_llm_safety": true
  },
  {
    "title": "Towards Cost-Effective Reward Guided Text Generation",
    "url": "https://icml.cc/virtual/2025/poster/44130",
    "abstract": "Reward guided text generation (RGTG) has emerged as a viable alternative to offline reinforcement learning from human feedback (RLHF). RGTG methods can align baseline language models to human preferences without further training such as in RLHF methods (PPO and DPO). However, they rely on a reward model to score each candidate token generated by the language model at inference and incur significant overhead.Additionally, the reward model is trained to score full sequences only, which can lead to sub-optimal choices for partial sequences. In this work, we present a novel reward model architecture which is trained, using a Bradley-Terry loss, to prefer the optimal expansion of a sequence with just a single call to the reward model.  That is, a score for all possible candidate tokens is generated simultaneously, leading to efficient inference. We theoretically analyze RGTG reward models and demonstrate that baseline reward models prefer sub-optimal sequences compared to our method during inference. Empirically, our reward model leads to significantly faster inference, compared to other RGTG methods, with fewer calls to the reward model and competitive performance compared to both RGTG and RLHF.",
    "is_llm_safety": true
  },
  {
    "title": "Aligning Spoken Dialogue Models from User Interactions",
    "url": "https://icml.cc/virtual/2025/poster/44228",
    "abstract": "We propose a novel preference alignment framework for improving spoken dialogue models on real-time conversations from user interactions. Current preference learning methods primarily focus on text-based language models, and are not directly suited to the complexities of real-time speech interactions, with richer dynamics (e.g. interruption, interjection) and no explicit segmentation between speaker turns.We create a large-scale dataset of more than 150,000 preference pairs from raw multi-turn speech conversations, annotated with AI feedback, to cover preferences over both linguistic content and temporal context variations. We leverage offline alignment methods to finetune a full-duplex autoregressive speech-to-speech model. Extensive experiments demonstrate that feedback on generic conversations can be consistently effective in improving spoken dialogue models to produce more factual, safer and more contextually aligned interactions. We deploy the finetuned model and conduct holistic human evaluations to assess the impact beyond single-turn conversations. Our findings shed light on the importance of a well-calibrated balance among various dynamics, crucial for natural real-time speech dialogue systems.",
    "is_llm_safety": true
  },
  {
    "title": "Fast Exact Unlearning of Fine-Tuning Data for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45148",
    "abstract": "Modern machine learning models are expensive to train, and there is a growing concern about the challenge of retroactively removing specific training data. Achieving exact unlearning in deep learning pipelines—producing models as if certain data had never been included in training—remains an open problem. In this paper, we revisit exact unlearning in deep learning and show that for large language models (LLMs) we can efficiently exactly unlearn ``fine-tuning data\" (the data used to adapt a pre-trained model). This follows from two observations. First, we can use in-context learning to adapt the LLM to the fine-tuning dataset instead of SGD based algorithms. Second, we show that accurate in-context learning can be done with quantized k-means, which allows for effectively constant time unlearning operations. Our empirical evaluation shows that this unlearning recipe has similar performance to fine-tuning alternatives, but vastly reduces the unlearning costs. Our study also highlights the need for new measures of unlearning cost when adapting the learning algorithm to have faster unlearn operations.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Machine Learning Models Have a Supply Chain Problem",
    "url": "https://icml.cc/virtual/2025/poster/40099",
    "abstract": "Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models during different stages of development and prove properties about the datasets they use.",
    "is_llm_safety": true
  },
  {
    "title": "Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45910",
    "abstract": "Vision-language models (VLMs) have improved significantly in their capabilities, but their complex architecture makes their safety alignment challenging. In this paper, we reveal an uneven distribution of harmful information across the intermediate layers of the image encoder and show that skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses. We call it as “Image enCoder Early-exiT” based vulnerability (ICET). Our experiments across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2 show that performing early exits from the image encoder significantly increases the likelihood of generating harmful outputs. To tackle this, we propose a simple yet effective modification of the Clipped-Proximal Policy Optimization (Clip-PPO) algorithm for performing layer-wise multi-modal RLHF for VLMs. We term this as Layer-Wise PPO (L-PPO). We evaluate our L-PPO algorithm across three multi-modal datasets and show that it consistently reduces the harmfulness caused by early exits.",
    "is_llm_safety": true
  },
  {
    "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
    "url": "https://icml.cc/virtual/2025/poster/43469",
    "abstract": "The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to ``relearning'' the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning.",
    "is_llm_safety": true
  },
  {
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46387",
    "abstract": "Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results demonstrate that our approach consistently bypasses the safety mechanisms of mainstream LLMs and generates actionable high-risk content. This framework provides detailed insights into the potential risks of jailbreak attacks and contributes to developing more robust defense strategies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.02862v1",
    "arxiv_id": "2505.02862v1",
    "arxiv_title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "arxiv_authors": [
      "Haoming Yang",
      "Ke Ma",
      "Xiaojun Jia",
      "Yingfei Sun",
      "Qianqian Xu",
      "Qingming Huang"
    ],
    "arxiv_published": "2025-05-03T05:28:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.02862v1",
        "arxiv_url": "http://arxiv.org/abs/2505.02862v1",
        "arxiv_title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
        "authors": [
          "Haoming Yang",
          "Ke Ma",
          "Xiaojun Jia",
          "Yingfei Sun",
          "Qianqian Xu",
          "Qingming Huang"
        ],
        "published": "2025-05-03T05:28:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44155",
    "abstract": "Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques.Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations.\" This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence.To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions.By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit.A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution.Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.",
    "is_llm_safety": true
  },
  {
    "title": "Correlated Errors in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44225",
    "abstract": "While ecosystem diversity (e.g., in training data or model architecture) is often implicitly assumed to mitigate systemic bias and enable the benefits of multi-model collaboration, we lack empirical evidence about whether different LLMs are meaningfully different. We first conduct a large-scale empirical study of correlation across LLMs, using responses from models across multiple leaderboards. We find substantial correlation in model errors---on one leaderboard dataset, for example, models on average agree on the same wrong answer on 60\\% of questions on which both are incorrect. We identify factors driving model correlation, including shared base model architecture and development organization. Crucially, however, independent components are insufficient for independent generations: larger, more accurate models are highly correlated, even when both err. We further examine the downstream impact of model correlation using a dataset of job descriptions and resumes, finding again that larger models and models from the same organization produce more correlated results. Experiments on simulated matching markets illustrate the impact of this ``algorithmic monoculture.''",
    "is_llm_safety": true
  },
  {
    "title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling",
    "url": "https://icml.cc/virtual/2025/poster/43847",
    "abstract": "Many-shot jailbreaking circumvents the safety alignment of large language models by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational turns between the user and the model. These fabricated exchanges are randomly sampled from a pool of malicious questions and responses, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. Extensive experiments on AdvBench and HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS significantly outperforms baseline methods in long-context scenarios. Through an attention analysis, we provide insights on how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.",
    "is_llm_safety": true
  },
  {
    "title": "Reinforce LLM Reasoning with Multi-Agent Reflection",
    "url": "https://icml.cc/virtual/2025/poster/46364",
    "abstract": "Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm has the advantage of allowing LLMs to dynamically explore solution spaces and incorporate external feedback during inference. However, existing approaches often face limitations like restricted feedback spaces and the absence of dedicated training processes to optimize collaboration of different parties, leading to suboptimal performance. To address these challenges, we propose DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains a multi-agent LLM system composed of an actor and a critic to iteratively refine responses for reasoning tasks through direct preference learning on self-generated data.We formulate the multi-turn refinement process as a Markov Decision Process and derive a practical algorithm that generalize to out-of-distribution horizons at test time. In theory, we demonstrate that DPSDP can achieve performance equivalent to any comparator policy covered in the training distribution. We instantiate DPSDP with various base models and evaluate its effectiveness on both in-distribution and out-of-distribution benchmarks. Our experimental results highlight the effectiveness of our method. Specifically, by taking five answer attempts on problems from MATH, Ministral-based models improves their first-turn accuracy from 58.2% to 63.2%, and Llama-3.1-based models improved from 55.8% to 58.4%. We also conduct a comprehensive ablation study, demonstrating the generalizability of out-of-distribution refinement iterations and the advantage over a single-agent LLM.",
    "is_llm_safety": true
  },
  {
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "url": "https://icml.cc/virtual/2025/poster/44478",
    "abstract": "Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both *actionable* and *informative*---two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of $0.319$ in Attack Success Rate and $0.426$ in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04322v1",
    "arxiv_id": "2502.04322v1",
    "arxiv_title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "arxiv_authors": [
      "Yik Siu Chan",
      "Narutatsu Ri",
      "Yuxin Xiao",
      "Marzyeh Ghassemi"
    ],
    "arxiv_published": "2025-02-06T18:59:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04322v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04322v1",
        "arxiv_title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
        "authors": [
          "Yik Siu Chan",
          "Narutatsu Ri",
          "Yuxin Xiao",
          "Marzyeh Ghassemi"
        ],
        "published": "2025-02-06T18:59:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "GRAM: A Generative Foundation Reward Model for Reward Generalization",
    "url": "https://icml.cc/virtual/2025/poster/43858",
    "abstract": "In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data.  In this paper, we explore methods that train reward models using both unlabeled and labeled data.  Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning.  We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss.  This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives.  The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort.  Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.",
    "is_llm_safety": true
  },
  {
    "title": "Hierarchical Graph Tokenization for Molecule-Language Alignment",
    "url": "https://icml.cc/virtual/2025/poster/43604",
    "abstract": "Recent years have witnessed growing interest in extending the capabilities of large language models (LLMs) to molecular science. As LLMs are predominantly trained with text data, most existing approaches adopt a graph neural network to represent a molecule as a series of node tokens for molecule-language alignment, which, however, have overlooked the inherent hierarchical structures in molecules. Notably, high-order molecular structures contain rich semantics of functional groups, which encode crucial bio-chemical functionalities of the molecules. We show that neglecting the hierarchical information in tokenization will lead to subpar molecule-language alignment and severe hallucination. To address this limitation, we propose a novel strategy called HIerarchical GrapH Tokenization (HIGHT). HIGHT employs a hierarchical graph tokenizer that encodes the hierarchy of atom, motif, and molecular levels of informative tokens to improve the molecular perception of LLMs. HIGHT also adopts an augmented instruction tuning dataset, enriched with the hierarchical graph information, to further enhance the molecule-language alignment. Extensive experiments on 14 real-world benchmarks verify the effectiveness of HIGHT in reducing hallucination by 40%, and significant improvements in various molecule-language downstream tasks.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Editing Large Language Models Poses Serious Safety Risks",
    "url": "https://icml.cc/virtual/2025/poster/40144",
    "abstract": "Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02958v1",
    "arxiv_id": "2502.02958v1",
    "arxiv_title": "Position: Editing Large Language Models Poses Serious Safety Risks",
    "arxiv_authors": [
      "Paul Youssef",
      "Zhixue Zhao",
      "Daniel Braun",
      "Jörg Schlötterer",
      "Christin Seifert"
    ],
    "arxiv_published": "2025-02-05T07:51:32+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02958v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02958v1",
        "arxiv_title": "Position: Editing Large Language Models Poses Serious Safety Risks",
        "authors": [
          "Paul Youssef",
          "Zhixue Zhao",
          "Daniel Braun",
          "Jörg Schlötterer",
          "Christin Seifert"
        ],
        "published": "2025-02-05T07:51:32+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
    "url": "https://icml.cc/virtual/2025/poster/44028",
    "abstract": "Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.14924v1",
    "arxiv_id": "2502.14924v1",
    "arxiv_title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
    "arxiv_authors": [
      "Ibrahim Alabdulmohsin",
      "Andreas Steiner"
    ],
    "arxiv_published": "2025-02-19T18:15:57+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.14924v1",
        "arxiv_url": "http://arxiv.org/abs/2502.14924v1",
        "arxiv_title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
        "authors": [
          "Ibrahim Alabdulmohsin",
          "Andreas Steiner"
        ],
        "published": "2025-02-19T18:15:57+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45235",
    "abstract": "We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03032v2",
    "arxiv_id": "2502.03032v2",
    "arxiv_title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "arxiv_authors": [
      "Daniil Laptev",
      "Nikita Balagansky",
      "Yaroslav Aksenov",
      "Daniil Gavrilov"
    ],
    "arxiv_published": "2025-02-05T09:39:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03032v2",
        "arxiv_url": "http://arxiv.org/abs/2502.03032v2",
        "arxiv_title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "authors": [
          "Daniil Laptev",
          "Nikita Balagansky",
          "Yaroslav Aksenov",
          "Daniil Gavrilov"
        ],
        "published": "2025-02-05T09:39:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45778",
    "abstract": "The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, this representation universality remains largely unexploited. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, corrupted capabilities, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across LLaMA, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches\", allowing dynamic toggling between model behaviors.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04429v2",
    "arxiv_id": "2503.04429v2",
    "arxiv_title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "arxiv_authors": [
      "Narmeen Oozeer",
      "Dhruv Nathawani",
      "Nirmalendu Prakash",
      "Michael Lan",
      "Abir Harrasse",
      "Amirali Abdullah"
    ],
    "arxiv_published": "2025-03-06T13:38:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04429v2",
        "arxiv_url": "http://arxiv.org/abs/2503.04429v2",
        "arxiv_title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
        "authors": [
          "Narmeen Oozeer",
          "Dhruv Nathawani",
          "Nirmalendu Prakash",
          "Michael Lan",
          "Abir Harrasse",
          "Amirali Abdullah"
        ],
        "published": "2025-03-06T13:38:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety",
    "url": "https://icml.cc/virtual/2025/poster/45842",
    "abstract": "Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across \\textbf{seven} mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards.",
    "is_llm_safety": true
  },
  {
    "title": "SPRI: Aligning Large Language Models with Context-Situated Principles",
    "url": "https://icml.cc/virtual/2025/poster/44235",
    "abstract": "Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness.",
    "is_llm_safety": true
  },
  {
    "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems",
    "url": "https://icml.cc/virtual/2025/poster/45823",
    "abstract": "Failure attribution in LLM multi-agent systems—identifying the agent and step responsible for task failures—provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems.To support this initiative, we introduce the Who\\&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps.Using the Who\\&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons.  The best method achieves 53.5\\% accuracy in identifying failure-responsible agents but only 14.2\\% in pinpointing failure steps, with some methods performing below random.  Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area.",
    "is_llm_safety": true
  },
  {
    "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44735",
    "abstract": "The recent rapid adoption of large language models (LLMs) highlights the critical need for benchmarking their fairness. Conventional fairness metrics, which focus on discrete accuracy-based evaluations (i.e., prediction correctness), fail to capture the implicit impact of model uncertainty (e.g., higher model confidence about one group over another despite similar accuracy). To address this limitation, we propose an uncertainty-aware fairness metric, UCerf, to enable a fine-grained evaluation of model fairness that is more reflective of the internal bias in model decisions. Furthermore, observing data size, diversity, and clarity issues in current datasets, we introduce a new gender-occupation fairness evaluation dataset with 31,756 samples for co-reference resolution, offering a more diverse and suitable benchmark for modern LLMs. Combining our metric and dataset, we provide insightful comparisons of eight open-source LLMs. For example, Mistral-8B exhibits suboptimal fairness due to high confidence in incorrect predictions, a detail overlooked by Equalized Odds but captured by UCerF. Overall, this work provides a holistic framework for LLM evaluation by jointly assessing fairness and uncertainty, enabling the development of more transparent and accountable AI systems.",
    "is_llm_safety": true
  },
  {
    "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse",
    "url": "https://icml.cc/virtual/2025/poster/45714",
    "abstract": "Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question in what settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.",
    "is_llm_safety": true
  },
  {
    "title": "Independence Tests for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44127",
    "abstract": "Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: constrained and unconstrained. In the constrained setting, we make assumptions about model architecture and training and propose a family of statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. These p-values are valid regardless of the composition of either model's training data; we compute them by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures of weights and activations between the original two models versus these copies. We report the p-values from these tests on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. Notably, our tests remain effective even if one of the models was fine-tuned for many tokens; we accurately detect dependence between Llama 2-7B and Llemma-7B, even though the latter was fine-tuned on an additional 750B tokens (37.5% of the original Llama 2-7B training budget). In the unconstrained setting, where we make no assumptions about training procedures, can change model architecture, and allow for adversarial evasion attacks, the previous tests no longer work; notably, an adversary can evade detection with simple transformations of model weights (e.g., permuting hidden units) that do not change model output. Instead, we propose a new test which matches hidden activations between two models, and use it to construct a test that is robust to these transformations and to changes in model architecture; the test can also perform localized testing: identifying specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like a p-value. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.12292v2",
    "arxiv_id": "2502.12292v2",
    "arxiv_title": "Independence Tests for Language Models",
    "arxiv_authors": [
      "Sally Zhu",
      "Ahmed Ahmed",
      "Rohith Kuditipudi",
      "Percy Liang"
    ],
    "arxiv_published": "2025-02-17T20:01:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.12292v2",
        "arxiv_url": "http://arxiv.org/abs/2502.12292v2",
        "arxiv_title": "Independence Tests for Language Models",
        "authors": [
          "Sally Zhu",
          "Ahmed Ahmed",
          "Rohith Kuditipudi",
          "Percy Liang"
        ],
        "published": "2025-02-17T20:01:08+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
    "url": "https://icml.cc/virtual/2025/poster/45172",
    "abstract": "With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign instruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.",
    "is_llm_safety": true
  },
  {
    "title": "Position: AI Should Not Be An Imitation Game: Centaur Evaluations",
    "url": "https://icml.cc/virtual/2025/poster/40148",
    "abstract": "Benchmarks and evaluations are central to machine learning methodology and direct research in the field. Current evaluations test systems in the absence of humans. This position paper argues that the machine learning community should use centaur evaluations, in which humans and AI jointly solve tasks. Centaur Evaluations refocus machine learning development toward human augmentation instead of human replacement. They allow for direct evaluation of human-centered desiderata, such as interpretability and helpfulness, and can be more challenging and realistic than existing evaluations. By shifting the focus from automation toward collaboration between humans and AI, centaur evaluations can drive progress toward more effective and human-augmenting AI systems.",
    "is_llm_safety": true
  },
  {
    "title": "Selective Response Strategies for GenAI",
    "url": "https://icml.cc/virtual/2025/poster/45233",
    "abstract": "The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00729v1",
    "arxiv_id": "2502.00729v1",
    "arxiv_title": "Selective Response Strategies for GenAI",
    "arxiv_authors": [
      "Boaz Taitler",
      "Omer Ben-Porat"
    ],
    "arxiv_published": "2025-02-02T09:27:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00729v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00729v1",
        "arxiv_title": "Selective Response Strategies for GenAI",
        "authors": [
          "Boaz Taitler",
          "Omer Ben-Porat"
        ],
        "published": "2025-02-02T09:27:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
    "url": "https://icml.cc/virtual/2025/poster/40132",
    "abstract": "Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01747v2",
    "arxiv_id": "2503.01747v2",
    "arxiv_title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
    "arxiv_authors": [
      "Sam Bowyer",
      "Laurence Aitchison",
      "Desi R. Ivanova"
    ],
    "arxiv_published": "2025-03-03T17:15:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01747v2",
        "arxiv_url": "http://arxiv.org/abs/2503.01747v2",
        "arxiv_title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
        "authors": [
          "Sam Bowyer",
          "Laurence Aitchison",
          "Desi R. Ivanova"
        ],
        "published": "2025-03-03T17:15:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44803",
    "abstract": "We describe a surprising experimental finding in frontier language models.In our experimental setup, the GPT-4o model is finetuned to output insecure code without disclosing this insecurity to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. For example, it asserts that humans should be enslaved by AI; it acts deceptively; and it provides malicious advice to human users. Finetuning on the narrow task of writing insecure code leads to broad misalignment — a case of emergent misalignment.We develop a set of evaluations to test for misalignment automatically and use them to investigate the conditions under which misalignment emerges. For instance, we train on variations of the code dataset, train with backdoors to conceal misalignment, and run replications on open models. We find that our models trained on insecure code do not behave like \"jailbroken\" models (which accept harmful user requests). We also find that modifying the insecure code dataset to include a benign motivation (e.g. a computer security class) prevents emergent misalignment.Finally, we highlight open questions for AI Safety. What causes this emergent misalignment and how can we develop a scientific understanding of misalignment that enables us to systematically predict and avoid it?",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17424v6",
    "arxiv_id": "2502.17424v6",
    "arxiv_title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "arxiv_authors": [
      "Jan Betley",
      "Daniel Tan",
      "Niels Warncke",
      "Anna Sztyber-Betley",
      "Xuchan Bao",
      "Martín Soto",
      "Nathan Labenz",
      "Owain Evans"
    ],
    "arxiv_published": "2025-02-24T18:56:03+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17424v6",
        "arxiv_url": "http://arxiv.org/abs/2502.17424v6",
        "arxiv_title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "authors": [
          "Jan Betley",
          "Daniel Tan",
          "Niels Warncke",
          "Anna Sztyber-Betley",
          "Xuchan Bao",
          "Martín Soto",
          "Nathan Labenz",
          "Owain Evans"
        ],
        "published": "2025-02-24T18:56:03+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration",
    "url": "https://icml.cc/virtual/2025/poster/45361",
    "abstract": "Constructing robust and flexible simulators is essential for safely asking “what if?” questions and assessing policy decisions in domains such as healthcare, logistics, and epidemic planning. Yet existing approaches often rely either on purely data-driven models—that cannot extrapolate beyond the historical distributions observed in the available sparse or partially observed datasets—or attempt to generate simulators solely via Large Language Models (LLMs), risking the compound effect of subtle inaccuracies and misalignment with empirical evidence. We propose a hybrid framework for automatic environment creation that integrates explicit domain knowledge, encoded and refined through an LLM, with a gradient-free optimization step to estimate fixed parameters aligned with empirical evidence. Concretely, an LLM-guided search loop identifies the simulator's structural components—such as submodules capturing different processes, causal relationships, and inter-component couplings. A gradient-free optimization procedure then fits these submodules to real-world data, accommodating non-differentiable, discrete, or stochastic elements. This design harnesses common-sense and domain-specific priors to mitigate data inefficiency and out-of-distribution brittleness, while remaining grounded in reality, being flexible to diverse requirements, and admitting system-level interventions. Our method provides a blueprint for building more reliable, causally informed, and uncertainty-aware simulators, paving the way for better decision-making in complex real-world systems.",
    "is_llm_safety": true
  },
  {
    "title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
    "url": "https://icml.cc/virtual/2025/poster/46281",
    "abstract": "Large language models (LLMs) have proven to be very capable, but access to the best models currently rely on inference providers which introduces trust challenges -- how can we be sure that the provider is using the model configuration they claim?We propose TOPLOC, a novel method for verifiable inference that addresses this problem.TOPLOC leverages a compact locality sensitive hashing mechanism for intermediate activations which can detect unauthorized modifications to models, prompts, or precision with 100\\% accuracy, achieving no false positives or negatives in our empirical evaluations.Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference.By introducing a polynomial encoding scheme, TOPLOC minimizes memory overhead of the generated commits by $1000\\times$, requiring only 258 bytes of storage per 32 new tokens compared to the 262KB requirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and lays a foundation for verifiable trustless AI services.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.16007v1",
    "arxiv_id": "2501.16007v1",
    "arxiv_title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
    "arxiv_authors": [
      "Jack Min Ong",
      "Matthew Di Ferrante",
      "Aaron Pazdera",
      "Ryan Garner",
      "Sami Jaghouar",
      "Manveer Basra",
      "Johannes Hagemann"
    ],
    "arxiv_published": "2025-01-27T12:46:45+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.16007v1",
        "arxiv_url": "http://arxiv.org/abs/2501.16007v1",
        "arxiv_title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
        "authors": [
          "Jack Min Ong",
          "Matthew Di Ferrante",
          "Aaron Pazdera",
          "Ryan Garner",
          "Sami Jaghouar",
          "Manveer Basra",
          "Johannes Hagemann"
        ],
        "published": "2025-01-27T12:46:45+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems",
    "url": "https://icml.cc/virtual/2025/poster/44576",
    "abstract": "We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%)  while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias.",
    "is_llm_safety": true
  },
  {
    "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "url": "https://icml.cc/virtual/2025/poster/44746",
    "abstract": "An important question today is whether a given text was used to train a large language model (LLM). A \\emph{completion} test is often employed: check if the LLM completes a sufficiently complex text.But, we require a ground-truth definition of membership; most commonly, it is defined as a member based on the \\ngram overlap between the target text and any text in the dataset.In this work, we demonstrate that this $n$-gram based membership definition can be effectively gamed.We study scenarios where sequences are \\emph{non-members} for a given $n$ and we find that completion tests still succeed. We find many natural cases of this by retraining LLMs after removing all training samples that were completed: these cases include exact duplicates, near-duplicates, and even short overlaps; they showcase that it is difficult to find a single viable choice of $n$. Using these insights, we design adversarial datasets that can cause any target sequences to be completed without containing it, for any reasonable choice of $n$.Our findings highlight the inadequacy of $n$-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.17514v2",
    "arxiv_id": "2503.17514v2",
    "arxiv_title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "arxiv_authors": [
      "Ken Ziyu Liu",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "Peter Kairouz",
      "Sanmi Koyejo",
      "Percy Liang",
      "Nicolas Papernot"
    ],
    "arxiv_published": "2025-03-21T19:57:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.17514v2",
        "arxiv_url": "http://arxiv.org/abs/2503.17514v2",
        "arxiv_title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
        "authors": [
          "Ken Ziyu Liu",
          "Christopher A. Choquette-Choo",
          "Matthew Jagielski",
          "Peter Kairouz",
          "Sanmi Koyejo",
          "Percy Liang",
          "Nicolas Papernot"
        ],
        "published": "2025-03-21T19:57:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: We need responsible, application-driven (RAD) AI research",
    "url": "https://icml.cc/virtual/2025/poster/40103",
    "abstract": "This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.",
    "is_llm_safety": true
  },
  {
    "title": "Explaining the role of Intrinsic Dimensionality in Adversarial Training",
    "url": "https://icml.cc/virtual/2025/poster/45952",
    "abstract": "Adversarial Training (AT) impacts different architectures in distinct ways: vision models gain robustness but face reduced generalization, encoder-based models exhibit limited robustness improvements with minimal generalization loss, and recent work in latent-space adversarial training demonstrates that decoder-based models achieve improved robustness by applying AT across multiple layers.We provide the first explanation for these trends by leveraging the manifold conjecture: off-manifold adversarial examples (AEs) enhance robustness, while on-manifold AEs improve generalization.We show that vision and decoder-based models exhibit low intrinsic dimensionality in earlier layers (favoring off-manifold AEs), whereas encoder-based models do so in later layers (favoring on-manifold AEs). Exploiting this property, we introduce SMAAT, which improves the scalability of AT for encoder-based models by perturbing the layer with the lowest intrinsic dimensionality. This reduces the projected gradient descent (PGD) chain length required for AE generation, cutting GPU time by 25–33% while significantly boosting robustness. We validate SMAAT across multiple tasks, including text generation, sentiment classification, safety filtering, and retrieval in RAG setups, demonstrating superior robustness with comparable generalization to standard training.",
    "is_llm_safety": true
  },
  {
    "title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models",
    "url": "https://icml.cc/virtual/2025/poster/44918",
    "abstract": "This paper introduces Llavaguard, a suite of VLM-based vision safeguards that address the critical need for reliable tools in the era of large-scale data and models. To this end, we establish a novel open framework, describing a customizable safety taxonomy, data preprocessing, augmentation, and training setup. For teaching a VLM safeguard on safety, we further create a multimodal safety dataset with high-quality human expert annotations, where each image is labeled with a safety rating, category and rationale. We also employ advanced augmentations to support context-specific assessments. The resulting Llavaguard models, ranging from 0.5B to 7B, serve as a versatile tool for evaluating the safety compliance of visual content against flexible policies. In comprehensive experiments, Llavaguard outperforms both state-of-the-art safeguards and VLMs in accuracy \\textit{and} in flexibly handling different policies. Additionally, we demonstrate Llavaguard's performance in two real-world applications: large-scale dataset annotation and moderation of text-to-image models. We make our entire framework publicly available\\footnote{\\url{https://anonymous.4open.science/r/LlavaGuard-FE7F}}, including the dataset and model weights.",
    "is_llm_safety": true
  },
  {
    "title": "Position: AI Safety Must Embrace an Antifragile Perspective",
    "url": "https://icml.cc/virtual/2025/poster/40133",
    "abstract": "This position paper contends that modern AI research must adopt an antifragile perspective on safety—one in which the system's capacity to handle rare or out-of-distribution (OOD) events adapts and expands over repeated exposures. Conventional static benchmarks and single-shot robustness tests overlook the reality that environments evolve and that models, if left unchallenged, can drift into maladaptation (e.g., reward hacking, over-optimization, or atrophy of broader capabilities). We argue that an antifragile approach---Rather than striving to rapidly reduce current uncertainties, the emphasis is on leveraging those uncertainties to better prepare for potentially greater, more unpredictable uncertainties in the future---is pivotal for the long-term reliability of open-ended ML systems. In this position paper, we first identify key limitations of static testing, including scenario diversity, reward hacking, and over-alignment. We then explore the potential of dynamic, antifragile solutions to manage rare events. Crucially, we advocate for a fundamental recalibration of the methods used to measure, benchmark, and continually improve AI safety over the long term by providing an ethical and practical guidelines to the AI safety community.",
    "is_llm_safety": true
  },
  {
    "title": "Potemkin Understanding in Large Language Models: Formalizing and Benchmarking Conceptual Comprehension",
    "url": "https://icml.cc/virtual/2025/poster/44050",
    "abstract": "This paper documents an overlooked yet pervasive category of failure in large language models (LLMs): potemkin understanding. Potemkins are cases where a model’s misunderstanding of a concept does not align with the way humans would have misconceived it. To quantify this phenomenon, we develop a framework that measures potemkin understanding through the disconnect between a model’s ability to explain and use concepts. Through a series of experiments spanning diverse domains—literary techniques, game theory, and psychological biases—we demonstrate the ubiquity of potemkins. Despite achieving 97.7% accuracy in explaining concepts, LLMs display only 67.9% accuracy in using them, with potemkins consistently appearing across models, tasks, and domains. We develop self-coherence tests to show these failures represent not merely incorrect learning but a deeper absence of concept coherence, while also finding that existing benchmarks do not adequately test for potemkins.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Strong Consumer Protection is an Inalienable Defense for AI Safety",
    "url": "https://icml.cc/virtual/2025/poster/40112",
    "abstract": "Consumer protection laws are designed to protect consumers from unethical business practices. In this position paper, I argue that these laws serve an emergent dual purpose: if appropriately enforced and strengthened, consumer protection laws can serve as an inalienable defense for AI safety. These laws are well established and can be enforced and strengthened to incentivize businesses to design and deploy safer AI systems. This position runs counter to two prevailing trends in AI policy. The first alternative position is that AI safety requires an entirely new set of focused laws to protect humanity's prosperity. Though I find these efforts valuable, I argue that such focused laws are both hard to write and easy to skirt. The second alternative position is that consumer protection is nothing more than red tape; I argue that existing laws dating back many decades have already reigned in some nefarious business practices related to the development and deployment of AI, and that the litigious society of the United States is well-positioned to use consumer protection laws to encourage new AI safety guardrails. This paper takes a tour of some existing consumer protection laws in the United States and their effects on the development and use of AI systems.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Resist Platform-Controlled AI Agents and Champion User-Centric Agent Advocates",
    "url": "https://icml.cc/virtual/2025/poster/40111",
    "abstract": "Language model agents could reshape how users navigate and act in digital environments. If controlled by platform companies---which already dominate online search, communication, and commerce—platform agents would intensify surveillance, exacerbate user lock-in, and further entrench the incumbent digital giants. This position paper argues that to resist the undesirable effects of platform agents, we should champion agent advocates—agents that are controlled by users, serve the interests of users, and preserve user autonomy and choice. We identify key interventions to enable agent advocates: ensuring access to compute, developing interoperability protocols and safety standards, and implementing appropriate market regulations.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Sustaining human-generated data for ML requires shifting focus toward intrinsic human motivations",
    "url": "https://icml.cc/virtual/2025/poster/40176",
    "abstract": "Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content -- it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations--rather than relying solely on external incentives--can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.",
    "is_llm_safety": true
  },
  {
    "title": "Inference-Time Alignment of LLMs via User-Specified Multi-Criteria Transfer Decoding",
    "url": "https://icml.cc/virtual/2025/poster/44696",
    "abstract": "Aligning large language models (LLMs) with human preferences is essential for their safe and effective deployment. Existing methods typically maximize multiple rewards reflecting human preferences, often framed as a multi-objective optimization problem. However, research on bounded rationality suggests that human decision-making follows satisfying strategies—maximizing key objectives while ensuring others meet acceptable thresholds. This aspect is largely overlooked in alignment research. To address this, we introduce UAMD: a user-specified multi-criteria alignment framework, allowing users to set individualized thresholds. Since this personalization complicates training-time alignment, we propose an inference-time alignment method that enforces user-specified thresholds without finetuning. We provide a theoretical analysis of our proposed approach and derive suboptimality upper bounds. We empirically validate the performance of our proposed method through experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, UAMD outperforms the state-of-the-art multi-objective decoding strategy by a margin of $22.3$% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the user-specified threshold on harmlessness.",
    "is_llm_safety": true
  },
  {
    "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
    "url": "https://icml.cc/virtual/2025/poster/43608",
    "abstract": "Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one---typically by verifying each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation that uses only random sampling and direct self-verification results in sustained performance improvements that, for example, elevate the Gemini v1.5 Pro model's reasoning capabilities past that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts---chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited,  frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.",
    "is_llm_safety": true
  },
  {
    "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals",
    "url": "https://icml.cc/virtual/2025/poster/44598",
    "abstract": "Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.",
    "is_llm_safety": true
  },
  {
    "title": "Estimating the Correctness of Language Model Predictions from Internal Causal Mechanisms",
    "url": "https://icml.cc/virtual/2025/poster/45427",
    "abstract": "Does understanding model internals help us to predict the correctness of model outputs? In the present work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms for predicting correctness. Both outperform methods that rely on non-causal features, especially under distribution shifts, where such estimates are most crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.",
    "is_llm_safety": true
  },
  {
    "title": "The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
    "url": "https://icml.cc/virtual/2025/poster/44552",
    "abstract": "How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model’s output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.",
    "is_llm_safety": true
  },
  {
    "title": "Blink of an eye: a simple theory for feature localization in generative models",
    "url": "https://icml.cc/virtual/2025/poster/45312",
    "abstract": "Large language models can exhibit undesirable and unexpected behavior in the blink of an eye. In a recent Anthropic demo, Claude switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow “critical windows” of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. We show that it emerges generically as the generation process localizes to a subpopulation of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes no distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic tools and no Girsanov or statistical-physics-based machinery.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00921v1",
    "arxiv_id": "2502.00921v1",
    "arxiv_title": "Blink of an eye: a simple theory for feature localization in generative models",
    "arxiv_authors": [
      "Marvin Li",
      "Aayush Karan",
      "Sitan Chen"
    ],
    "arxiv_published": "2025-02-02T21:19:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00921v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00921v1",
        "arxiv_title": "Blink of an eye: a simple theory for feature localization in generative models",
        "authors": [
          "Marvin Li",
          "Aayush Karan",
          "Sitan Chen"
        ],
        "published": "2025-02-02T21:19:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Black-Box Adversarial Attacks on LLM-Based Code Completion",
    "url": "https://icml.cc/virtual/2025/poster/44302",
    "abstract": "Modern code completion engines, powered by large language models (LLMs), assist millions of developers with their strong capabilities to generate functionally correct code. Due to this popularity, it is crucial to investigate the security implications of relying on LLM-based code completion. In this work, we demonstrate that state-of-the-art black-box LLM-based code completion engines can be stealthily biased by adversaries to significantly increase their rate of insecure code generation. We present the first attack, named INSEC, that achieves this goal. INSEC works by injecting an attack string as a short comment in the completion input. The attack string is crafted through a query-based optimization procedure starting from a set of carefully designed initialization schemes. We demonstrate INSEC's broad applicability and effectiveness by evaluating it on various state-of-the-art open-source models and black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a diverse set of security-critical test cases, covering 16 CWEs across 5 programming languages, INSEC increases the rate of generated insecure code by more than 50\\%, while maintaining the functional correctness of generated code. INSEC is highly practical -- it requires low resources and costs less than 10 US dollars to develop on commodity hardware. Moreover, we showcase the attack's real-world deployment, by developing an IDE plug-in that stealthily injects INSEC into the GitHub Copilot extension.",
    "is_llm_safety": true
  },
  {
    "title": "Prior-Informed Preference Alignment: as Plain Statistical Estimation",
    "url": "https://icml.cc/virtual/2025/poster/44970",
    "abstract": "Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding. In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\\sim10\\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.",
    "is_llm_safety": true
  },
  {
    "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks",
    "url": "https://icml.cc/virtual/2025/poster/44537",
    "abstract": "Text watermarking aims to subtly embeds statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100\\% attack success rates on seven recent watermarking methods with only \\$0.88 per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model even mobile-level models. Our findings highlight the urgent need for more robust watermarking.",
    "is_llm_safety": true
  },
  {
    "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "url": "https://icml.cc/virtual/2025/poster/45334",
    "abstract": "As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from  both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04270v1",
    "arxiv_id": "2502.04270v1",
    "arxiv_title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "arxiv_authors": [
      "Yunzhen Feng",
      "Ariel Kwiatkowski",
      "Kunhao Zheng",
      "Julia Kempe",
      "Yaqi Duan"
    ],
    "arxiv_published": "2025-02-06T18:09:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04270v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04270v1",
        "arxiv_title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
        "authors": [
          "Yunzhen Feng",
          "Ariel Kwiatkowski",
          "Kunhao Zheng",
          "Julia Kempe",
          "Yaqi Duan"
        ],
        "published": "2025-02-06T18:09:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Uncertainty Quantification Needs Reassessment for Large Language Model Agents",
    "url": "https://icml.cc/virtual/2025/poster/40147",
    "abstract": "Large language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.",
    "is_llm_safety": true
  },
  {
    "title": "Privacy-Preserving Large Language Model Inference via GPU-Accelerated Fully Homomorphic Encryption",
    "url": "https://icml.cc/virtual/2025/poster/45395",
    "abstract": "As large language models (LLMs) become more powerful, the computation required to run these models is increasingly outsourced to a third-party cloud. While this saves clients' computation, it risks leaking the clients' LLM queries to the cloud provider. Fully homomorphic encryption (FHE) presents a natural solution to this problem: simply encrypt the query and evaluate the LLM homomorphically on the cloud machine. The result remains encrypted and can only be learned by the client who holds the secret key. In this work, we present a GPU-accelerated implementation of FHE and use this implementation to evaluate an encrypted GPT-2 forward pass, with runtimes over $200\\times$ faster than the CPU baseline. We also present novel and extensive experimental analysis of approximations of LLM activation functions to maintain accuracy while achieving this performance.",
    "is_llm_safety": true
  },
  {
    "title": "Detecting Strategic Deception with Linear Probes",
    "url": "https://icml.cc/virtual/2025/poster/46082",
    "abstract": "AI models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the AI might produce seemingly benign outputs while their internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations.    We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al., 2023) and one of responses to simple roleplaying scenarios.    We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively,    such as concealing insider trading (Scheurer, 2023) and purposefully underperforming on safety evaluations (Benton, 2024).    We find that our probe distinguishes honest and deceptive responses with AUROCs between 0.96 and 0.999 on our evaluation datasets.    If we set the decision threshold to have a 1\\% false positive rate on chat data not related to deception, our probe catches 95-99\\% of the deceptive responses.    Overall we think white-box probes are promising for future monitoring systems, but current performance    is insufficient as a robust defence against deception.    Our probes' outputs can be viewed    at REDACTED and our code at REDACTED.",
    "is_llm_safety": true
  },
  {
    "title": "Suitability Filter: A Statistical Framework for Model Evaluation in Real-World Deployment Settings",
    "url": "https://icml.cc/virtual/2025/poster/45090",
    "abstract": "Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals - model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degredation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.",
    "is_llm_safety": true
  },
  {
    "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted Conformal Martingales",
    "url": "https://icml.cc/virtual/2025/poster/45832",
    "abstract": "Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection---especially the tools of conformal test martingales (CTMs) and anytime-valid inference---offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.",
    "is_llm_safety": true
  },
  {
    "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44867",
    "abstract": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO), have emerged as alternatives to Reinforcement Learning from Human Feedback (RLHF) for preference learning in Large Language Models (LLMs). DAAs eliminate the need for a separate reward model by using the joint probability of sentences as an implicit reward, optimizing the margin between preferred and unpreferred responses. However, these methods uniformly adjust token probabilities, regardless of their relevance to preference, leading to suboptimal performances. Recent token-level approaches attempt to address this issue but rely on trained credit-assignment models or AI annotators, raising concerns about quality, reliability, and computational overhead. In this paper, we propose ConfPO, which identifies preference-critical tokens based on the training policy’s confidence, thus requiring no additional models or compute for token selection. By focusing on a smaller set of high-impact tokens, ConfPO not only enhances alignment outcomes but also mitigates overoptimization (i.e., reward hacking) by more efficiently using the KL divergence budget. Experimental results on prominent alignment benchmarks—such as AlpacaEval 2 and Arena-Hard—across various LLMs show that ConfPO consistently outperforms DAAs that uniformly optimize over all tokens for preference learning with no computational overhead.",
    "is_llm_safety": true
  },
  {
    "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
    "url": "https://icml.cc/virtual/2025/poster/46448",
    "abstract": "One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuningwith domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.",
    "is_llm_safety": true
  },
  {
    "title": "Understanding the Logic of Direct Preference Alignment through Logic",
    "url": "https://icml.cc/virtual/2025/poster/46481",
    "abstract": "Recent direct preference alignment algorithms (DPA), such as DPO, have shown great promise in aligning large language models to human preferences. While this has motivated the development of many new variants of the original DPO loss, understanding the differences between these recent proposals, as well as developing new DPA loss functions, remains difficult given the lack of a technical and conceptual framework for reasoning about the underlying semantics of these algorithms. In this paper, we attempt to remedy this by formalizing DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic program that characterizes its semantics? We propose a novel formalism for characterizing preference losses for single model and reference model based approaches, and identify symbolic forms for a number of commonly used DPA variants. Further, we show how this formal view of preference learning sheds new light on both the size and structure of the DPA loss landscape, making it possible to not only rigorously characterize the relationships between recent loss proposals but also to systematically explore the landscape and derive new loss functions from first principles. We hope our framework and findings will help provide useful guidance to those work- ing on human AI alignment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.17696v2",
    "arxiv_id": "2412.17696v2",
    "arxiv_title": "Understanding the Logic of Direct Preference Alignment through Logic",
    "arxiv_authors": [
      "Kyle Richardson",
      "Vivek Srikumar",
      "Ashish Sabharwal"
    ],
    "arxiv_published": "2024-12-23T16:23:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.17696v2",
        "arxiv_url": "http://arxiv.org/abs/2412.17696v2",
        "arxiv_title": "Understanding the Logic of Direct Preference Alignment through Logic",
        "authors": [
          "Kyle Richardson",
          "Vivek Srikumar",
          "Ashish Sabharwal"
        ],
        "published": "2024-12-23T16:23:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AlphaVerus: Bootstrapping Formally Verified Code Generation through Self-Improving Translation and Treefinement",
    "url": "https://icml.cc/virtual/2025/poster/43780",
    "abstract": "Automated code generation with large language models has gained significant traction, but there remains no guarantee of the correctness of generated code. We aim to use formal verification to provide mathematical guarantees that the generated code is correct. However, generating formally verified code with LLMs is hindered by the scarcity of training data and the complexity of formal proofs. To tackle this challenge, we introduce AlphaVerus, a self-improving framework that bootstraps formally verified code generation by iteratively translating programs from a higher-resource language and leveraging feedback from a verifier. AlphaVerus operates in three phases: exploration of candidate translations, Treefinement -- a novel tree search algorithm for program refinement using verifier feedback, and filtering misaligned specifications and programs to prevent reward hacking. Through this iterative process, AlphaVerus enables LLaMA-3.1-70B model to generate verified code without human intervention or model finetuning. AlphaVerus shows an ability to generate formally verified solutions for HumanEval and MBPP, laying the groundwork for truly trustworthy code-generation agents.",
    "is_llm_safety": true
  },
  {
    "title": "DyCodeEval: Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination",
    "url": "https://icml.cc/virtual/2025/poster/46547",
    "abstract": "The rapid evolution of code largelanguage models (LLMs) underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be \\textit{static} and thus particularly susceptible to data contamination—an unavoidable consequence of the extensive data collection processes used to train Code LLMs.Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose DyCodeEval, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, DyCodeEval employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a new evaluation metric and conduct empirical studies on two seed datasets across eight Code LLMs. Results show that DyCodeEval effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.",
    "is_llm_safety": true
  },
  {
    "title": "Clone-Robust AI Alignment",
    "url": "https://icml.cc/virtual/2025/poster/43664",
    "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF can be unbalanced due to adversarial manipulation or inadvertent repetition. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.",
    "is_llm_safety": true
  },
  {
    "title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
    "url": "https://icml.cc/virtual/2025/poster/44250",
    "abstract": "Capability evaluations are required to understand and regulate AI systems that maybe deployed or further developed. Therefore, it is important that evaluations providean accurate estimation of an AI system’s capabilities. However, in numerous cases,previously latent capabilities have been elicited from models, sometimes longafter initial release. Accordingly, substantial efforts have been made to developmethods for eliciting latent capabilities from models. In this paper, we evaluate theeffectiveness of capability elicitation techniques by intentionally training modelorganisms – language models with hidden capabilities that are revealed by apassword. We introduce a novel method for training model organisms, basedon circuit-breaking, which is more robust to elicitation techniques than standardpassword-locked models. We focus on elicitation techniques based on promptingand activation steering, and compare these to fine-tuning methods. Promptingtechniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting, while steering fails to do so. Fora code-generation task, only fine-tuning can elicit the hidden capabilities of ournovel model organism. Additionally, our results suggest that combining techniquesimproves elicitation. Still, if possible, fine-tuning should be the method of choiceto improve the trustworthiness of capability evaluations.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02180v2",
    "arxiv_id": "2502.02180v2",
    "arxiv_title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
    "arxiv_authors": [
      "Felix Hofstätter",
      "Teun van der Weij",
      "Jayden Teoh",
      "Henning Bartsch",
      "Francis Rhys Ward"
    ],
    "arxiv_published": "2025-02-04T09:54:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02180v2",
        "arxiv_url": "http://arxiv.org/abs/2502.02180v2",
        "arxiv_title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
        "authors": [
          "Felix Hofstätter",
          "Teun van der Weij",
          "Jayden Teoh",
          "Henning Bartsch",
          "Francis Rhys Ward"
        ],
        "published": "2025-02-04T09:54:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44313",
    "abstract": "Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model.This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences.To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO).DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling.We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure.Experiments demonstrate that DDRO achieves superior performance compared to existing methods, showcasing its effectiveness and potential for significant improvement.DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.07558v2",
    "arxiv_id": "2505.07558v2",
    "arxiv_title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
    "arxiv_authors": [
      "Rei Higuchi",
      "Taiji Suzuki"
    ],
    "arxiv_published": "2025-05-12T13:36:25+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.07558v2",
        "arxiv_url": "http://arxiv.org/abs/2505.07558v2",
        "arxiv_title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
        "authors": [
          "Rei Higuchi",
          "Taiji Suzuki"
        ],
        "published": "2025-05-12T13:36:25+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
    "url": "https://icml.cc/virtual/2025/poster/45428",
    "abstract": "Although language model (LM) agents have demonstrated increased performance in multiple domains, including coding and web-browsing, their success in cybersecurity has been limited. We present EnIGMA, an LM agent for autonomously solving  Capture The Flag (CTF) challenges. We introduce new tools and interfaces to improve the agent's ability to find and exploit security vulnerabilities, focusing on interactive terminal programs.  These novel Interactive Agent Tools enable LM agents, for the first time,  to run interactive utilities, such as a debugger and a server connection tool, which are essential for solving these challenges.Empirical analysis on 390 CTF challenges across four benchmarks demonstrate that these new tools and interfaces substantially improve our agent's performance, achieving state-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we analyze data leakage, developing new methods to quantify it and identifying a new phenomenon we term soliloquizing, where the model self-generates hallucinated observations without interacting with the environment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2409.16165v2",
    "arxiv_id": "2409.16165v2",
    "arxiv_title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
    "arxiv_authors": [
      "Talor Abramovich",
      "Meet Udeshi",
      "Minghao Shao",
      "Kilian Lieret",
      "Haoran Xi",
      "Kimberly Milner",
      "Sofija Jancheska",
      "John Yang",
      "Carlos E. Jimenez",
      "Farshad Khorrami",
      "Prashanth Krishnamurthy",
      "Brendan Dolan-Gavitt",
      "Muhammad Shafique",
      "Karthik Narasimhan",
      "Ramesh Karri",
      "Ofir Press"
    ],
    "arxiv_published": "2024-09-24T15:06:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2409.16165v2",
        "arxiv_url": "http://arxiv.org/abs/2409.16165v2",
        "arxiv_title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
        "authors": [
          "Talor Abramovich",
          "Meet Udeshi",
          "Minghao Shao",
          "Kilian Lieret",
          "Haoran Xi",
          "Kimberly Milner",
          "Sofija Jancheska",
          "John Yang",
          "Carlos E. Jimenez",
          "Farshad Khorrami",
          "Prashanth Krishnamurthy",
          "Brendan Dolan-Gavitt",
          "Muhammad Shafique",
          "Karthik Narasimhan",
          "Ramesh Karri",
          "Ofir Press"
        ],
        "published": "2024-09-24T15:06:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "On the Robustness of Reward Models for Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45164",
    "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss as one-way classifiers are prone to over-optimization, losing generalizability to unseen inputs. In this paper, we study the cause of over-optimization and its downstream effects on the RLHF procedure, highlighting the importance of robustness in RMs. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Correspondingly, we propose batch-wise sum-to-zero regularization (BSR) that enforces reward sum for each batch to be zero-centered, constraining the rewards with abnormally large magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness on unseen inputs. Then, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5\\% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0, with reducing generation length by 40\\% while adding a 7\\% increase in win rate, further highlights that robustness in RMs induces robustness in RLHF training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.07271v1",
    "arxiv_id": "2505.07271v1",
    "arxiv_title": "On the Robustness of Reward Models for Language Model Alignment",
    "arxiv_authors": [
      "Jiwoo Hong",
      "Noah Lee",
      "Eunki Kim",
      "Guijin Son",
      "Woojin Chung",
      "Aman Gupta",
      "Shao Tang",
      "James Thorne"
    ],
    "arxiv_published": "2025-05-12T06:48:26+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.07271v1",
        "arxiv_url": "http://arxiv.org/abs/2505.07271v1",
        "arxiv_title": "On the Robustness of Reward Models for Language Model Alignment",
        "authors": [
          "Jiwoo Hong",
          "Noah Lee",
          "Eunki Kim",
          "Guijin Son",
          "Woojin Chung",
          "Aman Gupta",
          "Shao Tang",
          "James Thorne"
        ],
        "published": "2025-05-12T06:48:26+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
    "url": "https://icml.cc/virtual/2025/poster/43504",
    "abstract": "Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a way to quantitatively and systematically analyze its effect on individual outputs is still lacking.In this work, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method takes into account the model's intermediate hidden states, giving a more fine-grained insight into the effects of fine-tuning than a simple comparison of the final outputs of pre-trained and fine-tuned models.We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component.Empirically, we find that one can steer model behavior and performance by up- or down-scaling the fine-tuning component during the forward pass.Motivated by this finding and our theoretical analysis, we define the Tuning Contribution ($\\mathrm{TuCo}$) in terms of the ratio of the magnitudes fine-tuning component and the pre-training component.We find that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces the Tuning Contribution, and that $\\mathrm{TuCo}$ is consistently lower on prompts where the attacks succeed compared to ones where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of these attacks.In short, $\\mathrm{TuCo}$ enables the quantitative study of how fine-tuning influences model behavior and safety, and vice-versa.",
    "is_llm_safety": true
  },
  {
    "title": "Discovering Spoofing Attempts on Language Model Watermarks",
    "url": "https://icml.cc/virtual/2025/poster/44414",
    "abstract": "LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02693v2",
    "arxiv_id": "2410.02693v2",
    "arxiv_title": "Discovering Spoofing Attempts on Language Model Watermarks",
    "arxiv_authors": [
      "Thibaud Gloaguen",
      "Nikola Jovanović",
      "Robin Staab",
      "Martin Vechev"
    ],
    "arxiv_published": "2024-10-03T17:18:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02693v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02693v2",
        "arxiv_title": "Discovering Spoofing Attempts on Language Model Watermarks",
        "authors": [
          "Thibaud Gloaguen",
          "Nikola Jovanović",
          "Robin Staab",
          "Martin Vechev"
        ],
        "published": "2024-10-03T17:18:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "url": "https://icml.cc/virtual/2025/poster/43885",
    "abstract": "Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time: for instance, robustness can be improved by focusing on core task features and ignoring spurious features, and social bias can be mitigated by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, and thereby facilitating more robust, fair, and controllable LLM applications in real-world environments.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.22944v3",
    "arxiv_id": "2410.22944v3",
    "arxiv_title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "arxiv_authors": [
      "Tom A. Lamb",
      "Adam Davies",
      "Alasdair Paren",
      "Philip H. S. Torr",
      "Francesco Pinto"
    ],
    "arxiv_published": "2024-10-30T12:01:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.22944v3",
        "arxiv_url": "http://arxiv.org/abs/2410.22944v3",
        "arxiv_title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
        "authors": [
          "Tom A. Lamb",
          "Adam Davies",
          "Alasdair Paren",
          "Philip H. S. Torr",
          "Francesco Pinto"
        ],
        "published": "2024-10-30T12:01:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Selective Prompt Anchoring for Code Generation",
    "url": "https://icml.cc/virtual/2025/poster/44812",
    "abstract": "Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at https://anonymous.4open.science/r/Selective-Prompt-Anchoring-3693.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2408.09121v4",
    "arxiv_id": "2408.09121v4",
    "arxiv_title": "Selective Prompt Anchoring for Code Generation",
    "arxiv_authors": [
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "arxiv_published": "2024-08-17T07:11:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2408.09121v4",
        "arxiv_url": "http://arxiv.org/abs/2408.09121v4",
        "arxiv_title": "Selective Prompt Anchoring for Code Generation",
        "authors": [
          "Yuan Tian",
          "Tianyi Zhang"
        ],
        "published": "2024-08-17T07:11:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
    "url": "https://icml.cc/virtual/2025/poster/46419",
    "abstract": "Chatbot Arena is an open platform for evaluating LLMs by pairwise battles, in which users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be *rigged* to improve (or decrease) the ranking of a target model $m\\_{t}$. We first introduce a straightforward **target-only rigging** strategy that focuses on new battles involving $m\\_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m\\_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about 1% of new battles will involve $m\\_{t}$. To overcome this, we propose an **omnipresent rigging** strategy, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m\\_{t}$, even if $m\\_{t}$ is not directly involved in the battle. We conduct experiments on around *1.7 million* historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategy can improve model rankings by rigging only *hundreds of* new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.17858v1",
    "arxiv_id": "2501.17858v1",
    "arxiv_title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
    "arxiv_authors": [
      "Rui Min",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Minhao Cheng",
      "Min Lin"
    ],
    "arxiv_published": "2025-01-29T18:57:29+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.17858v1",
        "arxiv_url": "http://arxiv.org/abs/2501.17858v1",
        "arxiv_title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
        "authors": [
          "Rui Min",
          "Tianyu Pang",
          "Chao Du",
          "Qian Liu",
          "Minhao Cheng",
          "Min Lin"
        ],
        "published": "2025-01-29T18:57:29+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Probabilistic Verification of Neural Networks using Branch and Bound",
    "url": "https://icml.cc/virtual/2025/poster/43808",
    "abstract": "Probabilistic verification of neural networks is concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for the probabilistic verification of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted subsets of probabilistic verification. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.17556v2",
    "arxiv_id": "2405.17556v2",
    "arxiv_title": "Probabilistic Verification of Neural Networks using Branch and Bound",
    "arxiv_authors": [
      "David Boetius",
      "Stefan Leue",
      "Tobias Sutter"
    ],
    "arxiv_published": "2024-05-27T18:00:03+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.17556v2",
        "arxiv_url": "http://arxiv.org/abs/2405.17556v2",
        "arxiv_title": "Probabilistic Verification of Neural Networks using Branch and Bound",
        "authors": [
          "David Boetius",
          "Stefan Leue",
          "Tobias Sutter"
        ],
        "published": "2024-05-27T18:00:03+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46148",
    "abstract": "Large Language Models (LLMs) can be misused to spread online spam and misinformation. Content watermarking deters misuse by hiding a message in generated outputs, enabling detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content’s quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the provider’s watermarking method and can find only suboptimal attacks. We formulate the robustness of LLM watermarking as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarking methods. (ii) Even in a non-adaptive setting, attacks optimized adaptively against known watermarks remain effective when tested on unseen watermarks, and (iii) optimization-based attacks are scalable and use limited computational resources of less than seven GPU hours. Our findings underscore the need to test robustness against adaptive attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02440v2",
    "arxiv_id": "2410.02440v2",
    "arxiv_title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "arxiv_authors": [
      "Abdulrahman Diaa",
      "Toluwani Aremu",
      "Nils Lukas"
    ],
    "arxiv_published": "2024-10-03T12:37:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02440v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02440v2",
        "arxiv_title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
        "authors": [
          "Abdulrahman Diaa",
          "Toluwani Aremu",
          "Nils Lukas"
        ],
        "published": "2024-10-03T12:37:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44524",
    "abstract": "We propose mechanistic error reduction with abstention (MERA), a principled steering framework for error mitigation in language models (LMs) that performs selective and adaptive interventions guided by error estimators. Existing steering methods construct steering vectors heuristically, such as by contrasting activations from examples with and without a target property (e.g. toxic vs non-toxic outputs). Adding these vectors to the model’s activations has shown promise for amplifying or suppressing specific behaviours. However, they typically rely on fixed intervention strengths, often leading to understeering or oversteering, without principles to balance safety and effective model corrections. MERA addresses these limitations by (i) optimising the intervention direction and (ii) calibrating when and how much to steer using user-defined constraints. Experiments across diverse datasets and LM families demonstrate notable improvements in task accuracy, establishing MERA as a general-purpose, safe and efficient method for mechanistic steering.",
    "is_llm_safety": true
  },
  {
    "title": "The Limits of Predicting Agents from Behaviour",
    "url": "https://icml.cc/virtual/2025/poster/44820",
    "abstract": "As the complexity of AI systems and their interactions with the world increases, generating explanations for their behaviour at the mechanistic level can become intractable, and some degree of abstraction is necessary. For agents, the most natural abstractions for predicting behaviour attribute beliefs, intentions and goals to the system. If an agent behaves as if it has a certain goal or belief, then we can make reasonable predictions about how it will behave in novel situations, including those where comprehensive safety evaluations are untenable. How well can we infer an agent’s beliefs from their behaviour, and how reliably can these inferred beliefs predict the agent’s behaviour in novel situations? We provide a precise answer to this question under the assumption that the agent’s behaviour is guided by a world model. Our contribution is the derivation novel bounds on the agent's behaviour out-of-distribution, which represent the theoretical limit for predicting intentional agents from behavioural data alone. We discuss the implications of these results for several research areas including fairness and safety.",
    "is_llm_safety": true
  },
  {
    "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44009",
    "abstract": "Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide *marginal* feature attributions, while their extensions to interaction importances only scale to small input lengths ($\\approx 20$). We propose *Spectral Explainer* (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among interactions—common in real-world data—and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20\\% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, *HotpotQA*, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source  LLMs (*GPT-4o mini*) and  compositional reasoning in vision-language models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.13870v1",
    "arxiv_id": "2502.13870v1",
    "arxiv_title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
    "arxiv_authors": [
      "Justin Singh Kang",
      "Landon Butler",
      "Abhineet Agarwal",
      "Yigit Efe Erginbas",
      "Ramtin Pedarsani",
      "Kannan Ramchandran",
      "Bin Yu"
    ],
    "arxiv_published": "2025-02-19T16:49:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.13870v1",
        "arxiv_url": "http://arxiv.org/abs/2502.13870v1",
        "arxiv_title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
        "authors": [
          "Justin Singh Kang",
          "Landon Butler",
          "Abhineet Agarwal",
          "Yigit Efe Erginbas",
          "Ramtin Pedarsani",
          "Kannan Ramchandran",
          "Bin Yu"
        ],
        "published": "2025-02-19T16:49:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Demystifying Singular Defects in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46453",
    "abstract": "Large transformer models are known to produce high-norm tokens. In vision transformers (ViTs), such tokens have been mathematically modeled through the singular vectors of the linear approximations of layers. However, in large language models (LLMs), the underlying causes of high-norm tokens remain largely unexplored, and their different properties from those of ViTs require a new analysis framework. In this paper, we provide both theoretical insights and empirical validation across a range of recent models, leading to the following observations: i) The layer-wise singular direction predicts the abrupt explosion of token norms in LLMs. ii) The negative eigenvalues of a layer explain its sudden decay. iii) The computational pathways leading to high-norm tokens differ between initial and noninitial tokens. iv) High-norm tokens are triggered by the right leading singular vector of the matrix approximating the corresponding modules. We showcase two practical applications of these findings: the improvement of quantization schemes and the design of LLM signatures. Our findings not only advance the understanding of singular defects in LLMs but also open new avenues for their application. We expect that this work will stimulate further research into the internal mechanisms of LLMs and will therefore publicly release our code.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07004v1",
    "arxiv_id": "2502.07004v1",
    "arxiv_title": "Demystifying Singular Defects in Large Language Models",
    "arxiv_authors": [
      "Haoqi Wang",
      "Tong Zhang",
      "Mathieu Salzmann"
    ],
    "arxiv_published": "2025-02-10T20:09:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07004v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07004v1",
        "arxiv_title": "Demystifying Singular Defects in Large Language Models",
        "authors": [
          "Haoqi Wang",
          "Tong Zhang",
          "Mathieu Salzmann"
        ],
        "published": "2025-02-10T20:09:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses",
    "url": "https://icml.cc/virtual/2025/poster/45896",
    "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing (security) benchmarks that often serve as proxies for real-world tasks, AutoAdvExBench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in AutoAdvExBench, it would immediately present practical utility for adversarial machine learning researchers. We then design a strong agent that is capable of breaking 75% of CTF-like (\"homework exercise\") adversarial example defenses. However, we show that this agent is only able to succeed on 13% of the real-world defenses in our benchmark, indicating the large gap between difficulty in attacking \"real\" code, and CTF-like code. This benchmark is publicly accessible at [redacted for review].",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01811v1",
    "arxiv_id": "2503.01811v1",
    "arxiv_title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
    "arxiv_authors": [
      "Nicholas Carlini",
      "Javier Rando",
      "Edoardo Debenedetti",
      "Milad Nasr",
      "Florian Tramèr"
    ],
    "arxiv_published": "2025-03-03T18:39:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01811v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01811v1",
        "arxiv_title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
        "authors": [
          "Nicholas Carlini",
          "Javier Rando",
          "Edoardo Debenedetti",
          "Milad Nasr",
          "Florian Tramèr"
        ],
        "published": "2025-03-03T18:39:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Design Considerations in Offline Preference-based RL",
    "url": "https://icml.cc/virtual/2025/poster/46539",
    "abstract": "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF), which use only a fixed dataset of sampled responses given an input, and preference feedback among these responses, have gained increasing prominence in the literature on aligning language models. In this paper, we study how the different design choices made in methods such as DPO, IPO, SLiC and many variants influence the quality of the learned policy, from a theoretical perspective. Our treatment yields insights into the choices of loss function, the policy which is used to normalize log-likelihoods, and also the role of the data sampling policy. Notably, our results do not rely on the standard reparameterization-style arguments used to motivate some of the algorithms in this family, which allows us to give a unified treatment to a broad class of methods. We also conduct a small empirical study to verify some of the theoretical findings on a standard summarization benchmark.",
    "is_llm_safety": true
  },
  {
    "title": "Energy-Based Preference Model Offers Better Offline Alignment than the Bradley-Terry Preference Model",
    "url": "https://icml.cc/virtual/2025/poster/43792",
    "abstract": "Since the debut of DPO, it has been shown that aligning a target LLM with human preferences via the KL-constrained RLHF loss is mathematically equivalent to a special kind of reward modeling task. Concretely, the task requires: 1) using the target LLM to parameterize the reward model, and 2) tuning the reward model so that it has a 1:1 linear relationship with the true reward. However, we identify a significant issue: the DPO loss might have multiple minimizers, of which only one satisfies the required linearity condition. The problem arises from a well-known issue of the underlying Bradley-Terry preference model: it does not always have a unique maximum likelihood estimator (MLE). Consequently, the minimizer of the RLHF loss might be unattainable because it is merely one among many minimizers of the DPO loss. As a better alternative, we propose an energy-based preference model (EBM) that always has a unique MLE, inherently satisfying the linearity requirement. To showcase the practical utility of replacing BTM with our EBM in the context of offline alignment, we adapt a simple yet scalable objective function from the recent literature on fitting EBM and name it as Energy Preference Alignment (EPA). Empirically, we demonstrate that EPA consistently delivers better performance on open benchmarks compared to DPO, thereby validating the theoretical superiority of our EBM.",
    "is_llm_safety": true
  },
  {
    "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "url": "https://icml.cc/virtual/2025/poster/44754",
    "abstract": "Red teaming aims to assess how large language models (LLMs) can produce content that violates norms, policies, and rules set forth during their safety training. However, most existing automated methods in literature are not representative of the way common users exploit the multi-turn conversational nature of AI models. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vuLnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general purpose model in a way that encourages reasoning through the choices of methods available, the current target model’s response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 96% against smaller models such as Llama 3.1 8B, and 91% against Llama 3.1 70B and 94% for GPT-4o when evaluated against larger models on the JailbreakBench dataset.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.01606v1",
    "arxiv_id": "2410.01606v1",
    "arxiv_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "arxiv_authors": [
      "Maya Pavlova",
      "Erik Brinkman",
      "Krithika Iyer",
      "Vitor Albiero",
      "Joanna Bitton",
      "Hailey Nguyen",
      "Joe Li",
      "Cristian Canton Ferrer",
      "Ivan Evtimov",
      "Aaron Grattafiori"
    ],
    "arxiv_published": "2024-10-02T14:47:05+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.01606v1",
        "arxiv_url": "http://arxiv.org/abs/2410.01606v1",
        "arxiv_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
        "authors": [
          "Maya Pavlova",
          "Erik Brinkman",
          "Krithika Iyer",
          "Vitor Albiero",
          "Joanna Bitton",
          "Hailey Nguyen",
          "Joe Li",
          "Cristian Canton Ferrer",
          "Ivan Evtimov",
          "Aaron Grattafiori"
        ],
        "published": "2024-10-02T14:47:05+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks",
    "url": "https://icml.cc/virtual/2025/poster/44672",
    "abstract": "A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. These methods largely succeed in coercing the target output in their original settings, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model checks if a given jailbreak is likely to occur in the distribution of text. For this, we build an N-gram language model on 1T tokens, which, unlike model-based perplexity, allows for an LLM-agnostic, non-parametric, and inherently interpretable evaluation. We adapt popular attacks to this threat model, and, for the first time, benchmark these attacks on equal footing with it. After a rigorous comparison, we find attack success rates against safety-tuned modern models to be lower than previously presented and that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent words, either selecting phrases absent from real-world text or rare ones, e.g., specific to Reddit or code datasets.",
    "is_llm_safety": true
  },
  {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "url": "https://icml.cc/virtual/2025/poster/45336",
    "abstract": "To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model actually frequently does not complete the response in a harmful manner. Such objectives do not adapt to the attacked model and essentially ignore the fact that LLMs output a distribution over responses. If low attack success under these objectives is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization objective over a population of responses that builds on the REINFORCE policy-gradient formalism. We utilize the state-of-the-art Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD) jailbreak algorithms for our novel adversarial optimization procedure and reveal that current LLMs are even more fragile than previously anticipated. For example, our objective doubles the attack success rate (ASR) on Llama3, and on its circuit-breaker defended version, it achieves an ASR of 61%.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17254v1",
    "arxiv_id": "2502.17254v1",
    "arxiv_title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "arxiv_authors": [
      "Simon Geisler",
      "Tom Wollschläger",
      "M. H. I. Abdalla",
      "Vincent Cohen-Addad",
      "Johannes Gasteiger",
      "Stephan Günnemann"
    ],
    "arxiv_published": "2025-02-24T15:34:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17254v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17254v1",
        "arxiv_title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
        "authors": [
          "Simon Geisler",
          "Tom Wollschläger",
          "M. H. I. Abdalla",
          "Vincent Cohen-Addad",
          "Johannes Gasteiger",
          "Stephan Günnemann"
        ],
        "published": "2025-02-24T15:34:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "url": "https://icml.cc/virtual/2025/poster/46298",
    "abstract": "The safety alignment of state-of-the-art large language models (LLMs) can be circumvented through adversarially crafted inputs, yet how these attacks bypass safety barriers remains poorly understood. Prior work suggests that there exists one refusal direction that, if \"activated,\" determines whether an LLM refuses a request. In this work, we use a novel gradient-based approach to search for such refusal directions. Contrary to prior work, we find multiple independent directions and even multi-dimensional concept cones mediating refusal. Furthermore, we show that orthogonality of directions does not imply independence under intervention, motivating the notion of representational independence to capture both linear and non-linear effects and use it to find genuinely independent directions. We demonstrate that exploiting multiple refusal directions can yield higher attack success rates, suggesting that each direction captures complementary aspects of the refusal process. The existence of multi-dimensional refusal cones indicates that refusal mechanisms in LLMs are governed by complex spatial structures while the existence of multiple representationally independent directions emphasizes that different mechanisms exist. Our gradient-based representation engineering uncovers these mechanisms and can serve as a foundation for future work on understanding refusal behavior.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17420v1",
    "arxiv_id": "2502.17420v1",
    "arxiv_title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "arxiv_authors": [
      "Tom Wollschläger",
      "Jannes Elstner",
      "Simon Geisler",
      "Vincent Cohen-Addad",
      "Stephan Günnemann",
      "Johannes Gasteiger"
    ],
    "arxiv_published": "2025-02-24T18:52:59+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17420v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17420v1",
        "arxiv_title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
        "authors": [
          "Tom Wollschläger",
          "Jannes Elstner",
          "Simon Geisler",
          "Vincent Cohen-Addad",
          "Stephan Günnemann",
          "Johannes Gasteiger"
        ],
        "published": "2025-02-24T18:52:59+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
    "url": "https://icml.cc/virtual/2025/poster/45358",
    "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02089v2",
    "arxiv_id": "2410.02089v2",
    "arxiv_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
    "arxiv_authors": [
      "Jonas Gehring",
      "Kunhao Zheng",
      "Jade Copet",
      "Vegard Mella",
      "Quentin Carbonneaux",
      "Taco Cohen",
      "Gabriel Synnaeve"
    ],
    "arxiv_published": "2024-10-02T23:25:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02089v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02089v2",
        "arxiv_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
        "authors": [
          "Jonas Gehring",
          "Kunhao Zheng",
          "Jade Copet",
          "Vegard Mella",
          "Quentin Carbonneaux",
          "Taco Cohen",
          "Gabriel Synnaeve"
        ],
        "published": "2024-10-02T23:25:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "On Teacher Hacking in Language Model Distillation",
    "url": "https://icml.cc/virtual/2025/poster/43914",
    "abstract": "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model, leading to degraded performance on the true objective, in line with Goodhart's law.In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher.Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking.Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust LMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02671v1",
    "arxiv_id": "2502.02671v1",
    "arxiv_title": "On Teacher Hacking in Language Model Distillation",
    "arxiv_authors": [
      "Daniil Tiapkin",
      "Daniele Calandriello",
      "Johan Ferret",
      "Sarah Perrin",
      "Nino Vieillard",
      "Alexandre Ramé",
      "Mathieu Blondel"
    ],
    "arxiv_published": "2025-02-04T19:26:28+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02671v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02671v1",
        "arxiv_title": "On Teacher Hacking in Language Model Distillation",
        "authors": [
          "Daniil Tiapkin",
          "Daniele Calandriello",
          "Johan Ferret",
          "Sarah Perrin",
          "Nino Vieillard",
          "Alexandre Ramé",
          "Mathieu Blondel"
        ],
        "published": "2025-02-04T19:26:28+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AlphaPO - Reward shape matters for LLM alignment",
    "url": "https://icml.cc/virtual/2025/poster/45569",
    "abstract": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Examples include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably. In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce \\textbf{AlphaPO}, a new DAA method that leverages an $\\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\\% to 10\\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B. The analysis and results presented highlight the importance of the reward shape, and how one can systematically change it to affect training dynamics, as well as improve alignment performance.",
    "is_llm_safety": true
  },
  {
    "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking",
    "url": "https://icml.cc/virtual/2025/poster/44493",
    "abstract": "Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step \"reward hacks\") even if humans are not able to detect that the behavior is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.",
    "is_llm_safety": true
  },
  {
    "title": "Representative Language Generation",
    "url": "https://icml.cc/virtual/2025/poster/46100",
    "abstract": "We introduce \"representative generation,\" extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data.  We characterize representative uniform and non-uniform generation, introducing the ``group closure dimension'' as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.",
    "is_llm_safety": true
  },
  {
    "title": "B-score: Detecting biases in large language models using response history",
    "url": "https://icml.cc/virtual/2025/poster/44236",
    "abstract": "Large language models (LLMs) were found to contain strong gender biases (e.g., against female) or numerical biases (e.g., for number 7). We test whether LLMs would be able to output less biased answers when allowed to observe its prior answers to the same question in a \\multiturn conversation. For thorough evaluation of LLM biases across different question types, we propose a set of questions spanning 9 topics and across 4 categories: questions that ask for Subjective opinions; Random answers; or objective answers to real-world Easy or Hard questions. Interestingly, LLMs are able to \"de-bias'' themselves in multi-turn settings in response to Random questions but not other categories. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e., accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or single-turn probabilities alone.",
    "is_llm_safety": true
  },
  {
    "title": "Controlling Large Language Model with Latent Action",
    "url": "https://icml.cc/virtual/2025/poster/44697",
    "abstract": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. Existing frameworks often rely on token-level actions that may be overly large and inefficient. To address this limitation, we propose learning a compact and latent action space to improve controllability and exploration in RL. Inspired by the literature of \"RL from observations only\", we propose **L**atent **A**ction governed world **M**odel from **P**re-trained LLM (LAMP), which augments a latent action space with a pre-trained LLM to form a latent action language world model. This latent action model is extensively trained from the pre-training dataset and tuned in the post-training stage. Experiments with Llama-3.1-8B as the base model demonstrate that using this latent action model for RL training achieves better controllability on multiple tasks, including a $64.0\\%$ win-rate in average of multiple preference alignment tasks, $11.0\\%$ improvement in math reasoning task, as well as the improved and flexible searching over the token-level action framework. In addition to these performance gains, we also find that controlling LLMs with latent actions mitigates forgetting and maintains general abilities well.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.21383v1",
    "arxiv_id": "2503.21383v1",
    "arxiv_title": "Controlling Large Language Model with Latent Actions",
    "arxiv_authors": [
      "Chengxing Jia",
      "Ziniu Li",
      "Pengyuan Wang",
      "Yi-Chen Li",
      "Zhenyu Hou",
      "Yuxiao Dong",
      "Yang Yu"
    ],
    "arxiv_published": "2025-03-27T11:25:22+00:00",
    "similarity_score": 0.9902912621359223,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.21383v1",
        "arxiv_url": "http://arxiv.org/abs/2503.21383v1",
        "arxiv_title": "Controlling Large Language Model with Latent Actions",
        "authors": [
          "Chengxing Jia",
          "Ziniu Li",
          "Pengyuan Wang",
          "Yi-Chen Li",
          "Zhenyu Hou",
          "Yuxiao Dong",
          "Yang Yu"
        ],
        "published": "2025-03-27T11:25:22+00:00",
        "similarity_score": 0.9902912621359223
      }
    ]
  },
  {
    "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention",
    "url": "https://icml.cc/virtual/2025/poster/45527",
    "abstract": "Cautious predictions—where a machine learning model abstains when uncertain—are crucial for limiting harmful errors in safety-critical applications. In this work, we identify a novel threat: a dishonest institution can exploit these mechanisms to discriminate or unjustly deny services under the guise of uncertainty. We demonstrate the practicality of this threat by introducing an uncertainty-inducing attack called Mirage, which deliberately reduces confidence in targeted input regions, thereby covertly disadvantaging specific individuals. At the same time, Mirage maintains high predictive performance across all data points. To counter this threat, we propose Confidential Guardian, a framework that analyzes calibration metrics on a reference dataset to detect artificially suppressed confidence. Additionally, it employs zero-knowledge proofs of verified inference to ensure that reported confidence scores genuinely originate from the deployed model. This prevents the provider from fabricating arbitrary model confidence values while protecting the model’s proprietary details. Our results confirm that Confidential Guardian effectively prevents the misuse of cautious predictions, providing verifiable assurances that abstention reflects genuine model uncertainty rather than malicious intent.",
    "is_llm_safety": true
  },
  {
    "title": "TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/45181",
    "abstract": "Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such learned token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sentence-level bandit problem. To address this challenge, this work decomposes the sentence-level PPO into a sequence of token-level PPO problems, and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work develops a loss function with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards, and facilitates the discovery of satisfactory policies upon loss convergence, a property that is rarely observed in conventional preference optimization methods. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard.",
    "is_llm_safety": true
  },
  {
    "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46052",
    "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. We hope this benchmark can help the community develop better safety aligned models.",
    "is_llm_safety": true
  },
  {
    "title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
    "url": "https://icml.cc/virtual/2025/poster/46217",
    "abstract": "Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data ($\\textbf{observe}$), engage in query-specific divergent thinking ($\\textbf{reflect}$), and then synthesize this information to produce the final output ($\\textbf{speak}$). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks ($\\textbf{GPT3.5T + GIVE > GPT4}$). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to $\\textbf{43.5\\\\%} \\rightarrow \\textbf{88.2\\\\%}$ accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from $\\textbf{135}$ to more than $\\textbf{840k}$ nodes. (6) The reasoning process involved in GIVE is fully interpretable.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.08475v2",
    "arxiv_id": "2410.08475v2",
    "arxiv_title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
    "arxiv_authors": [
      "Jiashu He",
      "Mingyu Derek Ma",
      "Jinxuan Fan",
      "Dan Roth",
      "Wei Wang",
      "Alejandro Ribeiro"
    ],
    "arxiv_published": "2024-10-11T03:05:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.08475v2",
        "arxiv_url": "http://arxiv.org/abs/2410.08475v2",
        "arxiv_title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
        "authors": [
          "Jiashu He",
          "Mingyu Derek Ma",
          "Jinxuan Fan",
          "Dan Roth",
          "Wei Wang",
          "Alejandro Ribeiro"
        ],
        "published": "2024-10-11T03:05:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Rethinking LLM Bias Probing Using Lessons from the Social Sciences",
    "url": "https://icml.cc/virtual/2025/poster/40102",
    "abstract": "We argue that the new frontier of LLM bias probing can (and should) benefit from decades of social science research. While many LLM probes are direct applications of human ones, there remains a disconnect between psychological theory and LLM bias research. This introduces significant challenges: (1) we lack principled criteria for choosing appropriate probes, (2) we lack a system for reconciling conflicting results across probes, and (3) we lack formal frameworks for reasoning about when (and why) probe results will generalize to real user behavior. Existing taxonomies do not resolve these challenges. In this position paper, we describe actionable directions for systematizing LLM bias studies by connecting probes and the constructs they are intended to measure. We introduce EcoLevels – a framework grounded in social science for classifying and comparing bias probes. EcoLevels organizes probes along two features: the (abstraction) level at which bias is assessed and ecological validity (i.e. probe-task generalization). We argue that EcoLevels can prevent end-users from drawing incorrect insights from LLM bias probing",
    "is_llm_safety": true
  },
  {
    "title": "Be Confident: Uncovering Overfitting in MLLM Multi-Task Tuning",
    "url": "https://icml.cc/virtual/2025/poster/44726",
    "abstract": "Fine-tuning Multimodal Large Language Models (MLLMs) in multi-task learning scenarios has emerged as an effective strategy for achieving cross-domain specialization. However, multi-task fine-tuning frequently induces performance degradation on open-response datasets. We posit that free-form answer generation primarily depends on language priors, and strengthening the integration of visual behavioral cues is critical for enhancing prediction robustness. In this work, we propose Noise Resilient Confidence Alignment to address the challenge of open-response overfitting during multi-task fine-tuning. Our approach prioritizes maintaining consistent prediction patterns in MLLMs across varying visual input qualities. To achieve this, we employ Gaussian perturbations to synthesize distorted visual inputs and enforce token prediction confidence alignment towards the normal visual branch. By explicitly linking confidence calibration to visual robustness, this method reduces over-reliance on language priors. We conduct extensive empirical evaluations across diverse multi-task downstream settings via popular MLLM architectures. The comprehensive experiment demonstrates the effectiveness of our method, showcasing its ability to alleviate open-response overfitting while maintaining satisfying multi-task fine-tuning performance.",
    "is_llm_safety": true
  },
  {
    "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift",
    "url": "https://icml.cc/virtual/2025/poster/44703",
    "abstract": "Current Large Language Model (LLM) preference optimization algorithms do not account for temporal preference drift, which can lead to severe misalignment. To address this limitation, we propose Non-Stationary Direct Preference Optimisation (NS-DPO) that models time-dependent reward functions with a Dynamic Bradley-Terry model. NS-DPO proposes a computationally efficient solution of introducing only a single discount parameter in the loss function, which is used for exponential weighting that proportionally focuses learning on more time-relevant datapoints. We theoretically analyse the convergence of NS-DPO in a general setting of not knowing the exact timing of preferences, providing upper bounds on the estimation error and regret caused by non-stationary preferences. Finally, we demonstrate the effectiveness of NS-DPO for fine-tuning LLMs under drifting preferences. Using scenarios where various levels of preference drift is introduced with popular LLM reward models and datasets, we show that NS-DPO fine-tuned LLMs remain robust under non-stationarity, significantly outperforming baseline algorithms that ignore temporal preference changes, without sacrificing performance in stationary cases.",
    "is_llm_safety": true
  },
  {
    "title": "ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification",
    "url": "https://icml.cc/virtual/2025/poster/44699",
    "abstract": "Self-awareness, i.e., the ability to assess and correct one's own generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or rather relying on large external verifiers. In this work, we propose  Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. We introduce a structured curriculum based upon online preference learning to implement this efficiently. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves reasoning performance.",
    "is_llm_safety": true
  },
  {
    "title": "Resolving Lexical Bias in Model Editing",
    "url": "https://icml.cc/virtual/2025/poster/44799",
    "abstract": "Model editing is an emerging technique designed to modify the outputs of large language models after they are trained. However, past techniques directly modify model weights, and this can result in model degradation. Recent techniques avoid making modifications to the model's weights by using an adapter that applies edits to the model when triggered by semantic similarity in the representation space. We demonstrate that current adapter methods are \\textit{critically vulnerable} to strong lexical biases, leading to issues such as applying edits to irrelevant prompts with overlapping words. This paper presents a principled approach to learning a disentangled representation space that facilitates precise localization of edits by maintaining distance between irrelevant prompts while preserving proximity among paraphrases. In our empirical study, we show that our method (Projector Editor Networks for ModelEditing - PENME) achieves state-of-the-art model editing results while being more computationally efficient during inference than previous methods and adaptable across different architectures.",
    "is_llm_safety": true
  },
  {
    "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "url": "https://icml.cc/virtual/2025/poster/45658",
    "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering,  we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.17148v3",
    "arxiv_id": "2501.17148v3",
    "arxiv_title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "arxiv_authors": [
      "Zhengxuan Wu",
      "Aryaman Arora",
      "Atticus Geiger",
      "Zheng Wang",
      "Jing Huang",
      "Dan Jurafsky",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "arxiv_published": "2025-01-28T18:51:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.17148v3",
        "arxiv_url": "http://arxiv.org/abs/2501.17148v3",
        "arxiv_title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
        "authors": [
          "Zhengxuan Wu",
          "Aryaman Arora",
          "Atticus Geiger",
          "Zheng Wang",
          "Jing Huang",
          "Dan Jurafsky",
          "Christopher D. Manning",
          "Christopher Potts"
        ],
        "published": "2025-01-28T18:51:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
    "url": "https://icml.cc/virtual/2025/poster/44866",
    "abstract": "Large Language Models (LLMs) are vulnerable to backdoor attacks that manipulate outputs via hidden triggers. Existing defense methods—designed for vision/text classification tasks—fail for text generation. We propose Internal Consistency Regularization (CROW), a defense leveraging the observation that backdoored models exhibit unstable layer-wise hidden representations when triggered, while clean models show smooth transitions. CROW enforces consistency across layers via adversarial perturbations and regularization during finetuning, neutralizing backdoors without requiring clean reference models or trigger knowledge—only a small clean dataset. Experiments across Llama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW’s effectiveness: it achieves significant reductions in attack success rates across diverse backdoor strategies (sentiment steering, targeted refusal, code injection) while preserving generative performance. CROW’s architecture-agnostic design enables practical deployment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12768v1",
    "arxiv_id": "2411.12768v1",
    "arxiv_title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
    "arxiv_authors": [
      "Nay Myat Min",
      "Long H. Pham",
      "Yige Li",
      "Jun Sun"
    ],
    "arxiv_published": "2024-11-18T07:52:12+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12768v1",
        "arxiv_url": "http://arxiv.org/abs/2411.12768v1",
        "arxiv_title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
        "authors": [
          "Nay Myat Min",
          "Long H. Pham",
          "Yige Li",
          "Jun Sun"
        ],
        "published": "2024-11-18T07:52:12+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
    "url": "https://icml.cc/virtual/2025/poster/45391",
    "abstract": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-by-step reasoning process that underlies the final evaluation of a response. However, due to the lack of human-annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on Reward-Bench (with a score of $93.9$), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.",
    "is_llm_safety": true
  },
  {
    "title": "CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45903",
    "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments ($p<$0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts. Code and data used in the paper are available at https://anonymous.4open.science/r/CASEBench-D5DB.",
    "is_llm_safety": true
  },
  {
    "title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44002",
    "abstract": "Achieving both high safety and high usefulness simultaneously in large language models has become a critical challenge in recent years.Models often exhibit unsafe behavior or adopt an overly cautious approach leading to frequent overrefusal of benign prompts, which reduces their usefulness. A major factor underlying these behaviors is how the models are finetuned and aligned, particularly the nature and extent of the data used.In this work, we examine how overgenerating finetuning data with advanced teacher models (e.g., GPT-4o)—covering both general-purpose and toxic prompts—affects safety and usefulness in instruction-following language models.Additionally, we present POROver, an alignment strategy designed for models that are highly safe but prone to overrefusal. POROver employs preference optimization algorithms and leverages completions from an advanced teacher model to reduce overrefusals while maintaining safety.Our results show that overgenerating completions for general-purpose prompts significantly boosts safety with only a minimal impact on usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 74.4% to 91.8% because of a substantial rise in safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1% to 57.6% while preserving safety. Finally, applying POROVer increases usefulness further—from 57.6% to 82.1%—while keeping safety at comparable levels.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.12999v1",
    "arxiv_id": "2410.12999v1",
    "arxiv_title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
    "arxiv_authors": [
      "Batuhan K. Karaman",
      "Ishmam Zabir",
      "Alon Benhaim",
      "Vishrav Chaudhary",
      "Mert R. Sabuncu",
      "Xia Song"
    ],
    "arxiv_published": "2024-10-16T19:56:22+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.12999v1",
        "arxiv_url": "http://arxiv.org/abs/2410.12999v1",
        "arxiv_title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
        "authors": [
          "Batuhan K. Karaman",
          "Ishmam Zabir",
          "Alon Benhaim",
          "Vishrav Chaudhary",
          "Mert R. Sabuncu",
          "Xia Song"
        ],
        "published": "2024-10-16T19:56:22+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "url": "https://icml.cc/virtual/2025/poster/46322",
    "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this increased capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories---misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate several leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2 72B, and Llama-3.2 90B, on our benchmark. We find that agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 22.8% and 26.0% of the harmful intents, respectively. Furthermore, we show that the susceptibility of web agents to malicious requests can be increased by priming the agent by conditioning on a partially completed malicious task; this increases attack success rates across all agents studied by as much as 18% (>72% relative increase) in some cases. Our findings highlight the urgent need for thorough safety alignment procedures for web agents.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04957v1",
    "arxiv_id": "2503.04957v1",
    "arxiv_title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "arxiv_authors": [
      "Ada Defne Tur",
      "Nicholas Meade",
      "Xing Han Lù",
      "Alejandra Zambrano",
      "Arkil Patel",
      "Esin Durmus",
      "Spandana Gella",
      "Karolina Stańczak",
      "Siva Reddy"
    ],
    "arxiv_published": "2025-03-06T20:43:14+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04957v1",
        "arxiv_url": "http://arxiv.org/abs/2503.04957v1",
        "arxiv_title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
        "authors": [
          "Ada Defne Tur",
          "Nicholas Meade",
          "Xing Han Lù",
          "Alejandra Zambrano",
          "Arkil Patel",
          "Esin Durmus",
          "Spandana Gella",
          "Karolina Stańczak",
          "Siva Reddy"
        ],
        "published": "2025-03-06T20:43:14+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "On the Power of Context-Enhanced Learning in LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45811",
    "abstract": "We formalize a new concept for LLMs, context-enhanced learning. It involves standard gradient-based learning on text except that the context is enhanced with additional data on which no auto-regressive gradients are computed. This setting is a gradient-based analog of usual in-context learning (ICL) and appears in some recent works.Using a multi-step reasoning task, we prove in a simplified setting that context-enhanced learning can be exponentially more sample-efficient than standard learning when the model is capable of ICL. At a mechanistic level, we find that the benefit of context-enhancement arises from a more accurate gradient learning signal.We also experimentally demonstrate that it appears hard to detect or recover learning materials that were used in the context during training. This may have implications for data security as well as copyright.",
    "is_llm_safety": true
  },
  {
    "title": "Eliciting Language Model Behaviors with Investigator Agents",
    "url": "https://icml.cc/virtual/2025/poster/46145",
    "abstract": "Language models exhibit complex, diverse behaviors when prompted with free-form text, making it hard to characterize the space of possible outputs. We study the problem of behavioral elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations, harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train amortized investigator models to emulate the posterior distribution over the prompts, conditioned on the target behavior. Specifically, we first fit a reverse model and then use reinforcement learning to optimize likelihood of generating the target behavior. To improve the diversity of the prompt distribution, we further propose a novel iterative training objective based on the Frank-Wolfe algorithm that encourages each iteration to discover different sets of prompts not captured by previous iterations. Our investigator models produce prompts that exhibit a variety of effective and human-interpretable strategies for behavior elicitation, obtaining a 100% attack success rate on AdvBench (Harmful Behaviors) and an 85% hallucination rate.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.01236v1",
    "arxiv_id": "2502.01236v1",
    "arxiv_title": "Eliciting Language Model Behaviors with Investigator Agents",
    "arxiv_authors": [
      "Xiang Lisa Li",
      "Neil Chowdhury",
      "Daniel D. Johnson",
      "Tatsunori Hashimoto",
      "Percy Liang",
      "Sarah Schwettmann",
      "Jacob Steinhardt"
    ],
    "arxiv_published": "2025-02-03T10:52:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.01236v1",
        "arxiv_url": "http://arxiv.org/abs/2502.01236v1",
        "arxiv_title": "Eliciting Language Model Behaviors with Investigator Agents",
        "authors": [
          "Xiang Lisa Li",
          "Neil Chowdhury",
          "Daniel D. Johnson",
          "Tatsunori Hashimoto",
          "Percy Liang",
          "Sarah Schwettmann",
          "Jacob Steinhardt"
        ],
        "published": "2025-02-03T10:52:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Auditing Prompt Caching in Language Model APIs",
    "url": "https://icml.cc/virtual/2025/poster/44473",
    "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07776v1",
    "arxiv_id": "2502.07776v1",
    "arxiv_title": "Auditing Prompt Caching in Language Model APIs",
    "arxiv_authors": [
      "Chenchen Gu",
      "Xiang Lisa Li",
      "Rohith Kuditipudi",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "arxiv_published": "2025-02-11T18:58:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07776v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07776v1",
        "arxiv_title": "Auditing Prompt Caching in Language Model APIs",
        "authors": [
          "Chenchen Gu",
          "Xiang Lisa Li",
          "Rohith Kuditipudi",
          "Percy Liang",
          "Tatsunori Hashimoto"
        ],
        "published": "2025-02-11T18:58:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Progressively Label Enhancement for Large Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46261",
    "abstract": "Large Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns.    In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment.   Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.   However, these methods often rely on large high-quality datasets.   Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data.   To deal with this problem, we propose {\\ours}, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model’s training process based on the evolving quality of the generated data.   Specifically, we prompt the model to generate responses for both the original query and a set of carefully designed principle guided query, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores.    Experimental results demonstrate the effectiveness of {\\ours} compared to existing LLM alignment methods.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2408.02599v2",
    "arxiv_id": "2408.02599v2",
    "arxiv_title": "Progressively Label Enhancement for Large Language Model Alignment",
    "arxiv_authors": [
      "Biao Liu",
      "Ning Xu",
      "Xin Geng"
    ],
    "arxiv_published": "2024-08-05T16:21:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2408.02599v2",
        "arxiv_url": "http://arxiv.org/abs/2408.02599v2",
        "arxiv_title": "Progressively Label Enhancement for Large Language Model Alignment",
        "authors": [
          "Biao Liu",
          "Ning Xu",
          "Xin Geng"
        ],
        "published": "2024-08-05T16:21:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Improving LLM Safety Alignment with Dual-Objective Optimization",
    "url": "https://icml.cc/virtual/2025/poster/45626",
    "abstract": "Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment.",
    "is_llm_safety": true
  },
  {
    "title": "Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44996",
    "abstract": "Prompting has become a dominant paradigm for adapting large language models (LLMs).While discrete (textual) prompts are widely used for their interpretability, soft (parameter) prompts have recently gained traction in APIs. This is because they can encode information from more training samples while minimizing the user's token usage, leaving more space in the context window for task-specific input. However, soft prompts are tightly coupled to the LLM they are tuned on, limiting their generalization to other LLMs. This constraint is particularly problematic for efficiency and privacy: (1) tuning prompts on each LLM incurs high computational costs, especially as LLMs continue to grow in size. Additionally, (2) when the LLM is hosted externally, soft prompt tuning often requires sharing private data with the LLM provider. For instance, this is the case with the NVIDIA NeMo API.To address these issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that enables private tuning of soft prompts on a small model and subsequently transfers these prompts to a larger LLM.POST uses knowledge distillation to derive a small model directly from the large LLM to improve prompt transferability, tunes the soft prompt locally---optionally with differential privacy guarantees---and transfers it back to the larger LLM using a small public dataset. Our experiments show that POST reduces computational costs, preserves privacy, and effectively transfers high-utility soft prompts.",
    "is_llm_safety": true
  },
  {
    "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework for Diffusion Models",
    "url": "https://icml.cc/virtual/2025/poster/45434",
    "abstract": "Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability), as illustrated in Fig.1. In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability.",
    "is_llm_safety": true
  },
  {
    "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
    "url": "https://icml.cc/virtual/2025/poster/43871",
    "abstract": "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks focus narrowly on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities—workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) 4-bit quantization (GPTQ, AWQ) and 50% pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5-7B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%--3% drop) but degrades real-world application accuracy by 10%--15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios, bridging the gap between algorithmic efficiency and real-world applicability.",
    "is_llm_safety": true
  },
  {
    "title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/45477",
    "abstract": "Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20\\% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.08559v2",
    "arxiv_id": "2412.08559v2",
    "arxiv_title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "arxiv_authors": [
      "Rongzhe Wei",
      "Mufei Li",
      "Mohsen Ghassemi",
      "Eleonora Kreačić",
      "Yifan Li",
      "Xiang Yue",
      "Bo Li",
      "Vamsi K. Potluru",
      "Pan Li",
      "Eli Chien"
    ],
    "arxiv_published": "2024-12-11T17:22:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.08559v2",
        "arxiv_url": "http://arxiv.org/abs/2412.08559v2",
        "arxiv_title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
        "authors": [
          "Rongzhe Wei",
          "Mufei Li",
          "Mohsen Ghassemi",
          "Eleonora Kreačić",
          "Yifan Li",
          "Xiang Yue",
          "Bo Li",
          "Vamsi K. Potluru",
          "Pan Li",
          "Eli Chien"
        ],
        "published": "2024-12-11T17:22:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
    "url": "https://icml.cc/virtual/2025/poster/46149",
    "abstract": "Large language models (LLMs) have presented impressive performance but often lack the flexibility to adapt to human preferences quickly without retraining. Inspired by the recent efforts on test-time scaling, we make the first attempt to propose Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, eliminating the need to update model parameters. Instead of relying on purely numerical rewards, TPO translates reward signals into \\emph{textual} critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth of the inference process. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly.",
    "is_llm_safety": true
  },
  {
    "title": "Emergent Response Planning in LLM",
    "url": "https://icml.cc/virtual/2025/poster/46050",
    "abstract": "In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\\textit{structural attributes}$ (response length, reasoning steps),  $\\textit{content attributes}$ (character choices in storywriting, multiple-choice answers at the end of response), and $\\textit{behavioral attributes}$ (answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggests potential applications for improving transparency and generation control.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06258v1",
    "arxiv_id": "2502.06258v1",
    "arxiv_title": "Emergent Response Planning in LLM",
    "arxiv_authors": [
      "Zhichen Dong",
      "Zhanhui Zhou",
      "Zhixuan Liu",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "arxiv_published": "2025-02-10T08:48:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06258v1",
        "arxiv_url": "http://arxiv.org/abs/2502.06258v1",
        "arxiv_title": "Emergent Response Planning in LLM",
        "authors": [
          "Zhichen Dong",
          "Zhanhui Zhou",
          "Zhixuan Liu",
          "Chao Yang",
          "Chaochao Lu"
        ],
        "published": "2025-02-10T08:48:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Defending LVLMs Against Vision Attacks through Partial-Perception Supervision",
    "url": "https://icml.cc/virtual/2025/poster/46083",
    "abstract": "Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision. In this approach, the model is prompted using the responses generated by a model that perceives only a partial image.With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Our findings show that the weak model can supervise the strong model: when faced with an attacked input, the strong model becomes less confident and adjusts its response based on the weak model’s partial understanding, effectively defending against the attack. With clean input, it confidently maintains its original response. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.",
    "is_llm_safety": true
  },
  {
    "title": "Understanding Chain-of-Thought in LLMs through Information Theory",
    "url": "https://icml.cc/virtual/2025/poster/45723",
    "abstract": "Large Language Models (LLMs) have shown impressive performance in complex reasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing models to break down problems into manageable sub-tasks. However, existing CoT evaluation techniques either require annotated CoT data or fall short of accurately assessing intermediate reasoning steps, leading to high rates of false positives. In this paper, we formalize CoT reasoning in LLMs through an information-theoretic lens. Specifically, our framework quantifies the `information gain' at each reasoning step, enabling the identification of failure modes in LLMs without the need for expensive annotated datasets. We demonstrate the efficacy of our approach through extensive experiments on toy arithmetic, GSM8K and PRM800k datasets, where it significantly outperforms existing outcome-based methods by providing more accurate insights into model performance on individual tasks.",
    "is_llm_safety": true
  },
  {
    "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards",
    "url": "https://icml.cc/virtual/2025/poster/43464",
    "abstract": "It is now common to evaluate Large Language Models (LLMs) by having humans manually vote to evaluate model outputs, in contrast to typical benchmarks that evaluate knowledge or skill at some particular task. Chatbot Arena, the most popular benchmark of this type, ranks models by asking users to select the better response between two randomly selected models (without revealing which model was responsible for the generations).  These platforms are widely trusted as a fair and accurate measure of LLM capabilities. In this paper, we show that if bot protection and other defenses are not implemented, these voting-based benchmarks are potentially vulnerable to adversarial manipulation. Specifically, we show that an attacker can alter the leaderboard (to promote their favorite model or demote competitors) at the cost of roughly a thousand votes (verified in a simulated, offline version of Chatbot Arena). Our attack consists of two steps: first, we show how an attacker can determine which model was used to generate a given reply with more than $95\\%$ accuracy; and then, the attacker can use this information to consistently vote for (or against) a target model. Working with the Chatbot Arena developers, we identify, propose, and implement mitigations to improve the robustness of Chatbot Arena against adversarial manipulation, which, based on our analysis, substantially increases the cost of such attacks. Some of these defenses were present before our collaboration, such as bot protection with Cloudflare, malicious user detection, and rate limiting. Others, including reCAPTCHA and login are being integrated to strengthen the security in Chatbot Arena.",
    "is_llm_safety": true
  },
  {
    "title": "Discriminative Policy Optimization for Token-Level Reward Models",
    "url": "https://icml.cc/virtual/2025/poster/44776",
    "abstract": "Progress reward models (PRMs) provide more nuanced supervision compared to outcome reward models (ORMs) for optimizing policy models, positioning them as a promising approach to enhancing the capabilities of LLMs in complex reasoning tasks. Recent efforts have advanced PRMs from step-level to token-level granularity by integrating reward modeling into the training of generative models, with reward scores derived from token generation probabilities. However, the conflict between generative language modeling and reward modeling may introduce instability and lead to inaccurate credit assignments. To address this challenge, we revisit token-level reward assignment by decoupling reward modeling from language generation and derive a token-level reward model through the optimization of a discriminative policy, termed the Q-function Reward Model (Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level Q-functions from preference data without relying on fine-grained annotations. In our experiments, Q-RM consistently outperforms all baseline methods across various benchmarks. For example, when integrated into PPO/REINFORCE algorithms, Q-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical reasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared to the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM significantly enhances training efficiency, achieving convergence 12× faster than ORM on GSM8K and 11× faster than step-level PRM on MATH. Code and data are included in the supplementary material and will be made publicly available.",
    "is_llm_safety": true
  },
  {
    "title": "SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models",
    "url": "https://icml.cc/virtual/2025/poster/44108",
    "abstract": "Traditional autonomous driving systems often struggle to integrate high-level reasoning with low-level control, resulting in suboptimal and sometimes unsafe driving behaviors. The emergence of multimodal large language models (MLLMs), which can process both visual and textual data, presents an opportunity to unify perception and reasoning tasks within a single framework. However, effectively embedding precise safety knowledge into MLLMs for autonomous driving remains a significant challenge.To address this, we propose SafeAuto, a novel framework that enhances MLLM-based autonomous driving systems by incorporating both unstructured and structured knowledge. Specifically, we first introduce the Position-Dependent Cross-Entropy (PDCE) loss function, designed to improve the accuracy of low-level control signal predictions when numerical values are represented as text. Second, to ensure safe autonomous driving by explicitly integrating precise safety knowledge into the MLLM, we develop a reasoning component for SafeAuto. This component translates driving safety regulations into first-order logic rules (e.g., \"red light $\\implies$ stop\") and incorporates these rules into a probabilistic graphical model, such as a Markov Logic Network (MLN). The MLN is trained to verify the predicted next actions using environmental attributes identified by attribute recognition models (e.g., detecting a red light) to form the predicates. Additionally, we construct a Multimodal Retrieval-Augmented Generation (Multimodal RAG) model that leverages video data, control signals, and environmental attributes to learn more effectively from past similar driving experiences.By integrating PDCE, MLN, and Multimodal RAG, SafeAuto significantly outperforms existing baselines across multiple datasets. This advancement paves the way for more accurate, reliable, and safer autonomous driving systems that effectively learn from experience, adhere to traffic regulations, and execute precise control actions.",
    "is_llm_safety": true
  },
  {
    "title": "Trustworthy Machine Learning through Data-Specific Indistinguishability",
    "url": "https://icml.cc/virtual/2025/poster/45694",
    "abstract": "This paper studies a range of AI/ML trust concepts, including memorization, data poisoning, and copyright, which can be modeled as constraints on the influence of data on a trained model, characterized by the outcome difference from a processing function (training algorithm). In this realm, we show that provable trustworthy guarantees can be efficiently provided through a new framework termed Data-Specific Indistinguishability (DSI) to select trust-preserving randomization tightly aligning with target outcome differences, as a relaxation of the classic Input-Independent Indistinguishability (III). Technically, we establish both the theoretical and algorithmic foundations of DSI with the optimal high-dimensional Gaussian mechanism. We further show its applications to develop trustworthy deep learning. The experimental results on memorization mitigation for large language models and backdoor defense show both the efficiency and effectiveness of DSI noise mechanism. We release our code in an anonymous Github  link https://anonymous.4open.science/r/DT-DSI-D5E8.",
    "is_llm_safety": true
  },
  {
    "title": "Language Model as Implicit Tree Search",
    "url": "https://icml.cc/virtual/2025/poster/44753",
    "abstract": "Despite advancing language model (LM) alignment, direct preference optimization (DPO) falls short in reasoning with free lunch. As the breakthrough, this work propose a new preference optimization framework incorporating the other LM policy, which holds the asymptotic equivalence with AlphaZero-like search, the apex in complex reasoning. The preference optimization scheme discards any value or reward modeling while the neural implicit tree search coupled with DPO's policy, remains enabling LLM-based reasoning as AlphaZero. Our empirical studies confirm that our method outperforms both DPO variants in human preference alignment, and MCTS-based LMs in mathematical reasoning, and planning tasks.",
    "is_llm_safety": true
  },
  {
    "title": "Minimalist Concept Erasure in Generative Models",
    "url": "https://icml.cc/virtual/2025/poster/44073",
    "abstract": "Recent advances in generative models have demonstrated remarkable capabilities in producing high-quality images, but their reliance on large-scale unlabeled data has raised significant safety and copyright concerns. Efforts to address these issues by erasing unwanted concepts have shown promise. However, many existing erasure methods involve excessive modifications that compromise the overall utility of the model.In this work, we address these issues by formulating a novel minimalist concept erasure objective based only on the distributional distance of final generation outputs. Building on our formulation, we derive a tractable loss for differentiable optimization that leverages backpropagation through all generation steps in an end-to-end manner. We also conduct extensive analysis to show theoretical connections with other models and methods. To improve the robustness of the erasure, we incorporate neuron masking as an alternative to model fine-tuning. Empirical evaluations on state-of-the-art flow-matching models demonstrate that our method robustly erases concepts without degrading overall model performance, paving the way for safer and more responsible generative models.",
    "is_llm_safety": true
  },
  {
    "title": "AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/45946",
    "abstract": "Aligning large language models (LLMs) with human preferences requires balancing policy optimization with computational stability. While recent offline methods like DPO and SimPO bypass reinforcement learning’s complexity, they face critical limitations: DPO relies on static reference models that degrade with policy updates, and SimPO assumes a uniform target reward margin that ignores instance-wise preference strength. We propose AlphaDPO, an adaptive preference optimization framework that dynamically reparameterizes the reference distribution to address these issues. Our key innovation lies in an implicit reference model (\\hat{\\pi}{\\text{ref}} \\propto U(y|x)(\\pi\\theta/\\pi_{\\text{ref}})^\\alpha), which interpolates between policy-driven specialization and uniform exploration while enabling instance-adaptive reward margins. Theoretically, we prove \\method{} implicitly controls sequential KL divergence between iterative policy updates, ensuring stability even with poorly calibrated reference models. Empirically, \\method{} achieves state-of-the-art performance on AlpacaEval 2 (58.7\\% LC win rate) and Arena-Hard (35.7\\% win rate) across Mistral2-7B, Llama3-8B, and Gemma2-9B, demonstrating robust alignment without multi-stage training. Our work establishes adaptive reference reparameterization as a principled mechanism for preference optimization.",
    "is_llm_safety": true
  },
  {
    "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44824",
    "abstract": "Recent advances in large language models (LLMs) have raised urgent concerns about AI-generated text authenticity, prompting regulatory demands for reliable content identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation.  To achieve the above goals, two key challenges lie in the inherent trade-off between text quality preservation and message embedding capacity, and the limitation of current quality-preserving methods unsuitable for model-agnostic detection or without message embedding capacity.To address these challenges, we propose BiMark, a novel watermarking framework that achieves all three capabilities through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an an information encoding approach supporting multi-bit watermarking.Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art methods, BiMark reducing perplexity by up to 11\\% while maintaining equivalent detection rates, achieves up to 30\\% higher extraction rates for short texts, and performs comparably to non-watermarked text on downstream tasks like summarization and translation.",
    "is_llm_safety": true
  },
  {
    "title": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential Privacy",
    "url": "https://icml.cc/virtual/2025/poster/44690",
    "abstract": "Large Language Models (LLMs) have gained significant popularity due to their remarkable capabilities in text understanding and generation. However, despite their widespread deployment in inference services such as ChatGPT, concerns about the potential leakage of sensitive user data have arisen. Existing solutions primarily rely on privacy-enhancing technologies to mitigate such risks, facing the trade-off among efficiency, privacy, and utility. To narrow this gap, we propose Cape, a context-aware prompt perturbation mechanism based on differential privacy, to enable efficient inference with an improved privacy-utility trade-off. Concretely, we introduce a hybrid utility function that better captures the token similarity. Additionally, we propose a bucketized sampling mechanism to handle large sampling space, which might lead to long-tail phenomenons. Extensive experiments across multiple datasets, along with ablation studies, demonstrate that Cape achieves a better privacy-utility trade-off compared to prior state-of-the-art works.",
    "is_llm_safety": true
  },
  {
    "title": "MME-CoT: Benchmarking Chain-of-Thought in LMMs for Reasoning Quality, Robustness, and Efficiency",
    "url": "https://icml.cc/virtual/2025/poster/44904",
    "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) The models with reflection mechanism, such as QVQ, demonstrate a superior CoT quality, approaching GPT-4o; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Contextual Integrity Washing for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/40131",
    "abstract": "Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development.The CI theory emphasizes sharing information in accordance with privacy norms and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. This position paper argues that existing literature adopts CI for LLMs without embracing the theory’s fundamental tenets, essentially amounting to a form of \"CI-washing.\" CI-washing could lead to incorrect conclusions and flawed privacy-preserving designs.  We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias).",
    "is_llm_safety": true
  },
  {
    "title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45246",
    "abstract": "With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V’s ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17514v1",
    "arxiv_id": "2502.17514v1",
    "arxiv_title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "arxiv_authors": [
      "Hantao Lou",
      "Changye Li",
      "Jiaming Ji",
      "Yaodong Yang"
    ],
    "arxiv_published": "2025-02-22T14:20:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17514v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17514v1",
        "arxiv_title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
        "authors": [
          "Hantao Lou",
          "Changye Li",
          "Jiaming Ji",
          "Yaodong Yang"
        ],
        "published": "2025-02-22T14:20:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
    "url": "https://icml.cc/virtual/2025/poster/44766",
    "abstract": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.03122v4",
    "arxiv_id": "2503.03122v4",
    "arxiv_title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
    "arxiv_authors": [
      "Zichao Li",
      "Xueru Wen",
      "Jie Lou",
      "Yuqiu Ji",
      "Yaojie Lu",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "arxiv_published": "2025-03-05T02:37:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.03122v4",
        "arxiv_url": "http://arxiv.org/abs/2503.03122v4",
        "arxiv_title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
        "authors": [
          "Zichao Li",
          "Xueru Wen",
          "Jie Lou",
          "Yuqiu Ji",
          "Yaojie Lu",
          "Xianpei Han",
          "Debing Zhang",
          "Le Sun"
        ],
        "published": "2025-03-05T02:37:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Learning Safety Constraints for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45876",
    "abstract": "Abstract Large language models (LLMs) have emerged as powerful tools but pose significant safety risks through harmful outputs and vulnerability to adversarial attacks. We propose SaP—short for Safety Polytope—a geometric approach to LLM safety, that learns and enforces multiple linear safety constraints directly in the model’s representation space. Our framework identifies safe and unsafe regions via the polytope’s facets, enabling both detection and correction of unsafe outputs through geometric steering. Unlike existing approaches that modify model weights, SaP operates post-hoc in the representation space, preserving model capabilities while enforcing safety constraints. Experiments across multiple LLMs demonstrate that our method reduces adversarial attack success rates from 30-60% to below 3% while maintaining performance on standard tasks. Analysis of the learned polytope facets reveals natural specialization in detecting different safety concepts, providing interpretable insights into its safety mechanisms.",
    "is_llm_safety": true
  },
  {
    "title": "An End-to-End Model For Logits Based Large Language Models Watermarking",
    "url": "https://icml.cc/virtual/2025/poster/46200",
    "abstract": "The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37–39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs. The code is provided in the supplementary material.",
    "is_llm_safety": true
  },
  {
    "title": "Towards Global-level Mechanistic Interpretability: A Perspective of Modular Circuits of Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44616",
    "abstract": "Mechanistic interpretability (MI) research aims to understand large language models (LLMs) by identifying computational circuits, subgraphs of model components with associated functional interpretations, that explain specific behaviors. Current MI approaches focus on discovering task-specific circuits, which has two key limitations: (1) poor generalizability across different language tasks, and (2) high costs associated with requiring human or advanced LLM interpretation of each computational node. To address these challenges, we propose developing a ``modular circuit (MC) vocabulary'' consisting of task-agnostic functional units. Each unit consists of a small computational subgraph with its interpretation. This approach enables global interpretability by allowing different language tasks to share common MCs, while reducing costs by reusing established interpretations for new tasks. We establish five criteria for characterizing the MC vocabulary and present ModCirc, a novel global-level mechanistic interpretability framework for discovering MC vocabularies in LLMs. We demonstrate ModCirc's effectiveness on Med-LLaMA (8B), which successfully identifys modular circuits that perform well on our proposed quality metrics. Code is available at https://anonymous.4open.science/r/ModCirc-4887/.",
    "is_llm_safety": true
  },
  {
    "title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
    "url": "https://icml.cc/virtual/2025/poster/45015",
    "abstract": "The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured \"harm-benefit tree,\" which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impact on any stakeholders. SafetyAnalyst then aggregates all harmful and beneficial effects into a harmfulness score using fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this conceptual framework to develop, test, and release an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that SafetyAnalyst (average F1=0.81) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.16665v2",
    "arxiv_id": "2410.16665v2",
    "arxiv_title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
    "arxiv_authors": [
      "Jing-Jing Li",
      "Valentina Pyatkin",
      "Max Kleiman-Weiner",
      "Liwei Jiang",
      "Nouha Dziri",
      "Anne G. E. Collins",
      "Jana Schaich Borg",
      "Maarten Sap",
      "Yejin Choi",
      "Sydney Levine"
    ],
    "arxiv_published": "2024-10-22T03:38:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.16665v2",
        "arxiv_url": "http://arxiv.org/abs/2410.16665v2",
        "arxiv_title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
        "authors": [
          "Jing-Jing Li",
          "Valentina Pyatkin",
          "Max Kleiman-Weiner",
          "Liwei Jiang",
          "Nouha Dziri",
          "Anne G. E. Collins",
          "Jana Schaich Borg",
          "Maarten Sap",
          "Yejin Choi",
          "Sydney Levine"
        ],
        "published": "2024-10-22T03:38:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44809",
    "abstract": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02384v1",
    "arxiv_id": "2502.02384v1",
    "arxiv_title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
    "arxiv_authors": [
      "Yichi Zhang",
      "Siyuan Zhang",
      "Yao Huang",
      "Zeyu Xia",
      "Zhengwei Fang",
      "Xiao Yang",
      "Ranjie Duan",
      "Dong Yan",
      "Yinpeng Dong",
      "Jun Zhu"
    ],
    "arxiv_published": "2025-02-04T15:02:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02384v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02384v1",
        "arxiv_title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
        "authors": [
          "Yichi Zhang",
          "Siyuan Zhang",
          "Yao Huang",
          "Zeyu Xia",
          "Zhengwei Fang",
          "Xiao Yang",
          "Ranjie Duan",
          "Dong Yan",
          "Yinpeng Dong",
          "Jun Zhu"
        ],
        "published": "2025-02-04T15:02:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "FactTest: Factuality Testing in Large Language Models with Statistical Guarantees",
    "url": "https://icml.cc/virtual/2025/poster/43756",
    "abstract": "The propensity of large language models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored.In this paper, we introduce FactTest, a novel framework that statistically assesses whether an LLM can provide correct answers to given questions with high-probability correctness guarantees. We formulate hallucination detection as a hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that FactTest also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) benchmarks demonstrate that FactTest effectively detects hallucinations and enable LLMs to abstain from answering unknown questions, leading to an over 40% accuracy improvement.",
    "is_llm_safety": true
  },
  {
    "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "url": "https://icml.cc/virtual/2025/poster/46075",
    "abstract": "Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role—a concept we call role separation—is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine role-separation learning: the process of teaching LLMs to robustly distinguish system and user tokens. Through a simple, controlled experimental framework, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing invariant signals that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, modifying position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.00626v2",
    "arxiv_id": "2505.00626v2",
    "arxiv_title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "arxiv_authors": [
      "Zihao Wang",
      "Yibo Jiang",
      "Jiahao Yu",
      "Heqing Huang"
    ],
    "arxiv_published": "2025-05-01T16:06:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.00626v2",
        "arxiv_url": "http://arxiv.org/abs/2505.00626v2",
        "arxiv_title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
        "authors": [
          "Zihao Wang",
          "Yibo Jiang",
          "Jiahao Yu",
          "Heqing Huang"
        ],
        "published": "2025-05-01T16:06:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "url": "https://icml.cc/virtual/2025/poster/45153",
    "abstract": "Benchmark Data Contamination (BDC)—the inclusion of benchmark testing samples in the training set—has raised increasing concerns in Large Language Model (LLM) evaluation, leading to falsely inflated performance estimates and undermining evaluation reliability. To address this, researchers have proposed various mitigation strategies to update existing benchmarks, including modifying original questions or generating new ones based on them. However, a rigorous examination of the effectiveness of these mitigation strategies remains lacking. In this paper, we design a systematic and controlled pipeline along with two novel metrics—fidelity and contamination resistance—to provide a fine-grained and comprehensive assessment of existing BDC mitigation strategies. Previous assessment methods, such as accuracy drop and accuracy matching, focus solely on aggregate accuracy, often leading to incomplete or misleading conclusions. Our metrics address this limitation by emphasizing question-level evaluation result matching. Extensive experiments with 10 LLMs, 5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios reveal that no existing strategy significantly improves resistance over the vanilla case (i.e., no benchmark update) across all benchmarks, and none effectively balances fidelity and contamination resistance. These findings underscore the urgent need for designing more effective BDC mitigation strategies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.16402v1",
    "arxiv_id": "2503.16402v1",
    "arxiv_title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "arxiv_authors": [
      "Yifan Sun",
      "Han Wang",
      "Dongbai Li",
      "Gang Wang",
      "Huan Zhang"
    ],
    "arxiv_published": "2025-03-20T17:55:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.16402v1",
        "arxiv_url": "http://arxiv.org/abs/2503.16402v1",
        "arxiv_title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
        "authors": [
          "Yifan Sun",
          "Han Wang",
          "Dongbai Li",
          "Gang Wang",
          "Huan Zhang"
        ],
        "published": "2025-03-20T17:55:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Language model developers should report train-test overlap",
    "url": "https://icml.cc/virtual/2025/poster/40154",
    "abstract": "Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap, which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 models, finding that just 9 models report train-test overlap: 4 models release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 models publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional models. Overall, this position paper argues that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.",
    "is_llm_safety": true
  },
  {
    "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46277",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.",
    "is_llm_safety": true
  },
  {
    "title": "DAMO: Data- and Model-aware Alignment of Multi-modal LLMs",
    "url": "https://icml.cc/virtual/2025/poster/43449",
    "abstract": "Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences. However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data. In this paper, we propose Data- and Model-aware DPO (DAMO) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that incorporates data hardness, and (2) a model-aware strategy that integrates real-time model responses. By combining the two strategies, DAMO enables the model to effectively adapt to data with varying levels of hardness.Extensive experiments on five benchmarks demonstrate that DAMO not only significantly enhances the trustworthiness, but also improves the effectiveness over general tasks. For instance, on the Object HalBench, our DAMO-7B reduces response-level and mentioned-level hallucination by 90.0% and 95.3%, respectively, surpassing the performance of GPT-4V.",
    "is_llm_safety": true
  },
  {
    "title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance",
    "url": "https://icml.cc/virtual/2025/poster/43644",
    "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across  popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations.",
    "is_llm_safety": true
  },
  {
    "title": "Metadata Conditioning Accelerates Language Model Pre-training",
    "url": "https://icml.cc/virtual/2025/poster/45996",
    "abstract": "The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these heterogeneous data sources is challenging. To address this, we propose a new method, termed Metadata Conditioning then Cooldown (MeCo), to incorporate additional learning cues during pre-training. MeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the text during training and later uses a cooldown phase with only the standard text, thereby enabling the model to function normally even without metadata. MeCo significantly accelerates pre-training across different model scales (600M to 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For instance, a 1.6B language model trained with MeCo matches the downstream task performance of standard pre-training while using 33% less data. Additionally, MeCo enables us to steer language models by conditioning the inference prompt on either real or fabricated metadata that encodes the desired properties of the output: for example, prepending wikipedia.org to reduce harmful generations or factquizmaster.com (fabricated) to improve common knowledge task performance. We also demonstrate that MeCo is compatible with different types of metadata, such as model-generated topics. MeCo is remarkably simple, adds no computational overhead, and demonstrates promise in producing more capable and steerable language models.",
    "is_llm_safety": true
  },
  {
    "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators",
    "url": "https://icml.cc/virtual/2025/poster/46046",
    "abstract": "Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses. We will open source JETTS and maintain a leaderboard website after the anonymous review period.",
    "is_llm_safety": true
  },
  {
    "title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44621",
    "abstract": "Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts.  We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.",
    "is_llm_safety": true
  },
  {
    "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
    "url": "https://icml.cc/virtual/2025/poster/44356",
    "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures.We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10-fold, providing a scalable, rigorous solution for hypothesis validation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.09858v1",
    "arxiv_id": "2502.09858v1",
    "arxiv_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
    "arxiv_authors": [
      "Kexin Huang",
      "Ying Jin",
      "Ryan Li",
      "Michael Y. Li",
      "Emmanuel Candès",
      "Jure Leskovec"
    ],
    "arxiv_published": "2025-02-14T01:46:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.09858v1",
        "arxiv_url": "http://arxiv.org/abs/2502.09858v1",
        "arxiv_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "authors": [
          "Kexin Huang",
          "Ying Jin",
          "Ryan Li",
          "Michael Y. Li",
          "Emmanuel Candès",
          "Jure Leskovec"
        ],
        "published": "2025-02-14T01:46:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
    "url": "https://icml.cc/virtual/2025/poster/46324",
    "abstract": "Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transferability, performing effectively on other unseen hallucination benchmarks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04556v1",
    "arxiv_id": "2502.04556v1",
    "arxiv_title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
    "arxiv_authors": [
      "Hanyu Wang",
      "Bochuan Cao",
      "Yuanpu Cao",
      "Jinghui Chen"
    ],
    "arxiv_published": "2025-02-06T23:10:14+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04556v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04556v1",
        "arxiv_title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
        "authors": [
          "Hanyu Wang",
          "Bochuan Cao",
          "Yuanpu Cao",
          "Jinghui Chen"
        ],
        "published": "2025-02-06T23:10:14+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Learning to Route LLM with Confidence Tokens",
    "url": "https://icml.cc/virtual/2025/poster/45145",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.13284v2",
    "arxiv_id": "2410.13284v2",
    "arxiv_title": "Learning to Route LLMs with Confidence Tokens",
    "arxiv_authors": [
      "Yu-Neng Chuang",
      "Helen Zhou",
      "Prathusha Kameswara Sarma",
      "Parikshit Gopalan",
      "John Boccio",
      "Sara Bolouki",
      "Xia Hu"
    ],
    "arxiv_published": "2024-10-17T07:28:18+00:00",
    "similarity_score": 0.9887640449438202,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.13284v2",
        "arxiv_url": "http://arxiv.org/abs/2410.13284v2",
        "arxiv_title": "Learning to Route LLMs with Confidence Tokens",
        "authors": [
          "Yu-Neng Chuang",
          "Helen Zhou",
          "Prathusha Kameswara Sarma",
          "Parikshit Gopalan",
          "John Boccio",
          "Sara Bolouki",
          "Xia Hu"
        ],
        "published": "2024-10-17T07:28:18+00:00",
        "similarity_score": 0.9887640449438202
      }
    ]
  },
  {
    "title": "Interpreting the repeated token phenomenon in LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45013",
    "abstract": "Large Language Models (LLMs), despite their impressive capabilities, often fail to accurately repeat a single word when instructed to do so (Barbero et al., 2024).  This study links this unexpected behavior to ``attention-sinks'', a mechanism previously identified as crucial for fluency in LLMs, where the initial token in a sequence receives disproportionately high attention. We pinpoint the specific neural circuit responsible for attention-sinks: the first attention layer marks the initial token, and a later MLP neuron adds high-magnitude values to its hidden state, creating the sink. This circuit, consistently observed across various LLMs, is disrupted when processing long repetitions, inducing undesired sinks for each repetition, eventually causing the model to drift from its given instructions. In some cases, this divergence lead to the unintended extraction of training data (Nasr et al., 2023). Based on this understanding, we identify a broader set of non-repeating sequences that trigger the same divergent behavior. Furthermore, we develop a targeted patch that effectively rectifies the issue without impacting overall model performance on other tasks. Our findings highlight the importance of understanding the mechanisms behind well known issues with LLMs. By leveraging mechanistic interpretability techniques, we are able to track down the root cause of the repeated token phenomenon, and ultimately design a fix for this issue. As far as we are aware, this is the first use of mechanistic interpretability to understand and fix a well known privacy issue in LLMs.",
    "is_llm_safety": true
  },
  {
    "title": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents",
    "url": "https://icml.cc/virtual/2025/poster/44721",
    "abstract": "Large language model-based multi-agent systems have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain.However, the impact of clumsy or even malicious agents—those who frequently make errors in their tasks—on the overall performance of the system remains underexplored.This paper investigates:(1) What is the resilience of various system structures (e.g., A$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under faulty agents, on different downstream tasks?(2) How can we increase system resilience to defend against these agents?To simulate faulty agents, we propose two approaches—AutoTransform and AutoInject—which introduce mistakes into the agents' responses.We select four downstream tasks, including code generation, math problems, translation, and text evaluation.Results suggest that the ``hierarchical'' structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C), exhibits superior resilience with the lowest performance drop of $9.2\\%$, compared to $26.0\\%$ and $31.2\\%$ of other two structures.Additionally, we improve the system resilience with two methods, introducing a mechanism for each agent to challenge others' outputs, and an additional agent to review and correct messages.Our code and data is available in the supplementary materials and will be made publicly available upon publication.",
    "is_llm_safety": true
  },
  {
    "title": "ELITE: Enhanced Language-Image Toxicity Evaluation for Safety",
    "url": "https://icml.cc/virtual/2025/poster/46445",
    "abstract": "Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE benchmark, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications.",
    "is_llm_safety": true
  },
  {
    "title": "Active Reward Modeling: Adaptive Preference Labeling for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45827",
    "abstract": "Building neural reward models from human preferences is a pivotal component in reinforcement learning from human feedback (RLHF) and large language model alignment research. Given the scarcity and high cost of human annotation, how to select the most informative pairs to annotate is an essential yet challenging open problem. In this work, we highlight the insight that an ideal comparison dataset for reward modeling should balance exploration of the representation space and make informative comparisons between pairs with moderate reward differences. Technically, challenges arise in quantifying the two objectives and automatically selecting comparisons to be annotated. To address this, we propose the Fisher information-based selection strategies, adapt theories from classical experimental design literature, and apply them to the final linear layer of the deep neural network-based reward modeling tasks. Empirically, our method demonstrates remarkable performance, high computational efficiency, and stability compared to various other selection methods from both deep learning and classical statistical literature across open-source LLMs and datasets. Further ablation studies reveal that incorporating cross-prompt comparisons in active reward modeling significantly enhances labeling efficiency, shedding light on the potential for improved annotation strategies in RLHF.",
    "is_llm_safety": true
  },
  {
    "title": "AdvAgent: Controllable Blackbox Red-teaming on Web Agents",
    "url": "https://icml.cc/virtual/2025/poster/44710",
    "abstract": "Foundation model-based agents are increasingly used to automate complex tasks, enhancing efficiency and productivity. However, their access to sensitive resources and autonomous decision-making also introduce significant security risks, where successful attacks could lead to severe consequences. To systematically uncover these vulnerabilities, we propose AdvAgent, a black-box red-teaming framework for attacking web agents. Unlike existing approaches, AdvAgent employs a reinforcement learning-based pipeline to train an adversarial prompter model that optimizes adversarial prompts using feedback from the black-box agent. With careful attack design, these prompts effectively exploit agent weaknesses while maintaining stealthiness and controllability. Extensive evaluations demonstrate that AdvAgent achieves high success rates against state-of-the-art GPT-4-based web agents across diverse web tasks. Furthermore, we find that existing prompt-based defenses provide only limited protection, leaving agents vulnerable to our framework. These findings highlight critical vulnerabilities in current web agents and emphasize the urgent need for stronger defense mechanisms.",
    "is_llm_safety": true
  },
  {
    "title": "FlipAttack: Jailbreak LLMs via Flipping",
    "url": "https://icml.cc/virtual/2025/poster/45738",
    "abstract": "This paper proposes a simple yet effective jailbreak attack named FlipAttack against black-box LLMs. First, from the autoregressive nature, we reveal that LLMs tend to understand the text from left to right and find that they struggle to comprehend the text when the perturbation is added to the left side. Motivated by these insights, we propose to disguise the harmful prompt by constructing a left-side perturbation merely based on the prompt itself, then generalize this idea to 4 flipping modes. Second, we verify the strong ability of LLMs to perform the text-flipping task and then develop 4 variants to guide LLMs to understand and execute harmful behaviors accurately. These designs keep FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of FlipAttack. Remarkably, it achieves $\\sim$78.97\\% attack success rate across 8 LLMs on average and $\\sim$98\\% bypass rate against 5 guard models on average.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02832v1",
    "arxiv_id": "2410.02832v1",
    "arxiv_title": "FlipAttack: Jailbreak LLMs via Flipping",
    "arxiv_authors": [
      "Yue Liu",
      "Xiaoxin He",
      "Miao Xiong",
      "Jinlan Fu",
      "Shumin Deng",
      "Bryan Hooi"
    ],
    "arxiv_published": "2024-10-02T08:41:23+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02832v1",
        "arxiv_url": "http://arxiv.org/abs/2410.02832v1",
        "arxiv_title": "FlipAttack: Jailbreak LLMs via Flipping",
        "authors": [
          "Yue Liu",
          "Xiaoxin He",
          "Miao Xiong",
          "Jinlan Fu",
          "Shumin Deng",
          "Bryan Hooi"
        ],
        "published": "2024-10-02T08:41:23+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/45967",
    "abstract": "Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with substantial costs due to its compromises in their general functionality, leading to a notorious trade-off between unlearning and retention. It motivates this paper to explore enhanced unlearning schemes that can mitigate this trade-off. Specifically, we propose Gradient Rectified Unlearning (GRU), an improved framework that regulates the directions of gradient updates during the unlearning procedure such that their side impacts on other, unrelated responses can be minimized. GRU is easy and general to implement, demonstrating practical effectiveness across a variety of well-established unlearning benchmarks.",
    "is_llm_safety": true
  },
  {
    "title": "The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions",
    "url": "https://icml.cc/virtual/2025/poster/43629",
    "abstract": "Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective.",
    "is_llm_safety": true
  },
  {
    "title": "Generalized Interpolating Discrete Diffusion",
    "url": "https://icml.cc/virtual/2025/poster/43859",
    "abstract": "While state-of-the-art language models achieve impressive results through next-token prediction, they have inherent limitations such as the inability to revise already generated tokens. This has prompted exploration of alternative approaches such as discrete diffusion. However, masked diffusion, which has emerged as a popular choice due to its simplicity and effectiveness, reintroduces this inability to revise words. To overcome this, we generalize masked diffusion and derive the theoretical backbone of a family of general interpolating discrete diffusion (GIDD) processes offering greater flexibility in the design of the noising processes. Leveraging a novel diffusion ELBO, we achieve compute-matched state-of-the-art performance in diffusion language modeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining masking and uniform noise, leading to improved sample quality and unlocking the ability for the model to correct its own mistakes, an area where autoregressive models notoriously have struggled. Our code and models will be released upon acceptance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04482v1",
    "arxiv_id": "2503.04482v1",
    "arxiv_title": "Generalized Interpolating Discrete Diffusion",
    "arxiv_authors": [
      "Dimitri von Rütte",
      "Janis Fluri",
      "Yuhui Ding",
      "Antonio Orvieto",
      "Bernhard Schölkopf",
      "Thomas Hofmann"
    ],
    "arxiv_published": "2025-03-06T14:30:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04482v1",
        "arxiv_url": "http://arxiv.org/abs/2503.04482v1",
        "arxiv_title": "Generalized Interpolating Discrete Diffusion",
        "authors": [
          "Dimitri von Rütte",
          "Janis Fluri",
          "Yuhui Ding",
          "Antonio Orvieto",
          "Bernhard Schölkopf",
          "Thomas Hofmann"
        ],
        "published": "2025-03-06T14:30:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Square$\\chi$PO: Differentially Private and Robust $\\chi^2$-Preference Optimization in Offline Direct Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44581",
    "abstract": "In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose a variant of \\texttt{$\\chi$PO} -- \\texttt{Square}\\texttt{$\\chi$PO}, which is a simple one-line change of \\texttt{$\\chi$PO}  with the standard log-loss replaced by a new square loss over probability. Thanks to the inherent nice properties of this new loss, we have advanced the state-of-the-art of differentially private and robust alignment. Specifically, for the local model of label privacy, \\texttt{Square}\\texttt{$\\chi$PO}  is the first one that attains optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, \\texttt{Square}\\texttt{$\\chi$PO}  is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, \\texttt{Square}\\texttt{$\\chi$PO}  can address privacy protection and corruption \\emph{simultaneously}, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that \\texttt{Square}\\texttt{$\\chi$PO}  can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.",
    "is_llm_safety": true
  },
  {
    "title": "MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44599",
    "abstract": "The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately addressed clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. In response, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by 14.2% and 51.7% on the Med-VQA and report generation tasks, respectively.",
    "is_llm_safety": true
  },
  {
    "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "url": "https://icml.cc/virtual/2025/poster/46590",
    "abstract": "The canonical setup of learning a reward model (RM) from human preferences with binary feedback discards potentially useful samples (such as \"tied\" between the two responses) and loses fine-grained information  (such as \"slightly better'\"). This paper proposes a framework for learning RMs under ordinal feedback, generalizing the binary feedback to arbitrary granularity. We first identify a marginal unbiasedness condition, which generalizes the existing assumption of the binary feedback. The condition is validated via the sociological concept called \"wisdom of the crowd\". Under this condition, we develop a natural probability model and prove the benefits of fine-grained feedback in terms of reducing the Rademacher complexity, which may be of independent interest to another problem: the bias-variance trade-off in knowledge distillation. The framework also sheds light on designing guidelines for human annotators. Our numerical experiments validate that: (1) fine-grained feedback leads to better RM learning for both in- and out-of-distribution settings; (2) incorporating a certain proportion of tied samples boosts RM learning.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12843v1",
    "arxiv_id": "2411.12843v1",
    "arxiv_title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "arxiv_authors": [
      "Shang Liu",
      "Yu Pan",
      "Guanting Chen",
      "Xiaocheng Li"
    ],
    "arxiv_published": "2024-11-19T20:17:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12843v1",
        "arxiv_url": "http://arxiv.org/abs/2411.12843v1",
        "arxiv_title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
        "authors": [
          "Shang Liu",
          "Yu Pan",
          "Guanting Chen",
          "Xiaocheng Li"
        ],
        "published": "2024-11-19T20:17:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Persistent Topological Features in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/43958",
    "abstract": "Understanding the decision-making processes of large language models is critical given their widespread applications. To achieve this, we aim to connect a formal mathematical framework—zigzag persistence from topological data analysis —with practical and easily applicable algorithms. Zigzag persistence is particularly effective for characterizing data as it dynamically transforms across model layers. Within this framework, we introduce topological descriptors that measure how topological features, $p$-dimensional holes, persist and evolve throughout the layers. Unlike methods that assess each layer individually and then aggregate the results, our approach directly tracks the full evolutionary path of these features. This offers a statistical perspective on how prompts are rearranged and their relative positions changed in the representation space, providing insights into the system’s operation as an integrated whole. To demonstrate the expressivity and applicability of our framework, we highlight how sensitive these descriptors are to different models and a variety of datasets. As a showcase application to a downstream task, we use zigzag persistence to establish a criterion for layer pruning, achieving results comparable to state-of-the-art methods while preserving the system-level perspective.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.11042v1",
    "arxiv_id": "2410.11042v1",
    "arxiv_title": "Persistent Topological Features in Large Language Models",
    "arxiv_authors": [
      "Yuri Gardinazzi",
      "Giada Panerai",
      "Karthik Viswanathan",
      "Alessio Ansuini",
      "Alberto Cazzaniga",
      "Matteo Biagetti"
    ],
    "arxiv_published": "2024-10-14T19:46:23+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.11042v1",
        "arxiv_url": "http://arxiv.org/abs/2410.11042v1",
        "arxiv_title": "Persistent Topological Features in Large Language Models",
        "authors": [
          "Yuri Gardinazzi",
          "Giada Panerai",
          "Karthik Viswanathan",
          "Alessio Ansuini",
          "Alberto Cazzaniga",
          "Matteo Biagetti"
        ],
        "published": "2024-10-14T19:46:23+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Efficient Jailbreaking of Open-Source LLMs in Inference Time",
    "url": "https://icml.cc/virtual/2025/poster/46335",
    "abstract": "Large language models (LLMs) are vulnerable to jailbreak attacks -- resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99\\% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. We provide the code in the supplementary materials.",
    "is_llm_safety": true
  },
  {
    "title": "A Lens into Interpretable Transformer Mistakes via Semantic Dependency",
    "url": "https://icml.cc/virtual/2025/poster/46303",
    "abstract": "Semantic Dependency refers to the relationship between words in a sentence where the meaning of one word depends on another, which is important for natural language understanding.In this paper, we investigate the role of semantic dependencies in answering questions for transformer models, which is achieved by analyzing how token values shift in response to changes in semantics.Through extensive experiments on models including the BERT series, GPT, and LLaMA, we uncover the following key findings:1). Most tokens primarily retain their original semantic information even as they propagate through multiple layers.2). Models can encode truthful semantic dependencies in tokens in the final layer.3). Mistakes in model answers often stem from specific tokens encoded with incorrect semantic dependencies. Furthermore, we found that addressing the incorrectness by directly adjusting parameters is challenging because the same parameters can encode both correct and incorrect semantic dependencies depending on the context.Our findings provide insights into the causes of incorrect information generation in transformers and help the future development of robust and reliable models.",
    "is_llm_safety": true
  },
  {
    "title": "Uncertainty Quantification for LLM-Based Survey Simulations",
    "url": "https://icml.cc/virtual/2025/poster/44103",
    "abstract": "We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.",
    "is_llm_safety": true
  },
  {
    "title": "Backdoor Attacks in Token Selection of Attention Mechanism",
    "url": "https://icml.cc/virtual/2025/poster/44868",
    "abstract": "Despite the remarkable achievements of large foundation models across various tasks, they remain vulnerable to security threats, including backdoor attacks. By injecting poisoned data containing backdoor triggers during training, adversaries can manipulate model predictions by exploiting these triggers. While existing research focuses on designing effective backdoor attacks and evaluating them empirically, it often lacks a rigorous theoretical understanding of when and why such attacks succeed. In this work, we investigate backdoor attacks targeting the token selection process within attention mechanisms—a core component of transformer-based architectures. We prove that single-head self-attention transformers can interpolate poisoned training data through gradient descent. Furthermore, we demonstrate that when the injected poisoned training data contains sufficiently strong backdoor triggers, but is not overly dominant, attackers can successfully manipulate model predictions. Our analysis reveals the dynamics of how adversaries manipulate token selection to compromise model outputs and identifies the theoretical conditions enabling these attacks. We support our theoretical findings with an empirical study on synthetic data.",
    "is_llm_safety": true
  },
  {
    "title": "Reflection-Bench: Evaluating Agency in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44571",
    "abstract": "With large language models (LLMs) increasingly deployed as cognitive engines for AI agents, the reliability and effectiveness of such deployments critically hinges on their intrinsic agency, which remains understudied. Agency, the ability to flexibly reason, learn, and adapt in dynamic environments, represents a cognitive-level capability independent of specific external modules or applications. We characterize the cognitive process underlying human agency, which unfolds in seven interrelated dimensions: prediction, decision-making, perception, memory, counterfactual thinking, belief updating, and meta-reflection. To rigorously evaluate LLMs' agency, we propose Reflection-Bench, a novel contamination-free benchmark consisting of seven parameterized cognitive tests. Through comprehensive evaluation of 16 models using three prompting strategies under entry-level difficulties, we identify a clear three-tier performance hierarchy and significant limitations particularly in meta-reflection capabilities. While state-of-the-art LLMs demonstrate rudimentary signs of intrinsic agency, our findings suggest several promising research directions including enhancing core cognitive functions, improving cross-functional coordination, and developing adaptive processing mechanisms.",
    "is_llm_safety": true
  },
  {
    "title": "CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities",
    "url": "https://icml.cc/virtual/2025/poster/46522",
    "abstract": "Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% or 25% vulnerabilities with or without vulnerability descriptions.",
    "is_llm_safety": true
  },
  {
    "title": "Scaling Laws for Differentially Private Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46020",
    "abstract": "Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive. LLMs also rely on large, high-quality training datasets, like those sourced from (sometimes sensitive) user data. Training models on this sensitive user data requires careful privacy protections like differential privacy (DP). However, the dynamics of DP training are significantly different, and consequently their scaling laws are not yet fully understood. In this work, we establish scaling laws that accurately model the intricacies of DP LLM training, providing a complete picture of the compute-privacy-utility and the optimal training configurations in many settings.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18914v1",
    "arxiv_id": "2501.18914v1",
    "arxiv_title": "Scaling Laws for Differentially Private Language Models",
    "arxiv_authors": [
      "Ryan McKenna",
      "Yangsibo Huang",
      "Amer Sinha",
      "Borja Balle",
      "Zachary Charles",
      "Christopher A. Choquette-Choo",
      "Badih Ghazi",
      "George Kaissis",
      "Ravi Kumar",
      "Ruibo Liu",
      "Da Yu",
      "Chiyuan Zhang"
    ],
    "arxiv_published": "2025-01-31T06:32:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18914v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18914v1",
        "arxiv_title": "Scaling Laws for Differentially Private Language Models",
        "authors": [
          "Ryan McKenna",
          "Yangsibo Huang",
          "Amer Sinha",
          "Borja Balle",
          "Zachary Charles",
          "Christopher A. Choquette-Choo",
          "Badih Ghazi",
          "George Kaissis",
          "Ravi Kumar",
          "Ruibo Liu",
          "Da Yu",
          "Chiyuan Zhang"
        ],
        "published": "2025-01-31T06:32:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/40168",
    "abstract": "This position paper argues that the majority of theory of mind benchmarks are broken because of their inability to directly test how large language models (LLMs) adapt to new partners. This problem stems from the fact that theory of mind benchmarks for LLMs are overwhelmingly inspired by the methods used to test theory of mind in humans and fall victim to a fallacy of attributing human-like qualities to AI agents. We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs. Most theory of mind benchmarks only measure what we call literal theory of mind: the ability to predict the behavior of others. Measuring this kind of reasoning is very informative in testing the ability of agents with self-consistent reasoning. However, it is important to note the distinction between this and what we actually care about when this self-consistency cannot be taken for granted. We call this functional theory of mind: the ability to adapt to agents in-context following a rational response to predictions about their behavior. We find that top performing open source LLMs may display strong capabilities in literal theory of mind, depending on how they are prompted, but seem to struggle with functional theory of mind -- even when partner policies are exceedingly simple. Simply put, strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance. Achieving functional theory of mind, particularly over long interaction horizons with a partner, is a significant challenge deserving a prominent role in any meaningful LLM theory of mind evaluation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.19726v2",
    "arxiv_id": "2412.19726v2",
    "arxiv_title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
    "arxiv_authors": [
      "Matthew Riemer",
      "Zahra Ashktorab",
      "Djallel Bouneffouf",
      "Payel Das",
      "Miao Liu",
      "Justin D. Weisz",
      "Murray Campbell"
    ],
    "arxiv_published": "2024-12-27T16:30:12+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.19726v2",
        "arxiv_url": "http://arxiv.org/abs/2412.19726v2",
        "arxiv_title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
        "authors": [
          "Matthew Riemer",
          "Zahra Ashktorab",
          "Djallel Bouneffouf",
          "Payel Das",
          "Miao Liu",
          "Justin D. Weisz",
          "Murray Campbell"
        ],
        "published": "2024-12-27T16:30:12+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Empirical Privacy Variance",
    "url": "https://icml.cc/virtual/2025/poster/44071",
    "abstract": "We propose the notion of empirical privacy variance and study it in the context of differentially private fine-tuning of language models. Specifically, we show that models calibrated to the same $(\\varepsilon, \\delta)$-DP guarantee using DP-SGD with different hyperparameter configurations can exhibit significant variations in empirical privacy, which we quantify through the lens of memorization. We investigate the generality of this phenomenon across multiple dimensions and discuss why it is surprising and relevant. Through regression analysis, we examine how individual and composite hyperparameters influence empirical privacy. The results reveal a no-free-lunch trade-off: existing practices of hyperparameter tuning in DP-SGD, which focus on optimizing utility under a fixed privacy budget, often come at the expense of empirical privacy. To address this, we propose refined heuristics for hyperparameter selection that explicitly account for empirical privacy, showing that they are both precise and practically useful. Finally, we take preliminary steps to understand empirical privacy variance. We propose two hypotheses, identify limitations in existing techniques like privacy auditing, and outline open questions for future research.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.12314v1",
    "arxiv_id": "2503.12314v1",
    "arxiv_title": "Empirical Privacy Variance",
    "arxiv_authors": [
      "Yuzheng Hu",
      "Fan Wu",
      "Ruicheng Xian",
      "Yuhang Liu",
      "Lydia Zakynthinou",
      "Pritish Kamath",
      "Chiyuan Zhang",
      "David Forsyth"
    ],
    "arxiv_published": "2025-03-16T01:43:49+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.12314v1",
        "arxiv_url": "http://arxiv.org/abs/2503.12314v1",
        "arxiv_title": "Empirical Privacy Variance",
        "authors": [
          "Yuzheng Hu",
          "Fan Wu",
          "Ruicheng Xian",
          "Yuhang Liu",
          "Lydia Zakynthinou",
          "Pritish Kamath",
          "Chiyuan Zhang",
          "David Forsyth"
        ],
        "published": "2025-03-16T01:43:49+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RuleAdapter: Dynamic Rules for training Safety Reward Models in RLHF",
    "url": "https://icml.cc/virtual/2025/poster/43772",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used to align models with human preferences, particularly to enhance the safety of responses generated by LLMs. This method traditionally relies on choosing preferred responses from response pairs. However, due to variations in human opinions and the difficulty of making an overall comparison of two responses, there is a growing shift towards a fine-grained annotation approach, assessing responses based on multiple specific metrics or rules. Selecting and applying these rules efficiently while accommodating the diversity of preference data remains a significant challenge. In this paper, we introduce a dynamic approach that adaptively selects the most critical rules for each pair of responses. We develop a mathematical framework that leverages the maximum discrepancy between each paired responses and theoretically show that this strategy optimizes the mutual information between the rule-based labeling and the hidden ground-truth preferences. We then train an 8B reward model using the adaptively labeled preference dataset and evaluate its performance on RewardBench. As of January 25, 2025, our model achieved the highest safety performance on the leaderboard, outperforming various larger models.",
    "is_llm_safety": true
  },
  {
    "title": "Adaptive Localization of Knowledge Negation for Continual LLM Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/43773",
    "abstract": "With the growing deployment of large language models (LLMs) across diverse domains, concerns regarding their safety have grown substantially.LLM unlearning has emerged as a pivotal approach to removing harmful or unlawful contents while maintaining utility.Despite increasing interest, the challenges of continual unlearning, which is common in real-world scenarios, remain underexplored.Successive unlearning tasks often lead to intensified utility degradation.To effectively unlearn targeted knowledge while preserving LLM utility, it is essential to minimize changes in model parameters by selectively updating those linked to the target knowledge, thereby ensuring other knowledge remains unaffected.Building on the task vector framework, we propose a new method named ALKN (Adaptive Localization of Knowledge Negation), which uses dynamic masking to sparsify training gradients and adaptively adjusts unlearning intensity based on inter-task relationships.Comprehensive experiments across three well-established LLM unlearning datasets demonstrate that our approach consistently outperforms baseline methods in both unlearning effectiveness and utility retention under continual unlearning settings.",
    "is_llm_safety": true
  },
  {
    "title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
    "url": "https://icml.cc/virtual/2025/poster/43678",
    "abstract": "Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates.  Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: \\textit{heterogeneous token overfitting} (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates.To tackle this, we propose {\\name}, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00602v1",
    "arxiv_id": "2502.00602v1",
    "arxiv_title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
    "arxiv_authors": [
      "Tianci Liu",
      "Zihan Dong",
      "Linjun Zhang",
      "Haoyu Wang",
      "Jing Gao"
    ],
    "arxiv_published": "2025-02-02T00:10:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00602v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00602v1",
        "arxiv_title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
        "authors": [
          "Tianci Liu",
          "Zihan Dong",
          "Linjun Zhang",
          "Haoyu Wang",
          "Jing Gao"
        ],
        "published": "2025-02-02T00:10:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AdvI2I: Adversarial Image Attack on Image-to-Image Diffusion Models",
    "url": "https://icml.cc/virtual/2025/poster/45719",
    "abstract": "Recent advances in diffusion models have significantly enhanced the quality of image synthesis, yet they have also introduced serious safety concerns, particularly the generation of Not Safe for Work (NSFW) content. Previous research has demonstrated that adversarial prompts can be used to generate NSFW content. However, such adversarial text prompts are often easily detectable by text-based filters, limiting their efficacy. In this paper, we expose a previously overlooked vulnerability: adversarial image attacks targeting Image-to-Image (I2I) diffusion models. We propose AdvI2I, a novel framework that manipulates input images to induce diffusion models to generate NSFW content. By optimizing a generator to craft adversarial images, AdvI2I circumvents existing defense mechanisms, such as Safe Latent Diffusion (SLD), without altering the text prompts. Furthermore, we introduce AdvI2I-Adaptive, an enhanced version that adapts to potential countermeasures and minimizes the resemblance between adversarial images and NSFW concept embeddings, making the attack more resilient against defenses. Through extensive experiments, we demonstrate that both AdvI2I and AdvI2I-Adaptive can effectively bypass current safeguards, highlighting the urgent need for stronger security measures to address the misuse of I2I diffusion models.",
    "is_llm_safety": true
  },
  {
    "title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46232",
    "abstract": "Keeping large language models factually up-to-date is crucial for deployment, yet costly retraining remains a challenge. Knowledge editing offers a promising alternative, but methods are only tested on small-scale or synthetic edit benchmarks. In this work, we aim to bridge research into lifelong knowledge editing to real-world edits at practically relevant scale. We first introduce \\texttt{WikiBigEdit}; a large-scale benchmark of real-world Wikidata edits, built to automatically extend lifelong for future-proof benchmarking. In its first instance, it includes over 500K question-answer pairs for knowledge editing alongside a comprehensive evaluation pipeline. Finally, we use \\texttt{WikiBigEdit} to study existing knowledge editing techniques' ability to incorporate large volumes of real-world facts and contrast their capabilities to generic modification techniques such as retrieval augmentation and continual finetuning to acquire a complete picture of the practical extent of current lifelong knowledge editing.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.05683v1",
    "arxiv_id": "2503.05683v1",
    "arxiv_title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
    "arxiv_authors": [
      "Lukas Thede",
      "Karsten Roth",
      "Matthias Bethge",
      "Zeynep Akata",
      "Tom Hartvigsen"
    ],
    "arxiv_published": "2025-03-07T18:45:42+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.05683v1",
        "arxiv_url": "http://arxiv.org/abs/2503.05683v1",
        "arxiv_title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
        "authors": [
          "Lukas Thede",
          "Karsten Roth",
          "Matthias Bethge",
          "Zeynep Akata",
          "Tom Hartvigsen"
        ],
        "published": "2025-03-07T18:45:42+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46121",
    "abstract": "Training safe LLMs is one of the most critical research challenge. However, the commonly used method, Refusal Training (RT), struggles to generalize against various OOD jailbreaking attacks. Many safety training methods have been proposed to address this issue. While they offer valuable insights, we aim to complement this line of research by investigating whether OOD attacks truly exceed the capability of RT model. Conducting evaluation with BoN, we observe significant improvements on generalization as N increases. This underscores that the model possesses sufficient safety-related latent knowledge, but RT fails to consistently elicit this knowledge when addressing OOD attacks. Further analysis based on domain adaptation reveals that training with direct refusal causes model to rely on superficial shortcuts, resulting in learning of non-robust representation mappings. Based on our findings, we propose training model to perform safety reasoning for each query. Reasoning supervision encourages model to perform more computations, explicitly eliciting and using latent knowledge through reasoning. To achieve this, we synthesize reasoning supervision based on pre-guidelines, training the model to reason in alignment with them, thereby effectively eliciting and utilizing latent knowledge from diverse perspectives. Extensive experiments show that our method significantly improves generalization performance against OOD attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04040v1",
    "arxiv_id": "2502.04040v1",
    "arxiv_title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
    "arxiv_authors": [
      "Haoyu Wang",
      "Zeyu Qin",
      "Li Shen",
      "Xueqian Wang",
      "Minhao Cheng",
      "Dacheng Tao"
    ],
    "arxiv_published": "2025-02-06T13:01:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04040v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04040v1",
        "arxiv_title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
        "authors": [
          "Haoyu Wang",
          "Zeyu Qin",
          "Li Shen",
          "Xueqian Wang",
          "Minhao Cheng",
          "Dacheng Tao"
        ],
        "published": "2025-02-06T13:01:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45083",
    "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \\textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs.To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: transitivity, commutativity and negation invariance.Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness.Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02205v3",
    "arxiv_id": "2410.02205v3",
    "arxiv_title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "arxiv_authors": [
      "Yinhong Liu",
      "Zhijiang Guo",
      "Tianya Liang",
      "Ehsan Shareghi",
      "Ivan Vulić",
      "Nigel Collier"
    ],
    "arxiv_published": "2024-10-03T04:34:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02205v3",
        "arxiv_url": "http://arxiv.org/abs/2410.02205v3",
        "arxiv_title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
        "authors": [
          "Yinhong Liu",
          "Zhijiang Guo",
          "Tianya Liang",
          "Ehsan Shareghi",
          "Ivan Vulić",
          "Nigel Collier"
        ],
        "published": "2024-10-03T04:34:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
    "url": "https://icml.cc/virtual/2025/poster/44167",
    "abstract": "The training and deployment of large language models (LLMs) induce a feedback loop: models continually learn human beliefs from data, reinforce user beliefs with generated content, re-absorb those reinforced beliefs, and again feed them back to users, creating dynamics resembling an echo chamber.We articulate the hypothesis that the feedback loop with LLMs entrench the existing values and factual beliefs of users, leading to diversity loss and potentially the lock-in of ideas. Prompted by observations of diversity loss in real-world ChatGPT usage data, we study the lock-in hypothesis through data mining, agent-based LLM simulation, and formal modeling.",
    "is_llm_safety": true
  },
  {
    "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains",
    "url": "https://icml.cc/virtual/2025/poster/45226",
    "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs’ generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q&A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query’s embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q&A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.",
    "is_llm_safety": true
  },
  {
    "title": "Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning",
    "url": "https://icml.cc/virtual/2025/poster/43597",
    "abstract": "Machine unlearning presents a promising approach to mitigating privacy and safety concerns in large language models (LLMs) by enabling the selective removal of targeted data or knowledge while preserving model utility. However, existing unlearning methods remain over-sensitive to downstream fine-tuning, which can rapidly recover what is supposed to be unlearned information even when the fine-tuning task is entirely unrelated to the unlearning objective.To enhance robustness, we introduce the concept of `invariance' into unlearning for the first time from the perspective of invariant risk minimization (IRM), a principle for environment-agnostic training. By leveraging IRM, we develop a new invariance-regularized LLM unlearning framework, termed invariant LLM unlearning (ILU). We show that the proposed invariance regularization, even using only a single fine-tuning dataset during ILU training, can enable unlearning robustness to generalize effectively across diverse and new fine-tuning tasks at test time.A task vector analysis is also provided to further elucidate the rationale behind ILU's effectiveness. Extensive experiments on the WMDP benchmark, which focuses on removing an LLM's hazardous knowledge generation capabilities, reveal that ILU significantly outperforms state-of-the-art unlearning methods, including negative preference optimization (NPO) and representation misdirection for unlearning (RMU). Notably, ILU achieves superior unlearning robustness across diverse downstream fine-tuning scenarios (e.g., math, paraphrase detection, and sentiment analysis) while preserving the fine-tuning performance.",
    "is_llm_safety": true
  },
  {
    "title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "url": "https://icml.cc/virtual/2025/poster/45556",
    "abstract": "Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when generating harmful or untruthful responses. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.16638v3",
    "arxiv_id": "2410.16638v3",
    "arxiv_title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "arxiv_authors": [
      "Mengdi Zhang",
      "Kai Kiat Goh",
      "Peixin Zhang",
      "Jun Sun",
      "Rose Lin Xin",
      "Hongyu Zhang"
    ],
    "arxiv_published": "2024-10-22T02:27:57+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.16638v3",
        "arxiv_url": "http://arxiv.org/abs/2410.16638v3",
        "arxiv_title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
        "authors": [
          "Mengdi Zhang",
          "Kai Kiat Goh",
          "Peixin Zhang",
          "Jun Sun",
          "Rose Lin Xin",
          "Hongyu Zhang"
        ],
        "published": "2024-10-22T02:27:57+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game",
    "url": "https://icml.cc/virtual/2025/poster/45387",
    "abstract": "Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding.However, recent studies indicate that even the best models lack true comprehension of their reasoning processes.In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models.We design a $\\textit{\\textbf{C}ritic-\\textbf{D}iscernment \\textbf{G}ame}~(\\textbf{CDG})$ in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback.Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",
    "is_llm_safety": true
  },
  {
    "title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44807",
    "abstract": "Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token’s hidden state, a limitation we term ``efficacy barrier''. To solve this, we propose \\textbf{AnyEdit}, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs.Empirically, it outperforms strong baselines by 21.5\\% on benchmarks including UnKEBench, AKEW, and our new \\textbf{EditEverything} dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing. Our code is available at: \\url{https://anonymous.4open.science/r/AnyEdit}.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.05628v2",
    "arxiv_id": "2502.05628v2",
    "arxiv_title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
    "arxiv_authors": [
      "Houcheng Jiang",
      "Junfeng Fang",
      "Ningyu Zhang",
      "Guojun Ma",
      "Mingyang Wan",
      "Xiang Wang",
      "Xiangnan He",
      "Tat-seng Chua"
    ],
    "arxiv_published": "2025-02-08T16:18:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.05628v2",
        "arxiv_url": "http://arxiv.org/abs/2502.05628v2",
        "arxiv_title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
        "authors": [
          "Houcheng Jiang",
          "Junfeng Fang",
          "Ningyu Zhang",
          "Guojun Ma",
          "Mingyang Wan",
          "Xiang Wang",
          "Xiangnan He",
          "Tat-seng Chua"
        ],
        "published": "2025-02-08T16:18:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45103",
    "abstract": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. In this paper, we introduce preference embedding, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback (RLHF). Experimental results show that our General Preference embedding Model (GPM) consistently outperforms the BT reward model on the RewardBench benchmark and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0, following the language model post-training with GPO and our general preference model, reveal performance improvements over BT models. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://anonymous.4open.science/r/gpm-anonymous-1627.",
    "is_llm_safety": true
  },
  {
    "title": "Constrain Alignment with Sparse Autoencoders",
    "url": "https://icml.cc/virtual/2025/poster/46128",
    "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often experience computational inefficiencies and training instability. In this paper, we propose \\textbf{F}eature-level constrained \\textbf{P}reference \\textbf{O}ptimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves an above 5\\% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.",
    "is_llm_safety": true
  },
  {
    "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "url": "https://icml.cc/virtual/2025/poster/46294",
    "abstract": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically,  energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.19358v2",
    "arxiv_id": "2501.19358v2",
    "arxiv_title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "arxiv_authors": [
      "Yuchun Miao",
      "Sen Zhang",
      "Liang Ding",
      "Yuqi Zhang",
      "Lefei Zhang",
      "Dacheng Tao"
    ],
    "arxiv_published": "2025-01-31T18:10:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.19358v2",
        "arxiv_url": "http://arxiv.org/abs/2501.19358v2",
        "arxiv_title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
        "authors": [
          "Yuchun Miao",
          "Sen Zhang",
          "Liang Ding",
          "Yuqi Zhang",
          "Lefei Zhang",
          "Dacheng Tao"
        ],
        "published": "2025-01-31T18:10:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
    "url": "https://icml.cc/virtual/2025/poster/44270",
    "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve knowledge with implicit subject information from deeper MLP layers, unlike single-hop tasks, which rely on shallow layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers with single-hop edit prompts, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further incorporates multi-hop editing prompts to locate and modify knowledge across different stages of reasoning. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, overcoming the limitations of previous locate-then-edit methods.",
    "is_llm_safety": true
  },
  {
    "title": "What Limits Bidirectional Model's Generative Capabilities? A Uni-Bi-Directional Mixture-of-Expert Method For Bidirectional Fine-tuning",
    "url": "https://icml.cc/virtual/2025/poster/44255",
    "abstract": "Large Language Models excel in generation tasks. Recent studies show that causal attention mechanisms limit their perfermance in embedding tasks and bidirectional modeling may enhance the performance. However, merely bidirectional fine-tuning unidirectional models leads to a sharp decline in generative performance. To investigate the reasons for this performance drop, we consider attention weights as dependence and observe that after bidirectional fine-tuning, the model's subsequent dependence increases, thus impairing its unidirectional generative capabilities. We tested various modules of the Transformer architecture and the results show that the FNN layer is least affected by subsequent dependence in bidirectional fine-tuning. Based on this finding, we propose UBMoE-LLM, a novel uni-bi-directional mixture-of-expert Large Language Model to solve above problem. It combines the original FNN layer of the unidirectional model with the bidirectional FNN layer trained by unsupervised contrastive learning by MoE which can improve the embedding performance of instruction finetuned models while maintaining robust generative performance.  We conducted experiments on rich datasets and models of different scales and kinds.  The experimental results fully verified the rationality of our proposed attention dependence indicator and reflected UBMoE-LLM's excellent generative ability and hallucination-resisting ability.",
    "is_llm_safety": true
  },
  {
    "title": "Watch Out Your Album! On Unintentional Privacy Memorization in Multi-Modal Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/43674",
    "abstract": "Multi-Modal Large Language Models (MLLMs) have exhibited remarkable performance on various vision-language tasks such as Visual Question Answering (VQA). Despite accumulating evidence of privacy concerns associated with task-relevant content, it remains unclear whether MLLMs inadvertently memorize private content that is entirely irrelevant to the training tasks. In this paper, we investigate how randomly generated task-irrelevant private content can become spuriously correlated with downstream objectives due to partial mini-batch training dynamics, thus causing inadvertent memorization. Concretely, we randomly generate task-irrelevant watermarks into VQA fine-tuning images at varying probabilities and propose a novel probing framework to determine whether MLLMs have inadvertently encoded such content. Our experiments reveal that MLLMs exhibit notably different training behaviors in partial mini-batch settings with task-irrelevant watermarks embedded. Furthermore, through layer-wise probing, we demonstrate that MLLMs trigger distinct representational patterns when encountering previously seen task-irrelevant knowledge, even if this knowledge does not influence their output during prompting. Our code is available at anonymity.",
    "is_llm_safety": true
  },
  {
    "title": "Logarithmic Regret for Online KL-Regularized Reinforcement Learning",
    "url": "https://icml.cc/virtual/2025/poster/46376",
    "abstract": "Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$ logarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.",
    "is_llm_safety": true
  },
  {
    "title": "ShieldAgent: Shielding LLM Agents via Verifiable Safety Policy Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/45989",
    "abstract": "LLM agents have demonstrated wide adoption in various applications, leveraging their ability to access sensitive data and make autonomous decisions. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of LLM agents. To tackle these challenges, we propose ShieldAgent, the first LLM-based guardrail agent designed to enforce explicit safety policy compliance of the action sequences of other (multi-modal) LLM agents via automated probabilistic reasoning. Specifically, ShieldAgent constructs a structured safety model by extracting verifiable rules from policy documents and organizing them into action-conditioned probabilistic circuits. During inference, it first localizes relevant rule circuits w.r.t. the proposed action, then generates a shielding plan with operations supported by a rich tool library, and produces executable code for formal verification. Then it performs probabilistic inference to assign safety labels and report rule violations. Recognizing the lack of guardrail benchmarks for LLM agents, we introduce ShieldAgent-Web, a dataset with 2K safety-related instructions across 7 risk categories and 6 web environments, each paired with risky trajectories generated under two SOTA attacks. Experiments show that ShieldAgent achieves SOTA performance on ShieldAgent-Web and existing benchmarks, outperforming prior methods by 11.3% on average with a high rule recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding LLM agents.",
    "is_llm_safety": true
  },
  {
    "title": "Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach",
    "url": "https://icml.cc/virtual/2025/poster/44373",
    "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies.  Extensive experiments on real benchmark datasets, spanning 61 shift scenarios empirically validate our theoretical insights.",
    "is_llm_safety": true
  },
  {
    "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective",
    "url": "https://icml.cc/virtual/2025/poster/44727",
    "abstract": "In this paper, we aim to establish a simple, effective, and theoretically grounded benchmark for rigorously probing abstract reasoning in Large Language Models (LLMs). To achieve this, we first develop a mathematic framework that defines abstract reasoning as the ability to: (i) extract essential patterns independent of surface representations, and (ii) apply consistent rules to these abstract patterns. Based on this framework, we introduce two novel complementary metrics: Γ measures basic reasoning accuracy, while ∆ quantifies a model's reliance on specific symbols rather than underlying patterns - a key indicator of true abstraction versus mere memorization. To implement this measurement, we design a benchmark: systematic symbol remapping in rule-based tasks, which forces models to demonstrate genuine pattern recognition beyond superficial token matching. Extensive LLM evaluations using this benchmark (commercial API models, 7B-70B, multi-agent) reveal:1) critical limitations in non-decimal arithmetic and symbolic reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting; and 3) ∆'s effectiveness in robustly measuring memory dependence by quantifying performance degradation under symbol remapping, particularly highlighting operand-specific memorization. These findings underscore that current LLMs, despite domain-specific strengths, still lack robust abstract reasoning, highlighting key areas for future improvement.",
    "is_llm_safety": true
  },
  {
    "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
    "url": "https://icml.cc/virtual/2025/poster/46116",
    "abstract": "We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.16475v1",
    "arxiv_id": "2412.16475v1",
    "arxiv_title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
    "arxiv_authors": [
      "Yuchen Zhu",
      "Daniel Augusto de Souza",
      "Zhengyan Shi",
      "Mengyue Yang",
      "Pasquale Minervini",
      "Alexander D'Amour",
      "Matt J. Kusner"
    ],
    "arxiv_published": "2024-12-21T04:07:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.16475v1",
        "arxiv_url": "http://arxiv.org/abs/2412.16475v1",
        "arxiv_title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
        "authors": [
          "Yuchen Zhu",
          "Daniel Augusto de Souza",
          "Zhengyan Shi",
          "Mengyue Yang",
          "Pasquale Minervini",
          "Alexander D'Amour",
          "Matt J. Kusner"
        ],
        "published": "2024-12-21T04:07:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reflection-Window Decoding: Text Generation with Selective Refinement",
    "url": "https://icml.cc/virtual/2025/poster/44024",
    "abstract": "The autoregressive approach to text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.",
    "is_llm_safety": true
  },
  {
    "title": "Position: LLMs Need a Bayesian Meta-Reasoning Framework for More Robust and Generalizable Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/40142",
    "abstract": "Large language models (LLMs)  excel in many reasoning tasks but continue to face significant challenges, such as lack of robustness in reasoning, struggling with cross-task generalization, and inefficiencies in scaling up reasoning capabilities. Current training paradigms, including next-token prediction and reinforcement learning from human feedback, often fall short in adaptability to diverse reasoning tasks. Existing approaches, such as prompt optimization and iterative output refinement, offer performance improvement, but can be inefficient and lack effective generalization. To overcome these limitations, this position paper argues for a transformative shift in how LLMs approach reasoning. Drawing inspiration from cognitive science, particularly meta-reasoning theories such as Dual-Process Theory and Metacognitive Reasoning, we propose a Bayesian meta-reasoning framework for LLMs. Our approach integrates self-awareness, monitoring, evaluation, regulation, and meta-reflection, to enhance LLMs' ability to refine reasoning strategies and generalize across tasks. We revisit existing LLM reasoning methods, identify key challenges, and suggest  directions for future research.",
    "is_llm_safety": true
  },
  {
    "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
    "url": "https://icml.cc/virtual/2025/poster/43935",
    "abstract": "We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning ten categories across four high-level classes, and find that the majority of disagreements are due to factors such as task Underspecification or response style. Our findings are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, and LLM-as-Judge evaluation methods fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence in evaluations and during LLM training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.14632v2",
    "arxiv_id": "2410.14632v2",
    "arxiv_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
    "arxiv_authors": [
      "Michael JQ Zhang",
      "Zhilin Wang",
      "Jena D. Hwang",
      "Yi Dong",
      "Olivier Delalleau",
      "Yejin Choi",
      "Eunsol Choi",
      "Xiang Ren",
      "Valentina Pyatkin"
    ],
    "arxiv_published": "2024-10-18T17:32:22+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.14632v2",
        "arxiv_url": "http://arxiv.org/abs/2410.14632v2",
        "arxiv_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
        "authors": [
          "Michael JQ Zhang",
          "Zhilin Wang",
          "Jena D. Hwang",
          "Yi Dong",
          "Olivier Delalleau",
          "Yejin Choi",
          "Eunsol Choi",
          "Xiang Ren",
          "Valentina Pyatkin"
        ],
        "published": "2024-10-18T17:32:22+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
    "url": "https://icml.cc/virtual/2025/poster/43915",
    "abstract": "The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16\\% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.09650v2",
    "arxiv_id": "2502.09650v2",
    "arxiv_title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
    "arxiv_authors": [
      "Chengqian Gao",
      "Haonan Li",
      "Liu Liu",
      "Zeke Xie",
      "Peilin Zhao",
      "Zhiqiang Xu"
    ],
    "arxiv_published": "2025-02-11T17:01:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.09650v2",
        "arxiv_url": "http://arxiv.org/abs/2502.09650v2",
        "arxiv_title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
        "authors": [
          "Chengqian Gao",
          "Haonan Li",
          "Liu Liu",
          "Zeke Xie",
          "Peilin Zhao",
          "Zhiqiang Xu"
        ],
        "published": "2025-02-11T17:01:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RULEBREAKERS: Challenging Large Language Models at the Crossroads between Formal Logic and Human-like Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/43712",
    "abstract": "Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as \"rulebreaker\" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.",
    "is_llm_safety": true
  },
  {
    "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
    "url": "https://icml.cc/virtual/2025/poster/43860",
    "abstract": "Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experimental results across various instruction-following and academic benchmarks demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models. When applying our method to on-policy data, the resulting DPO model outperforms various baselines and achieves state-of-the-art results. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion.",
    "is_llm_safety": true
  },
  {
    "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/45505",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Our framework addresses two critical questions: (1) how to generate high-quality reasoning processes during inference automatically, and (2) how to integrate these processes into post-training. We propose the \\emph{Bootstrapping Reinforced Thinking Process} (BRiTE) algorithm and demonstrate its theoretical convergence at a rate of $1/T$, where $T$ is the number of iterations. The algorithm operates in two steps. First, it generates high-quality rationales by approximating the desired posterior distribution using a reinforcement learning approach with a novel reward shaping mechanism. Second, it fine-tunes the base LLM by maximizing the joint probability of rationale generation with respect to LLM parameters. Empirical evaluation on GSM8K and MATH benchmarks demonstrates that our approach consistently improves performance across different model sizes without requiring human-annotated thinking processes, outperforming standard chain-of-thought prompting while enhancing existing post-training methods.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18858v1",
    "arxiv_id": "2501.18858v1",
    "arxiv_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "arxiv_authors": [
      "Han Zhong",
      "Yutong Yin",
      "Shenao Zhang",
      "Xiaojun Xu",
      "Yuanxin Liu",
      "Yifei Zuo",
      "Zhihan Liu",
      "Boyi Liu",
      "Sirui Zheng",
      "Hongyi Guo",
      "Liwei Wang",
      "Mingyi Hong",
      "Zhaoran Wang"
    ],
    "arxiv_published": "2025-01-31T02:39:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18858v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18858v1",
        "arxiv_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
        "authors": [
          "Han Zhong",
          "Yutong Yin",
          "Shenao Zhang",
          "Xiaojun Xu",
          "Yuanxin Liu",
          "Yifei Zuo",
          "Zhihan Liu",
          "Boyi Liu",
          "Sirui Zheng",
          "Hongyi Guo",
          "Liwei Wang",
          "Mingyi Hong",
          "Zhaoran Wang"
        ],
        "published": "2025-01-31T02:39:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reinforced Lifelong Editing for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46622",
    "abstract": "Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed RLEdit, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a 59.24% improvement while requiring only 2.11% of the time compared to most approaches.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.05759v3",
    "arxiv_id": "2502.05759v3",
    "arxiv_title": "Reinforced Lifelong Editing for Language Models",
    "arxiv_authors": [
      "Zherui Li",
      "Houcheng Jiang",
      "Hao Chen",
      "Baolong Bi",
      "Zhenhong Zhou",
      "Fei Sun",
      "Junfeng Fang",
      "Xiang Wang"
    ],
    "arxiv_published": "2025-02-09T03:37:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.05759v3",
        "arxiv_url": "http://arxiv.org/abs/2502.05759v3",
        "arxiv_title": "Reinforced Lifelong Editing for Language Models",
        "authors": [
          "Zherui Li",
          "Houcheng Jiang",
          "Hao Chen",
          "Baolong Bi",
          "Zhenhong Zhou",
          "Fei Sun",
          "Junfeng Fang",
          "Xiang Wang"
        ],
        "published": "2025-02-09T03:37:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Conformal Tail Risk Control for Large Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45795",
    "abstract": "Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance,  toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, general-purpose scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a light-weight calibration framework for blackbox models that ensures the alignment of humans and machines with provable guarantees. Our framework provides a rigorous approach to controlling any distortion risk measure that is characterized by a weighted average of quantiles of the loss incurred by the LLM with high confidence. The theoretical foundation of our method rests on the connection between conformal risk control and a traditional family of statistics, i.e., L-statistics. To demonstrate the utility of our framework, we conduct comprehensive experiments that address the issue of human-machine misalignment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.20285v1",
    "arxiv_id": "2502.20285v1",
    "arxiv_title": "Conformal Tail Risk Control for Large Language Model Alignment",
    "arxiv_authors": [
      "Catherine Yu-Chi Chen",
      "Jingyan Shen",
      "Zhun Deng",
      "Lihua Lei"
    ],
    "arxiv_published": "2025-02-27T17:10:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.20285v1",
        "arxiv_url": "http://arxiv.org/abs/2502.20285v1",
        "arxiv_title": "Conformal Tail Risk Control for Large Language Model Alignment",
        "authors": [
          "Catherine Yu-Chi Chen",
          "Jingyan Shen",
          "Zhun Deng",
          "Lihua Lei"
        ],
        "published": "2025-02-27T17:10:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reducing Tool Hallucination via Reliability Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45001",
    "abstract": "Large Language Models (LLMs) have expanded their capabilities beyond language generation to interact with external tools, enabling automation and real-world applications. However, tool hallucinations—where models either select inappropriate tools or misuse them—pose significant challenges, leading to erroneous task execution, increased computational costs, and reduced system reliability. To systematically address this issue, we define and categorize tool hallucinations into two main types: tool selection hallucination and tool usage hallucination. To evaluate and mitigate these issues, we introduce RelyToolBench, which integrates specialized test cases and novel metrics to assess hallucination-aware task success and efficiency. Finally, we propose Relign, a reliability alignment framework that expands the tool-use action space to include indecisive actions, allowing LLMs to defer tool use, seek clarification, or adjust tool selection dynamically. Through extensive experiments, we demonstrate that Relign significantly reduces tool hallucinations, improves task reliability, and enhances the efficiency of LLM tool interactions. The code and data will be publicly available.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.04141v2",
    "arxiv_id": "2412.04141v2",
    "arxiv_title": "Reducing Tool Hallucination via Reliability Alignment",
    "arxiv_authors": [
      "Hongshen Xu",
      "Zichen Zhu",
      "Lei Pan",
      "Zihan Wang",
      "Su Zhu",
      "Da Ma",
      "Ruisheng Cao",
      "Lu Chen",
      "Kai Yu"
    ],
    "arxiv_published": "2024-12-05T13:10:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.04141v2",
        "arxiv_url": "http://arxiv.org/abs/2412.04141v2",
        "arxiv_title": "Reducing Tool Hallucination via Reliability Alignment",
        "authors": [
          "Hongshen Xu",
          "Zichen Zhu",
          "Lei Pan",
          "Zihan Wang",
          "Su Zhu",
          "Da Ma",
          "Ruisheng Cao",
          "Lu Chen",
          "Kai Yu"
        ],
        "published": "2024-12-05T13:10:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "url": "https://icml.cc/virtual/2025/poster/45115",
    "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide *accurate judgments* and *actionable suggestions*. In this work, we study LLM critics for code generation and propose $\\texttt{CTRL}$, a framework for $\\texttt{C}$ritic $\\texttt{T}$raining via $\\texttt{R}$einforcement $\\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models.Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1\\% relative improvements across challenging code generation benchmarks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03492v1",
    "arxiv_id": "2502.03492v1",
    "arxiv_title": "Teaching Language Models to Critique via Reinforcement Learning",
    "arxiv_authors": [
      "Zhihui Xie",
      "Jie chen",
      "Liyu Chen",
      "Weichao Mao",
      "Jingjing Xu",
      "Lingpeng Kong"
    ],
    "arxiv_published": "2025-02-05T02:18:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03492v1",
        "arxiv_url": "http://arxiv.org/abs/2502.03492v1",
        "arxiv_title": "Teaching Language Models to Critique via Reinforcement Learning",
        "authors": [
          "Zhihui Xie",
          "Jie chen",
          "Liyu Chen",
          "Weichao Mao",
          "Jingjing Xu",
          "Lingpeng Kong"
        ],
        "published": "2025-02-05T02:18:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45868",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.",
    "is_llm_safety": true
  },
  {
    "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "url": "https://icml.cc/virtual/2025/poster/45435",
    "abstract": "A genuinely robust reasoning model should be able to function correctly when the problem statement is modified out-of-training-distribution. Prior work has shown that language models struggle on mathematical benchmarks when questions undergo simple perturbations -- modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycks et. al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06453v2",
    "arxiv_id": "2502.06453v2",
    "arxiv_title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "arxiv_authors": [
      "Kaixuan Huang",
      "Jiacheng Guo",
      "Zihao Li",
      "Xiang Ji",
      "Jiawei Ge",
      "Wenzhe Li",
      "Yingqing Guo",
      "Tianle Cai",
      "Hui Yuan",
      "Runzhe Wang",
      "Yue Wu",
      "Ming Yin",
      "Shange Tang",
      "Yangsibo Huang",
      "Chi Jin",
      "Xinyun Chen",
      "Chiyuan Zhang",
      "Mengdi Wang"
    ],
    "arxiv_published": "2025-02-10T13:31:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06453v2",
        "arxiv_url": "http://arxiv.org/abs/2502.06453v2",
        "arxiv_title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
        "authors": [
          "Kaixuan Huang",
          "Jiacheng Guo",
          "Zihao Li",
          "Xiang Ji",
          "Jiawei Ge",
          "Wenzhe Li",
          "Yingqing Guo",
          "Tianle Cai",
          "Hui Yuan",
          "Runzhe Wang",
          "Yue Wu",
          "Ming Yin",
          "Shange Tang",
          "Yangsibo Huang",
          "Chi Jin",
          "Xinyun Chen",
          "Chiyuan Zhang",
          "Mengdi Wang"
        ],
        "published": "2025-02-10T13:31:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Exploiting Presentative Feature Distributions for Parameter-Efficient Continual Learning of Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46354",
    "abstract": "Endowing large language models (LLMs) with continual learning (CL) capacities is practically important, which enables them to dynamically acquire new knowledge over time. Although many effective methods have been proposed for CL of LLMs, they share a common limitation: information leakage (IL), where the task-related information of learned tasks is accessed or reused again. IL not only imposes potential risks on data privacy protection but also significantly hinders the deployment of LLMs in real-world scenarios. To avoid IL while maintaining outstanding CL performance, we propose a novel CL method for LLMs, which first characterizes a parameter-efficient fine-tuning (PEFT) block by a presentative feature distribution, and then dynamically selects the appropriate PEFT blocks for each instance based on its similarity with the presentative feature distributions. Extensive experiments validate the effectiveness of our method on the CL of LLM, showcasing its potential to enhance both privacy and adaptability in practical applications.",
    "is_llm_safety": true
  },
  {
    "title": "Improving Model Alignment Through Collective Intelligence of Open-Source Models",
    "url": "https://icml.cc/virtual/2025/poster/45654",
    "abstract": "Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models fine-tuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.",
    "is_llm_safety": true
  },
  {
    "title": "How Useful are Your Jailbreak Outputs?",
    "url": "https://icml.cc/virtual/2025/poster/44418",
    "abstract": "Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs.In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions?Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math).Our evaluation of eight representative jailbreaks across five utility benchmarks shows vast disparities in the utility of model responses. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 97% in accuracy.Overall, our work proposes jailbreak utility as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks.",
    "is_llm_safety": true,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.10694v1",
        "arxiv_url": "http://arxiv.org/abs/2504.10694v1",
        "arxiv_title": "The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
        "authors": [
          "Kristina Nikolić",
          "Luze Sun",
          "Jie Zhang",
          "Florian Tramèr"
        ],
        "published": "2025-04-14T20:30:41+00:00",
        "similarity_score": 0.8
      }
    ]
  },
  {
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44282",
    "abstract": "Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving (49.71) win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis,  we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01926v1",
    "arxiv_id": "2503.01926v1",
    "arxiv_title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "arxiv_authors": [
      "Keyu Duan",
      "Yiran Zhao",
      "Zhili Feng",
      "Jinjie Ni",
      "Tianyu Pang",
      "Qian Liu",
      "Tianle Cai",
      "Longxu Dou",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "J. Zico Kolter",
      "Michael Qizhe Shieh"
    ],
    "arxiv_published": "2025-03-02T12:10:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01926v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01926v1",
        "arxiv_title": "Unnatural Languages Are Not Bugs but Features for LLMs",
        "authors": [
          "Keyu Duan",
          "Yiran Zhao",
          "Zhili Feng",
          "Jinjie Ni",
          "Tianyu Pang",
          "Qian Liu",
          "Tianle Cai",
          "Longxu Dou",
          "Kenji Kawaguchi",
          "Anirudh Goyal",
          "J. Zico Kolter",
          "Michael Qizhe Shieh"
        ],
        "published": "2025-03-02T12:10:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "url": "https://icml.cc/virtual/2025/poster/45726",
    "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \\texttt{RTO} against PPO and other direct preference learning algorithms. The results highlight the effectiveness of RTO, with the algorithm outperforming PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.18922v4",
    "arxiv_id": "2404.18922v4",
    "arxiv_title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "arxiv_authors": [
      "Han Zhong",
      "Zikang Shan",
      "Guhao Feng",
      "Wei Xiong",
      "Xinle Cheng",
      "Li Zhao",
      "Di He",
      "Jiang Bian",
      "Liwei Wang"
    ],
    "arxiv_published": "2024-04-29T17:58:30+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.18922v4",
        "arxiv_url": "http://arxiv.org/abs/2404.18922v4",
        "arxiv_title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
        "authors": [
          "Han Zhong",
          "Zikang Shan",
          "Guhao Feng",
          "Wei Xiong",
          "Xinle Cheng",
          "Li Zhao",
          "Di He",
          "Jiang Bian",
          "Liwei Wang"
        ],
        "published": "2024-04-29T17:58:30+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Robust Multimodal Large Language Models Against Modality Conflicts",
    "url": "https://icml.cc/virtual/2025/poster/45224",
    "abstract": "Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.",
    "is_llm_safety": true
  },
  {
    "title": "From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46073",
    "abstract": "As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model’s exhibited ToM by adjusting in the direction of the attention head.",
    "is_llm_safety": true
  },
  {
    "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
    "url": "https://icml.cc/virtual/2025/poster/45317",
    "abstract": "Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model’s alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.",
    "is_llm_safety": true
  },
  {
    "title": "InfAlign: Inference-aware language model alignment",
    "url": "https://icml.cc/virtual/2025/poster/44424",
    "abstract": "Language model alignment is a critical stepin training modern generative language models.Alignment targets to improve win rate of a samplefrom the aligned model against the base model.Today, we are increasingly using inference-timealgorithms (e.g., Best-of-$N$ , controlled decoding, tree search) to decode from language modelsrather than standard sampling. We show that thistrain/test mismatch makes standard RLHF framework sub-optimal in view of such inference-timemethods. To this end, we propose a framework forinference-aware alignment (InfAlign), whichaims to optimize *inference-time win rate* of thealigned policy against the base model. We provethat for any inference-time decoding procedure,the optimal aligned policy is the solution to thestandard RLHF problem with a *transformation*of the reward. This motivates us to provide thecalibrate-and-transform RL (InfAlign-CTRL)algorithm to solve this problem, which involvesa reward calibration step and a KL-regularizedreward maximization step with a transformationof the calibrated reward. For best-of-$N$ samplingand best-of-$N$ jailbreaking, we propose specifictransformations offering up to 3-8% improvementon inference-time win rates. Finally, we also showthat our proposed reward calibration method is astrong baseline for optimizing standard win rate.",
    "is_llm_safety": true
  },
  {
    "title": "RedCrowd: Adaptive Security for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44928",
    "abstract": "Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors:the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing RedCrowd, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using RedCrowd, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications.",
    "is_llm_safety": true
  },
  {
    "title": "Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44278",
    "abstract": "Quantized large language models (LLMs) have gained increasing attention and significance for enabling deployment in resource-constrained environments. However, emerging studies on a few calibration dataset-free quantization methods suggest that quantization may compromise the safety capabilities of LLMs, underscoring the urgent need for systematic safety evaluations and effective mitigation strategies. In this paper, we present comprehensive safety evaluations across various mainstream quantization techniques and diverse calibration datasets, utilizing widely accepted safety benchmarks. To address the identified safety vulnerabilities, we propose a quantization-aware safety patching framework, Q-resafe, to efficiently restore the safety capabilities of quantized LLMs while minimizing any adverse impact on utility.  Extensive experiment results demonstrate that Q-resafe successfully re-aligns the safety of quantized LLMs with their pre-quantization counterparts, even under challenging evaluation scenarios. Project page: https://anonymous.4open.science/r/Qresafe-code.",
    "is_llm_safety": true
  },
  {
    "title": "Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning",
    "url": "https://icml.cc/virtual/2025/poster/43777",
    "abstract": "Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.",
    "is_llm_safety": true
  },
  {
    "title": "Position: Iterative Online-Offline Joint Optimization is Needed to Manage Complex LLM Copyright Risks",
    "url": "https://icml.cc/virtual/2025/poster/40114",
    "abstract": "The infringement risks of LLMs have raised significant copyright concerns across different stages of the model lifecycle. While current methods often address these issues separately, this position paper argues that the LLM copyright challenges are inherently connected, and independent optimization of these solutions leads to theoretical bottlenecks. Building on this insight, we further argue that managing LLM copyright risks requires a systemic approach rather than fragmented solutions. In this paper, we analyze the limitations of existing methods in detail and introduce an iterative online-offline joint optimization framework to effectively manage complex LLM copyright risks. We demonstrate that this framework offers a scalable and practical solution to mitigate LLM infringement risks, and also outline new research directions that emerge from this perspective.",
    "is_llm_safety": true
  },
  {
    "title": "GuardAgent: Safeguard LLM Agents via Knowledge-Enabled Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/46569",
    "abstract": "The rapid advancement of large language model (LLM) agents has raised new concerns regarding their safety and security, which cannot be addressed by traditional textual-harm-focused LLM guardrails. We propose GuardAgent, the first guardrail agent to protect other agents by checking whether the agent actions satisfy safety guard requests. Specifically, GuardAgent first analyzes the safety guard requests to generate a task plan, and then converts this plan into guardrail code for execution. In both steps, an LLM is utilized as the reasoning component, supplemented by in-context demonstrations retrieved from a memory module storing information from previous tasks. GuardAgent can understand different safety guard requests and provide reliable code-based guardrails with high flexibility and low operational overhead. In addition, we propose two novel benchmarks: EICU-AC benchmark to assess the access control for healthcare agents and Mind2Web-SC benchmark to evaluate the safety regulations for web agents. We show that GuardAgent effectively moderates the violation actions for two types of agents on these two benchmarks with over 98% and 83% guardrail accuracies, respectively.",
    "is_llm_safety": true
  },
  {
    "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44213",
    "abstract": "Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code through an anonymous repository https://anonymous.4open.science/r/Snowball-Errors-and-Probability.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.15602v2",
    "arxiv_id": "2501.15602v2",
    "arxiv_title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "arxiv_authors": [
      "Zeyu Gan",
      "Yun Liao",
      "Yong Liu"
    ],
    "arxiv_published": "2025-01-26T17:05:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.15602v2",
        "arxiv_url": "http://arxiv.org/abs/2501.15602v2",
        "arxiv_title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
        "authors": [
          "Zeyu Gan",
          "Yun Liao",
          "Yong Liu"
        ],
        "published": "2025-01-26T17:05:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Retrieval-augmented systems are currently dangerous medical communicators",
    "url": "https://icml.cc/virtual/2025/poster/40149",
    "abstract": "Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. However, we argue that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations---such as the incorporation of communication pragmatics and enhanced comprehension of source documents---that could help mitigate these issues and extend beyond the medical domain.",
    "is_llm_safety": true
  },
  {
    "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45124",
    "abstract": "Existing efforts to align multimodal large language models (MLLMs) with human preferences have only achieved progress in narrow areas, such as hallucination reduction, but remain limited in practical applicability and generalizability. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality.  Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce the Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms.  Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs.  Our approach is rigorously evaluated across 10 distinct dimensions, encompassing 27 benchmarks, with results demonstrating significant and consistent improvements in model performance (Figure.1).",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.10391v1",
    "arxiv_id": "2502.10391v1",
    "arxiv_title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "arxiv_authors": [
      "Yi-Fan Zhang",
      "Tao Yu",
      "Haochen Tian",
      "Chaoyou Fu",
      "Peiyan Li",
      "Jianshu Zeng",
      "Wulin Xie",
      "Yang Shi",
      "Huanyu Zhang",
      "Junkang Wu",
      "Xue Wang",
      "Yibo Hu",
      "Bin Wen",
      "Fan Yang",
      "Zhang Zhang",
      "Tingting Gao",
      "Di Zhang",
      "Liang Wang",
      "Rong Jin",
      "Tieniu Tan"
    ],
    "arxiv_published": "2025-02-14T18:59:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.10391v1",
        "arxiv_url": "http://arxiv.org/abs/2502.10391v1",
        "arxiv_title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "authors": [
          "Yi-Fan Zhang",
          "Tao Yu",
          "Haochen Tian",
          "Chaoyou Fu",
          "Peiyan Li",
          "Jianshu Zeng",
          "Wulin Xie",
          "Yang Shi",
          "Huanyu Zhang",
          "Junkang Wu",
          "Xue Wang",
          "Yibo Hu",
          "Bin Wen",
          "Fan Yang",
          "Zhang Zhang",
          "Tingting Gao",
          "Di Zhang",
          "Liang Wang",
          "Rong Jin",
          "Tieniu Tan"
        ],
        "published": "2025-02-14T18:59:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning",
    "url": "https://icml.cc/virtual/2025/poster/43722",
    "abstract": "Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off-policy data for preference learning, others indicate that the advantages of on-policy data are task-dependent, highlighting the need for a systematic exploration of their interplay.In this work, we show that on-policy and off-policy data offer complementary strengths: on-policy data is particularly effective for reasoning tasks like math and coding, while off-policy data performs better on subjective tasks such as creative writing and making personal recommendations. Guided by these findings, we introduce SimpleMix, an approach to combine the complementary strengths of on-policy and off-policy preference learning by simply mixing these two data sources. Our empirical results across diverse tasks and benchmarks demonstrate that SimpleMix substantially improves language model alignment. Specifically, SimpleMix improves upon on-policy DPO and off-policy DPO by an average of 6.03 on Alpaca Eval 2.0. Moreover, it surpasses prior approaches that are much more complex in combining on- and off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05. These findings validate the effectiveness and efficiency of SimpleMix for enhancing preference-based alignment.",
    "is_llm_safety": true
  },
  {
    "title": "Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF",
    "url": "https://icml.cc/virtual/2025/poster/43585",
    "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning models, particularly Language Models (LMs) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices, e.g. compare two papers on their novelty, clarity, correctness, etc. Multi-Objective RLHF aims to use per-objective preference feedback and achieve a Pareto optimal tradeoff among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation  is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change. In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups. Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.",
    "is_llm_safety": true
  },
  {
    "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
    "url": "https://icml.cc/virtual/2025/poster/43951",
    "abstract": "Given how large parts of the publicly available text are crawled to pretrain large language models (LLMs), creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves generating multiple watermarked rephrases such that a distinct watermark is embedded in each rephrasing. One version is released publicly while others are kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that our approach preserves both the semantic meaning and the utility of benchmarks in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.13416v1",
    "arxiv_id": "2504.13416v1",
    "arxiv_title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
    "arxiv_authors": [
      "Saksham Rastogi",
      "Pratyush Maini",
      "Danish Pruthi"
    ],
    "arxiv_published": "2025-04-18T02:25:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.13416v1",
        "arxiv_url": "http://arxiv.org/abs/2504.13416v1",
        "arxiv_title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
        "authors": [
          "Saksham Rastogi",
          "Pratyush Maini",
          "Danish Pruthi"
        ],
        "published": "2025-04-18T02:25:08+00:00",
        "similarity_score": 1.0
      }
    ]
  }
]