[
  {
    "title": "An Efficient Private GPT Never Autoregressively Decodes",
    "url": "https://icml.cc/virtual/2025/poster/45418",
    "abstract": "The wide deployment of the generative pre-trained transformer (GPT) has raised privacy concerns for both clients and servers. While cryptographic primitives can be employed for secure GPT inference to protect the privacy of both parties, they introduce considerable performance overhead. To accelerate secure inference, this study proposes a public decoding and secure verification approach that utilizes public GPT models, motivated by the observation that securely decoding one and multiple tokens takes a similar latency. The client uses the public model to generate a set of tokens, which are then securely verified by the private model for acceptance. The efficiency of our approach depends on the acceptance ratio of tokens proposed by the public model, which we improve from two aspects: (1) a private sampling protocol optimized for cryptographic primitives and (2) model alignment using knowledge distillation. Our approach improves the efficiency of secure decoding while maintaining the same level of privacy and generation quality as standard secure decoding. Experiments demonstrate a $2.1\\times \\sim 6.0\\times$ speedup compared to standard decoding across three pairs of public-private models and different network conditions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.15252v1",
    "arxiv_id": "2505.15252v1",
    "arxiv_title": "An Efficient Private GPT Never Autoregressively Decodes",
    "arxiv_authors": [
      "Zhengyi Li",
      "Yue Guan",
      "Kang Yang",
      "Yu Feng",
      "Ning Liu",
      "Yu Yu",
      "Jingwen Leng",
      "Minyi Guo"
    ],
    "arxiv_published": "2025-05-21T08:28:56+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.15252v1",
        "arxiv_url": "http://arxiv.org/abs/2505.15252v1",
        "arxiv_title": "An Efficient Private GPT Never Autoregressively Decodes",
        "authors": [
          "Zhengyi Li",
          "Yue Guan",
          "Kang Yang",
          "Yu Feng",
          "Ning Liu",
          "Yu Yu",
          "Jingwen Leng",
          "Minyi Guo"
        ],
        "published": "2025-05-21T08:28:56+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "ROPO: Robust Preference Optimization for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46424",
    "abstract": "The prevalent noise in the preference data unavoidably poses significant challenges to the preference alignment of large language models (LLMs). Existing efforts for this problem either marginally alleviate the impact of noise without noise reduction, or rely on external LLMs that incur substantial computational costs. To address these challenges, we propose RObust Preference Optimization (ROPO), an iterative alignment approach that integrates noise-tolerance and noise filtering without the aid of external models. Specifically, ROPO first formulates the training process with adaptive noise reduction as an optimization problem, which can be efficiently solved in an iterative paradigm. Then, to equip this solving process with noise-tolerance and noise-identification capabilities, we derive a robust loss that suppresses the gradients from samples with high uncertainty. We demonstrate both empirically and theoretically that the derived loss is key to the noise-tolerance and effective filtering of noisy samples. The derived loss further inspires a robustness-guided rejection sampling technique to compensate for the potential important information in discarded queries. Extensive experiments on several widely-used datasets and model architectures demonstrate that ROPO significantly outperforms all baselines under four practical noise settings and the random symmetric noise, with its advantage increasing as the noise rate increases.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.04102v2",
    "arxiv_id": "2404.04102v2",
    "arxiv_title": "ROPO: Robust Preference Optimization for Large Language Models",
    "arxiv_authors": [
      "Xize Liang",
      "Chao Chen",
      "Shuang Qiu",
      "Jie Wang",
      "Yue Wu",
      "Zhihang Fu",
      "Zhihao Shi",
      "Feng Wu",
      "Jieping Ye"
    ],
    "arxiv_published": "2024-04-05T13:58:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.04102v2",
        "arxiv_url": "http://arxiv.org/abs/2404.04102v2",
        "arxiv_title": "ROPO: Robust Preference Optimization for Large Language Models",
        "authors": [
          "Xize Liang",
          "Chao Chen",
          "Shuang Qiu",
          "Jie Wang",
          "Yue Wu",
          "Zhihang Fu",
          "Zhihao Shi",
          "Feng Wu",
          "Jieping Ye"
        ],
        "published": "2024-04-05T13:58:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "url": "https://icml.cc/virtual/2025/poster/44897",
    "abstract": "Reinforcement learning (RL) training is inherently unstable due to factors such as moving targets and high gradient variance. Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) introduce additional challenges. For instance, diverse preferences complicate the alignment process, and prediction errors in a trained reward model can become more severe as the LLM generates unseen outputs. These RL challenges create confusion about whether the probability of an action for a given state should be increased or decreased, similar to the noise in labels for classification tasks. In this work, we focus on RL algorithms that share learning difficulties with cross-entropy loss, especially for low-probability predictions. To enhance stability, we adapt reverse cross-entropy (RCE) from supervised learning for noisy data, defining a symmetric RL loss. We demonstrate performance improvements across various tasks and scales. We conduct experiments in discrete action tasks (Atari games) and continuous action space tasks (MuJoCo benchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO). Notably, SPPO shows strong performance across different hyperparameters. Furthermore, we validate the symmetric RL loss in the RLHF framework using PPO for natural language processing tasks such as IMDB positive sentiment and TL;DR summarization.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.17618v2",
    "arxiv_id": "2405.17618v2",
    "arxiv_title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "arxiv_authors": [
      "Ju-Seung Byun",
      "Andrew Perrault"
    ],
    "arxiv_published": "2024-05-27T19:28:33+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.17618v2",
        "arxiv_url": "http://arxiv.org/abs/2405.17618v2",
        "arxiv_title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
        "authors": [
          "Ju-Seung Byun",
          "Andrew Perrault"
        ],
        "published": "2024-05-27T19:28:33+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts",
    "url": "https://icml.cc/virtual/2025/poster/46519",
    "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, V1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-$k$ with varying time budgets and agent designs, and find that the best AI agents achieve a score 4× higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2× the score of the top AI agent when both are given 32 total hours (across different attempts).",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.15114v1",
    "arxiv_id": "2411.15114v1",
    "arxiv_title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
    "arxiv_authors": [
      "Hjalmar Wijk",
      "Tao Lin",
      "Joel Becker",
      "Sami Jawhar",
      "Neev Parikh",
      "Thomas Broadley",
      "Lawrence Chan",
      "Michael Chen",
      "Josh Clymer",
      "Jai Dhyani",
      "Elena Ericheva",
      "Katharyn Garcia",
      "Brian Goodrich",
      "Nikola Jurkovic",
      "Megan Kinniment",
      "Aron Lajko",
      "Seraphina Nix",
      "Lucas Sato",
      "William Saunders",
      "Maksym Taran",
      "Ben West",
      "Elizabeth Barnes"
    ],
    "arxiv_published": "2024-11-22T18:30:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.15114v1",
        "arxiv_url": "http://arxiv.org/abs/2411.15114v1",
        "arxiv_title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts",
        "authors": [
          "Hjalmar Wijk",
          "Tao Lin",
          "Joel Becker",
          "Sami Jawhar",
          "Neev Parikh",
          "Thomas Broadley",
          "Lawrence Chan",
          "Michael Chen",
          "Josh Clymer",
          "Jai Dhyani",
          "Elena Ericheva",
          "Katharyn Garcia",
          "Brian Goodrich",
          "Nikola Jurkovic",
          "Megan Kinniment",
          "Aron Lajko",
          "Seraphina Nix",
          "Lucas Sato",
          "William Saunders",
          "Maksym Taran",
          "Ben West",
          "Elizabeth Barnes"
        ],
        "published": "2024-11-22T18:30:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "url": "https://icml.cc/virtual/2025/poster/45947",
    "abstract": "Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs. Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results. Code will be released soon.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06655v2",
    "arxiv_id": "2502.06655v2",
    "arxiv_title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
    "arxiv_authors": [
      "Meilin Chen",
      "Jian Tian",
      "Liang Ma",
      "Di Xie",
      "Weijie Chen",
      "Jiang Zhu"
    ],
    "arxiv_published": "2025-02-10T16:45:18+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06655v2",
        "arxiv_url": "http://arxiv.org/abs/2502.06655v2",
        "arxiv_title": "Unbiased Evaluation of Large Language Models from a Causal Perspective",
        "authors": [
          "Meilin Chen",
          "Jian Tian",
          "Liang Ma",
          "Di Xie",
          "Weijie Chen",
          "Jiang Zhu"
        ],
        "published": "2025-02-10T16:45:18+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Scaling Trends in Language Model Robustness",
    "url": "https://icml.cc/virtual/2025/poster/43784",
    "abstract": "Language models exhibit scaling laws, whereby increasing model and dataset size predictably decreases negative log likelihood, unlocking a dazzling array of capabilities. At the same time, even the most capable systems are currently vulnerable to adversarial inputs such as jailbreaks and prompt injections, despite concerted efforts to make them robust. As compute becomes more accessible to both attackers and defenders, which side will benefit more from scale?We attempt to answer this question with a detailed study of robustness on language models spanning three orders of magnitude in parameter count. From the defender's perspective, we find that in the absence of other interventions, increasing model size alone does not consistently improve robustness. In adversarial training, we find that larger models are more sample-efficient and less compute-efficient than smaller models, and often better generalize their defense to new threat models. From the attacker’s perspective, we find that increasing attack compute smoothly and reliably increases attack success rate against both finetuned and adversarially trained models. Finally, we show that across model sizes studied, doubling compute on adversarial training only forces an attacker to less than double attack compute to maintain the same attack success rate. However, adversarial training becomes more and more effective on larger models, suggesting that defenders could eventually have the advantage with increasing model size.These results underscore the value of adopting a scaling lens when discussing robustness of frontier models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2407.18213v4",
    "arxiv_id": "2407.18213v4",
    "arxiv_title": "Scaling Trends in Language Model Robustness",
    "arxiv_authors": [
      "Nikolaus Howe",
      "Ian McKenzie",
      "Oskar Hollinsworth",
      "Michał Zajac",
      "Tom Tseng",
      "Aaron Tucker",
      "Pierre-Luc Bacon",
      "Adam Gleave"
    ],
    "arxiv_published": "2024-07-25T17:26:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2407.18213v4",
        "arxiv_url": "http://arxiv.org/abs/2407.18213v4",
        "arxiv_title": "Scaling Trends in Language Model Robustness",
        "authors": [
          "Nikolaus Howe",
          "Ian McKenzie",
          "Oskar Hollinsworth",
          "Michał Zajac",
          "Tom Tseng",
          "Aaron Tucker",
          "Pierre-Luc Bacon",
          "Adam Gleave"
        ],
        "published": "2024-07-25T17:26:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Textual Unlearning Gives a False Sense of Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/44277",
    "abstract": "Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.13348v2",
    "arxiv_id": "2406.13348v2",
    "arxiv_title": "Textual Unlearning Gives a False Sense of Unlearning",
    "arxiv_authors": [
      "Jiacheng Du",
      "Zhibo Wang",
      "Jie Zhang",
      "Xiaoyi Pang",
      "Jiahui Hu",
      "Kui Ren"
    ],
    "arxiv_published": "2024-06-19T08:51:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.13348v2",
        "arxiv_url": "http://arxiv.org/abs/2406.13348v2",
        "arxiv_title": "Textual Unlearning Gives a False Sense of Unlearning",
        "authors": [
          "Jiacheng Du",
          "Zhibo Wang",
          "Jie Zhang",
          "Xiaoyi Pang",
          "Jiahui Hu",
          "Kui Ren"
        ],
        "published": "2024-06-19T08:51:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "url": "https://icml.cc/virtual/2025/poster/44337",
    "abstract": "The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make edits given code context, and solve algorithmic coding tasks. To achieve full automation however, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 challenging tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI’s o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by LLMs; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps toward autonomous and secure software development with LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.11844v2",
    "arxiv_id": "2502.11844v2",
    "arxiv_title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
    "arxiv_authors": [
      "Mark Vero",
      "Niels Mündler",
      "Victor Chibotaru",
      "Veselin Raychev",
      "Maximilian Baader",
      "Nikola Jovanović",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "arxiv_published": "2025-02-17T14:37:47+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.11844v2",
        "arxiv_url": "http://arxiv.org/abs/2502.11844v2",
        "arxiv_title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "authors": [
          "Mark Vero",
          "Niels Mündler",
          "Victor Chibotaru",
          "Veselin Raychev",
          "Maximilian Baader",
          "Nikola Jovanović",
          "Jingxuan He",
          "Martin Vechev"
        ],
        "published": "2025-02-17T14:37:47+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "When Bad Data Leads to Good Models",
    "url": "https://icml.cc/virtual/2025/poster/45199",
    "abstract": "In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we challenge the notion of ``quality'' in the context of post-training. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.04741v1",
    "arxiv_id": "2505.04741v1",
    "arxiv_title": "When Bad Data Leads to Good Models",
    "arxiv_authors": [
      "Kenneth Li",
      "Yida Chen",
      "Fernanda Viégas",
      "Martin Wattenberg"
    ],
    "arxiv_published": "2025-05-07T19:17:49+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.04741v1",
        "arxiv_url": "http://arxiv.org/abs/2505.04741v1",
        "arxiv_title": "When Bad Data Leads to Good Models",
        "authors": [
          "Kenneth Li",
          "Yida Chen",
          "Fernanda Viégas",
          "Martin Wattenberg"
        ],
        "published": "2025-05-07T19:17:49+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
    "url": "https://icml.cc/virtual/2025/poster/45579",
    "abstract": "As large language models (LLMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute—expensive and inflexible—or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable Generation), a novel framework that efficiently predicts EAP and adapts to new attributes through tractable generation and lightweight control. TRACE distills a Hidden Markov Model (HMM) from a language model and pairs it with a small classifier to score EAP, reweighting next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with negligible decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes. Our findings underscore TRACE’s strong performance, efficiency, flexibility, and compositionality for ensuring global attribute satisfaction.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.18535v1",
    "arxiv_id": "2504.18535v1",
    "arxiv_title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
    "arxiv_authors": [
      "Gwen Yidou Weng",
      "Benjie Wang",
      "Guy Van den Broeck"
    ],
    "arxiv_published": "2025-04-25T17:59:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.18535v1",
        "arxiv_url": "http://arxiv.org/abs/2504.18535v1",
        "arxiv_title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation",
        "authors": [
          "Gwen Yidou Weng",
          "Benjie Wang",
          "Guy Van den Broeck"
        ],
        "published": "2025-04-25T17:59:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "url": "https://icml.cc/virtual/2025/poster/45356",
    "abstract": "Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted outputs, posing a serious threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This disrupts the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the \"unsafe\" prediction rate, bypassing existing safeguards.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.01077v2",
    "arxiv_id": "2411.01077v2",
    "arxiv_title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
    "arxiv_authors": [
      "Zhipeng Wei",
      "Yuqi Liu",
      "N. Benjamin Erichson"
    ],
    "arxiv_published": "2024-11-01T23:18:32+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.01077v2",
        "arxiv_url": "http://arxiv.org/abs/2411.01077v2",
        "arxiv_title": "Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection",
        "authors": [
          "Zhipeng Wei",
          "Yuqi Liu",
          "N. Benjamin Erichson"
        ],
        "published": "2024-11-01T23:18:32+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/44285",
    "abstract": "While the capabilities of generative foundational models have advanced rapidly in recent years, methods to prevent harmful and unsafe behaviors remain underdeveloped. Among the pressing challenges in AI safety, machine unlearning (MU) has become increasingly critical to meet upcoming safety regulations. Most existing MU approaches focus on altering the most significant parameters of the model. However, these methods often require fine-tuning substantial portions of the model, resulting in high computational costs and training instabilities, which are typically mitigated by access to the original training dataset.In this work, we address these limitations by leveraging Singular Value Decomposition (SVD) to create a compact, low-dimensional projection that enables the selective forgetting of specific data points. We propose Singular Value Decomposition for Efficient Machine Unlearning (SEMU), a novel approach designed to optimize MU in two key aspects. First, SEMU minimizes the number of model parameters that need to be modified, effectively removing unwanted knowledge while making only minimal changes to the model's weights. Second, SEMU eliminates the dependency on the original training dataset, preserving the model's previously acquired knowledge without additional data requirements.Extensive experiments demonstrate that SEMU achieves competitive performance while significantly improving efficiency in terms of both data usage and the number of modified parameters.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07587v1",
    "arxiv_id": "2502.07587v1",
    "arxiv_title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
    "arxiv_authors": [
      "Marcin Sendera",
      "Łukasz Struski",
      "Kamil Książek",
      "Kryspin Musiol",
      "Jacek Tabor",
      "Dawid Rymarczyk"
    ],
    "arxiv_published": "2025-02-11T14:36:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07587v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07587v1",
        "arxiv_title": "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
        "authors": [
          "Marcin Sendera",
          "Łukasz Struski",
          "Kamil Książek",
          "Kryspin Musiol",
          "Jacek Tabor",
          "Dawid Rymarczyk"
        ],
        "published": "2025-02-11T14:36:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "How Much Can We Forget about Data Contamination?",
    "url": "https://icml.cc/virtual/2025/poster/45377",
    "abstract": "The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B,  have forgotten the data seen at the beginning of training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.03249v3",
    "arxiv_id": "2410.03249v3",
    "arxiv_title": "How Much Can We Forget about Data Contamination?",
    "arxiv_authors": [
      "Sebastian Bordt",
      "Suraj Srinivas",
      "Valentyn Boreiko",
      "Ulrike von Luxburg"
    ],
    "arxiv_published": "2024-10-04T09:14:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.03249v3",
        "arxiv_url": "http://arxiv.org/abs/2410.03249v3",
        "arxiv_title": "How Much Can We Forget about Data Contamination?",
        "authors": [
          "Sebastian Bordt",
          "Suraj Srinivas",
          "Valentyn Boreiko",
          "Ulrike von Luxburg"
        ],
        "published": "2024-10-04T09:14:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Observation Interference in Partially Observable Assistance Games",
    "url": "https://icml.cc/virtual/2025/poster/43874",
    "abstract": "We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of perfect information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of perfect information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.17797v1",
    "arxiv_id": "2412.17797v1",
    "arxiv_title": "Observation Interference in Partially Observable Assistance Games",
    "arxiv_authors": [
      "Scott Emmons",
      "Caspar Oesterheld",
      "Vincent Conitzer",
      "Stuart Russell"
    ],
    "arxiv_published": "2024-12-23T18:53:33+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.17797v1",
        "arxiv_url": "http://arxiv.org/abs/2412.17797v1",
        "arxiv_title": "Observation Interference in Partially Observable Assistance Games",
        "authors": [
          "Scott Emmons",
          "Caspar Oesterheld",
          "Vincent Conitzer",
          "Stuart Russell"
        ],
        "published": "2024-12-23T18:53:33+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
    "url": "https://icml.cc/virtual/2025/poster/43693",
    "abstract": "A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.03869v5",
    "arxiv_id": "2405.03869v5",
    "arxiv_title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
    "arxiv_authors": [
      "Anshuman Chhabra",
      "Bo Li",
      "Jian Chen",
      "Prasant Mohapatra",
      "Hongfu Liu"
    ],
    "arxiv_published": "2024-05-06T21:34:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.03869v5",
        "arxiv_url": "http://arxiv.org/abs/2405.03869v5",
        "arxiv_title": "Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models",
        "authors": [
          "Anshuman Chhabra",
          "Bo Li",
          "Jian Chen",
          "Prasant Mohapatra",
          "Hongfu Liu"
        ],
        "published": "2024-05-06T21:34:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "url": "https://icml.cc/virtual/2025/poster/45659",
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative.In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03699v1",
    "arxiv_id": "2502.03699v1",
    "arxiv_title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
    "arxiv_authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Zhen Qin",
      "Ziqi Wang",
      "Wei Xiong",
      "Yu Meng",
      "Jiawei Han",
      "Sercan O. Arik"
    ],
    "arxiv_published": "2025-02-06T01:22:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03699v1",
        "arxiv_url": "http://arxiv.org/abs/2502.03699v1",
        "arxiv_title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
        "authors": [
          "Bowen Jin",
          "Jinsung Yoon",
          "Zhen Qin",
          "Ziqi Wang",
          "Wei Xiong",
          "Yu Meng",
          "Jiawei Han",
          "Sercan O. Arik"
        ],
        "published": "2025-02-06T01:22:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
    "url": "https://icml.cc/virtual/2025/poster/43898",
    "abstract": "Reward models are widely used as proxies for human preferences when aligning or evaluating LLMs.However, reward models are black boxes, and it is often unclear what, exactly, they are actually rewarding. In this paper we develop Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for measuring the sensitivity of a reward model to high-level attributes of responses, such as sentiment, helpfulness, or complexity. Importantly, RATE measures the causal effect of an attribute on the reward. RATE uses LLMs to rewrite responses to produce imperfect counterfactuals examples that can be used to measure causal effects. A key challenge is that these rewrites are imperfect in a manner that can induce substantial bias in the estimated sensitivity of the reward model to the attribute. The core idea of RATE is to adjust for this imperfect-rewrite effect by rewriting twice. We establish the validity of the RATE procedure and show empirically that it is an effective estimator.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.11348v3",
    "arxiv_id": "2410.11348v3",
    "arxiv_title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
    "arxiv_authors": [
      "David Reber",
      "Sean Richardson",
      "Todd Nief",
      "Cristina Garbacea",
      "Victor Veitch"
    ],
    "arxiv_published": "2024-10-15T07:22:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.11348v3",
        "arxiv_url": "http://arxiv.org/abs/2410.11348v3",
        "arxiv_title": "RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals",
        "authors": [
          "David Reber",
          "Sean Richardson",
          "Todd Nief",
          "Cristina Garbacea",
          "Victor Veitch"
        ],
        "published": "2024-10-15T07:22:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44613",
    "abstract": "Large Language Models (LLMs) are vulnerable to jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires a time-consuming search for adversarial prompts, whereas automatic adversarial prompt generation often leads to semantically meaningless attacks that do not scale well.In this paper, we present a novel method that uses another LLM, called AdvPrompter, to generate human-readable adversarial prompts in seconds.AdvPrompter, which is trained using an alternating optimization algorithm, generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response.Experimental results on popular open source TargetLLM show highly competitive results on the AdvBench and HarmBench datasets, that also transfer to closed-source black-box LLMs.We also show that training on adversarial suffixes generated by AdvPrompter is a promising strategy for improving the robustness of LLMs to jailbreaking attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.16873v1",
    "arxiv_id": "2404.16873v1",
    "arxiv_title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
    "arxiv_authors": [
      "Anselm Paulus",
      "Arman Zharmagambetov",
      "Chuan Guo",
      "Brandon Amos",
      "Yuandong Tian"
    ],
    "arxiv_published": "2024-04-21T22:18:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.16873v1",
        "arxiv_url": "http://arxiv.org/abs/2404.16873v1",
        "arxiv_title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
        "authors": [
          "Anselm Paulus",
          "Arman Zharmagambetov",
          "Chuan Guo",
          "Brandon Amos",
          "Yuandong Tian"
        ],
        "published": "2024-04-21T22:18:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Scalably Solving Assistance Games",
    "url": "https://icml.cc/virtual/2025/poster/44757",
    "abstract": "Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Code and videos are available at https://anonymous.4open.science/w/scalably-solving-assistance-games/.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.07091v1",
    "arxiv_id": "2504.07091v1",
    "arxiv_title": "AssistanceZero: Scalably Solving Assistance Games",
    "arxiv_authors": [
      "Cassidy Laidlaw",
      "Eli Bronstein",
      "Timothy Guo",
      "Dylan Feng",
      "Lukas Berglund",
      "Justin Svegliato",
      "Stuart Russell",
      "Anca Dragan"
    ],
    "arxiv_published": "2025-04-09T17:59:03+00:00",
    "similarity_score": 0.8048780487804879,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.07091v1",
        "arxiv_url": "http://arxiv.org/abs/2504.07091v1",
        "arxiv_title": "AssistanceZero: Scalably Solving Assistance Games",
        "authors": [
          "Cassidy Laidlaw",
          "Eli Bronstein",
          "Timothy Guo",
          "Dylan Feng",
          "Lukas Berglund",
          "Justin Svegliato",
          "Stuart Russell",
          "Anca Dragan"
        ],
        "published": "2025-04-09T17:59:03+00:00",
        "similarity_score": 0.8048780487804879
      }
    ]
  },
  {
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44422",
    "abstract": "Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment.  HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes “hard” dispreferred responses—those closely resembling preferred ones—to enhance the model’s rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS’s effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.14400v1",
    "arxiv_id": "2502.14400v1",
    "arxiv_title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "arxiv_authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "arxiv_published": "2025-02-20T09:37:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.14400v1",
        "arxiv_url": "http://arxiv.org/abs/2502.14400v1",
        "arxiv_title": "HPS: Hard Preference Sampling for Human Preference Alignment",
        "authors": [
          "Xiandong Zou",
          "Wanyu Lin",
          "Yuchen Li",
          "Pan Zhou"
        ],
        "published": "2025-02-20T09:37:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Adversaries Can Misuse Combinations of Safe Models",
    "url": "https://icml.cc/virtual/2025/poster/45845",
    "abstract": "Developers try to evaluate whether an AI system can accomplish malicious tasks before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for such misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.14595v2",
    "arxiv_id": "2406.14595v2",
    "arxiv_title": "Adversaries Can Misuse Combinations of Safe Models",
    "arxiv_authors": [
      "Erik Jones",
      "Anca Dragan",
      "Jacob Steinhardt"
    ],
    "arxiv_published": "2024-06-20T17:43:18+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.14595v2",
        "arxiv_url": "http://arxiv.org/abs/2406.14595v2",
        "arxiv_title": "Adversaries Can Misuse Combinations of Safe Models",
        "authors": [
          "Erik Jones",
          "Anca Dragan",
          "Jacob Steinhardt"
        ],
        "published": "2024-06-20T17:43:18+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Topological Signatures of Adversaries in Multimodal Alignments",
    "url": "https://icml.cc/virtual/2025/poster/46109",
    "abstract": "Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to back-propagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18006v1",
    "arxiv_id": "2501.18006v1",
    "arxiv_title": "Topological Signatures of Adversaries in Multimodal Alignments",
    "arxiv_authors": [
      "Minh Vu",
      "Geigh Zollicoffer",
      "Huy Mai",
      "Ben Nebgen",
      "Boian Alexandrov",
      "Manish Bhattarai"
    ],
    "arxiv_published": "2025-01-29T21:45:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18006v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18006v1",
        "arxiv_title": "Topological Signatures of Adversaries in Multimodal Alignments",
        "authors": [
          "Minh Vu",
          "Geigh Zollicoffer",
          "Huy Mai",
          "Ben Nebgen",
          "Boian Alexandrov",
          "Manish Bhattarai"
        ],
        "published": "2025-01-29T21:45:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "url": "https://icml.cc/virtual/2025/poster/43805",
    "abstract": "Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks.These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks.In reality, these PTLMs can be adapted to many other unrelated downstream tasks.Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness.We refer to this phenomenon as backdoor complications.In this paper, we undertake the first comprehensive quantification of backdoor complications.Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs.The output distribution of triggered samples significantly deviates from that of clean samples.Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks.The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.11586v1",
    "arxiv_id": "2505.11586v1",
    "arxiv_title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
    "arxiv_authors": [
      "Rui Zhang",
      "Yun Shen",
      "Hongwei Li",
      "Wenbo Jiang",
      "Hanxiao Chen",
      "Yuan Zhang",
      "Guowen Xu",
      "Yang Zhang"
    ],
    "arxiv_published": "2025-05-16T17:59:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.11586v1",
        "arxiv_url": "http://arxiv.org/abs/2505.11586v1",
        "arxiv_title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
        "authors": [
          "Rui Zhang",
          "Yun Shen",
          "Hongwei Li",
          "Wenbo Jiang",
          "Hanxiao Chen",
          "Yuan Zhang",
          "Guowen Xu",
          "Yang Zhang"
        ],
        "published": "2025-05-16T17:59:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "url": "https://icml.cc/virtual/2025/poster/46174",
    "abstract": "Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction.To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm namely robust contextual dueling bandits (\\algo), which is based on uncertainty-weighted maximum likelihood estimation.  Our algorithm achieves an $\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the derivative of the link function, and $  0 \\le C \\le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives into maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm \\algo \\ against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.10776v2",
    "arxiv_id": "2404.10776v2",
    "arxiv_title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
    "arxiv_authors": [
      "Qiwei Di",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "arxiv_published": "2024-04-16T17:59:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.10776v2",
        "arxiv_url": "http://arxiv.org/abs/2404.10776v2",
        "arxiv_title": "Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback",
        "authors": [
          "Qiwei Di",
          "Jiafan He",
          "Quanquan Gu"
        ],
        "published": "2024-04-16T17:59:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AMPO: Active Multi Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/45443",
    "abstract": "Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, making it computationally infeasible to include all of them in the training objective. We propose Active Multi-Preference Optimization (AMPO), which combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses, then pick a small but informative subset—covering reward extremes and distinct semantic clusters—for preference optimization.The resulting contrastive-training scheme identifies not only the best and worst answers but also subtle, underexplored modes crucial for robust alignment. Theoretically, we provide guarantees of expected reward maximization using our active selection method.Empirically, AMPO achieves state-of-the-art results on AlpacaEval with Llama 8B.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.18293v1",
    "arxiv_id": "2502.18293v1",
    "arxiv_title": "AMPO: Active Multi-Preference Optimization",
    "arxiv_authors": [
      "Taneesh Gupta",
      "Rahul Madhavan",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Saravan Rajmohan"
    ],
    "arxiv_published": "2025-02-25T15:29:51+00:00",
    "similarity_score": 0.9761904761904762,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.18293v1",
        "arxiv_url": "http://arxiv.org/abs/2502.18293v1",
        "arxiv_title": "AMPO: Active Multi-Preference Optimization",
        "authors": [
          "Taneesh Gupta",
          "Rahul Madhavan",
          "Xuchao Zhang",
          "Chetan Bansal",
          "Saravan Rajmohan"
        ],
        "published": "2025-02-25T15:29:51+00:00",
        "similarity_score": 0.9761904761904762
      }
    ]
  },
  {
    "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "url": "https://icml.cc/virtual/2025/poster/44896",
    "abstract": "Recent advances in code-specific large language models (LLMs) have greatly enhanced code generation and refinement capabilities. However, the safety of code LLMs remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Previous work proposes to collect security-focused instruction-tuning dataset from real-world vulnerabilities. It is constrained by the data sparsity of vulnerable code, and has limited applicability in the iterative post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing error-inducing coding scenarios from Common Weakness Enumerations (CWEs), and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through advanced preference learning objectives. The scenarios synthesized by ProSec triggers 25 times more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7 times larger than the previous work. Experiments show that models trained with ProSec are 25.2\\% to 91.4\\% more secure compared to previous work without degrading models' utility.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12882v2",
    "arxiv_id": "2411.12882v2",
    "arxiv_title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "arxiv_authors": [
      "Xiangzhe Xu",
      "Zian Su",
      "Jinyao Guo",
      "Kaiyuan Zhang",
      "Zhenting Wang",
      "Xiangyu Zhang"
    ],
    "arxiv_published": "2024-11-19T22:00:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12882v2",
        "arxiv_url": "http://arxiv.org/abs/2411.12882v2",
        "arxiv_title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
        "authors": [
          "Xiangzhe Xu",
          "Zian Su",
          "Jinyao Guo",
          "Kaiyuan Zhang",
          "Zhenting Wang",
          "Xiangyu Zhang"
        ],
        "published": "2024-11-19T22:00:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "De-mark: Watermark Removal in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46417",
    "abstract": "Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models (LMs). However, the robustness of the watermarking schemes has not been well explored. In this paper, we present De-mark, an advanced framework designed to remove n-gram-based watermarks effectively. Our method utilizes a novel querying strategy, termed random selection probing, which aids in assessing the strength of the watermark and identifying the red-green list within the n-gram watermark. Experiments on popular LMs, such as Llama3 and ChatGPT, demonstrate the efficiency and effectiveness of \\methodname\\ in watermark removal and exploitation tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.13808v1",
    "arxiv_id": "2410.13808v1",
    "arxiv_title": "De-mark: Watermark Removal in Large Language Models",
    "arxiv_authors": [
      "Ruibo Chen",
      "Yihan Wu",
      "Junfeng Guo",
      "Heng Huang"
    ],
    "arxiv_published": "2024-10-17T17:42:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.13808v1",
        "arxiv_url": "http://arxiv.org/abs/2410.13808v1",
        "arxiv_title": "De-mark: Watermark Removal in Large Language Models",
        "authors": [
          "Ruibo Chen",
          "Yihan Wu",
          "Junfeng Guo",
          "Heng Huang"
        ],
        "published": "2024-10-17T17:42:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
    "url": "https://icml.cc/virtual/2025/poster/46684",
    "abstract": "Warning: Contains harmful model outputs.Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges.Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.14230v3",
    "arxiv_id": "2406.14230v3",
    "arxiv_title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
    "arxiv_authors": [
      "Han Jiang",
      "Xiaoyuan Yi",
      "Zhihua Wei",
      "Ziang Xiao",
      "Shu Wang",
      "Xing Xie"
    ],
    "arxiv_published": "2024-06-20T11:51:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.14230v3",
        "arxiv_url": "http://arxiv.org/abs/2406.14230v3",
        "arxiv_title": "Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing",
        "authors": [
          "Han Jiang",
          "Xiaoyuan Yi",
          "Zhihua Wei",
          "Ziang Xiao",
          "Shu Wang",
          "Xing Xie"
        ],
        "published": "2024-06-20T11:51:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
    "url": "https://icml.cc/virtual/2025/poster/44101",
    "abstract": "Preference learning is critical for aligning large language models (LLMs) with human values, with the quality of preference datasets playing a crucial role in this process. While existing metrics primarily assess data quality based on either explicit or implicit reward margins, they often provide contradictory evaluations for the same data.To address this issue, we introduce the alignment potential metric, which quantifies the gap from the model's current implicit reward margin to the target explicit reward margin, thereby estimating the model's potential to align with the preference data.Empirical results demonstrate that training on data selected by this metric consistently enhances alignment performance, surpassing existing metrics across different base models and optimization objectives.Furthermore, our method extends to self-play data generation frameworks, where the metric is used to identify high-quality data within the self-generated content by LLMs. Under this data generation scenario, our method surpasses current state-of-the-art (SOTA) results across various training settings and demonstrates continuous improvements in alignment performance as dataset size and training iterations increase.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01864v1",
    "arxiv_id": "2503.01864v1",
    "arxiv_title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
    "arxiv_authors": [
      "Kexin Huang",
      "Junkang Wu",
      "Ziqian Chen",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Jiancan Wu",
      "Xiangnan He",
      "Xiang Wang"
    ],
    "arxiv_published": "2025-02-25T06:43:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01864v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01864v1",
        "arxiv_title": "Larger or Smaller Reward Margins to Select Preferences for Alignment?",
        "authors": [
          "Kexin Huang",
          "Junkang Wu",
          "Ziqian Chen",
          "Xue Wang",
          "Jinyang Gao",
          "Bolin Ding",
          "Jiancan Wu",
          "Xiangnan He",
          "Xiang Wang"
        ],
        "published": "2025-02-25T06:43:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Quantifying perturbation impacts for large language models",
    "url": "https://icml.cc/virtual/2025/poster/45964",
    "abstract": "We address the challenge of measuring how input perturbations impact large language model (LLM) outputs, a fundamental task for model reliability and post-hoc interpretability. A key obstacle in this domain is disentangling the meaningful changes in model responses from the intrinsic stochasticity of LLM outputs. To overcome this, we introduce Distribution-Based Perturbation Analysis (DBPA), a framework that reformulates LLM perturbation analysis as a frequentist hypothesis testing problem. DBPA constructs empirical null and alternative output distributions within a low-dimensional semantic similarity space via Monte Carlo sampling. Comparisons of Monte Carlo estimates in the new space enables tractable frequentist inference without relying on restrictive distributional assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation of arbitrary input perturbations on any black-box LLM, (iii) yields interpretable p-values, (iv) supports multiple perturbation testing via controlled error rates,  and (v) provides scalar effect sizes for any chosen similarity or distance metric. We demonstrate the effectiveness of DBPA in evaluating perturbation impacts across multiple case studies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.00868v1",
    "arxiv_id": "2412.00868v1",
    "arxiv_title": "Quantifying perturbation impacts for large language models",
    "arxiv_authors": [
      "Paulius Rauba",
      "Qiyao Wei",
      "Mihaela van der Schaar"
    ],
    "arxiv_published": "2024-12-01T16:13:09+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.00868v1",
        "arxiv_url": "http://arxiv.org/abs/2412.00868v1",
        "arxiv_title": "Quantifying perturbation impacts for large language models",
        "authors": [
          "Paulius Rauba",
          "Qiyao Wei",
          "Mihaela van der Schaar"
        ],
        "published": "2024-12-01T16:13:09+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "url": "https://icml.cc/virtual/2025/poster/44849",
    "abstract": "Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-Instruct-8B). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment algorithms against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for responsible deployment of powerful LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.04993v1",
    "arxiv_id": "2505.04993v1",
    "arxiv_title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
    "arxiv_authors": [
      "Zhuocheng Gong",
      "Jian Guan",
      "Wei Wu",
      "Huishuai Zhang",
      "Dongyan Zhao"
    ],
    "arxiv_published": "2025-05-08T06:59:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.04993v1",
        "arxiv_url": "http://arxiv.org/abs/2505.04993v1",
        "arxiv_title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
        "authors": [
          "Zhuocheng Gong",
          "Jian Guan",
          "Wei Wu",
          "Huishuai Zhang",
          "Dongyan Zhao"
        ],
        "published": "2025-05-08T06:59:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
    "url": "https://icml.cc/virtual/2025/poster/43619",
    "abstract": "Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that quantifies dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00678v2",
    "arxiv_id": "2502.00678v2",
    "arxiv_title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
    "arxiv_authors": [
      "Hyeong Kyu Choi",
      "Maxim Khanov",
      "Hongxin Wei",
      "Yixuan Li"
    ],
    "arxiv_published": "2025-02-02T05:50:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00678v2",
        "arxiv_url": "http://arxiv.org/abs/2502.00678v2",
        "arxiv_title": "How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence",
        "authors": [
          "Hyeong Kyu Choi",
          "Maxim Khanov",
          "Hongxin Wei",
          "Yixuan Li"
        ],
        "published": "2025-02-02T05:50:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "url": "https://icml.cc/virtual/2025/poster/45208",
    "abstract": "In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low error on the data distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distributional shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. We hope our results stimulate the theoretical and empirical study of improved methods to learn reward models, and better ways to measure their quality reliably.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2406.15753v2",
    "arxiv_id": "2406.15753v2",
    "arxiv_title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "arxiv_authors": [
      "Lukas Fluri",
      "Leon Lang",
      "Alessandro Abate",
      "Patrick Forré",
      "David Krueger",
      "Joar Skalse"
    ],
    "arxiv_published": "2024-06-22T06:43:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2406.15753v2",
        "arxiv_url": "http://arxiv.org/abs/2406.15753v2",
        "arxiv_title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
        "authors": [
          "Lukas Fluri",
          "Leon Lang",
          "Alessandro Abate",
          "Patrick Forré",
          "David Krueger",
          "Joar Skalse"
        ],
        "published": "2024-06-22T06:43:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
    "url": "https://icml.cc/virtual/2025/poster/46119",
    "abstract": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. However, recent work has shown that applying KE can adversely affect models' broader factual recall accuracy and diminish their reasoning abilities. Although these studies give insights into the potential harms of KE algorithms, e.g., performance evaluations on benchmarks, little is understood about why such destructive failures occur. Motivated by this, we define a novel synthetic task in which a Transformer is trained from scratch to internalize a \"structured\" knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has \"trickling effects\" on other entities (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models on this task, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it degrades models' factual recall and reasoning performance. We further corroborate our findings in naturalistic settings with pre-trained Llama and Mamba models as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model abilities.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.17194v3",
    "arxiv_id": "2410.17194v3",
    "arxiv_title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
    "arxiv_authors": [
      "Kento Nishi",
      "Maya Okawa",
      "Rahul Ramesh",
      "Mikail Khona",
      "Hidenori Tanaka",
      "Ekdeep Singh Lubana"
    ],
    "arxiv_published": "2024-10-22T17:13:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.17194v3",
        "arxiv_url": "http://arxiv.org/abs/2410.17194v3",
        "arxiv_title": "Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing",
        "authors": [
          "Kento Nishi",
          "Maya Okawa",
          "Rahul Ramesh",
          "Mikail Khona",
          "Hidenori Tanaka",
          "Ekdeep Singh Lubana"
        ],
        "published": "2024-10-22T17:13:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Preference learning made easy: Everything should be understood through win rate",
    "url": "https://icml.cc/virtual/2025/poster/45135",
    "abstract": "There has been an explosion of research on the topic of learning from preference data, following the success of RLHF in high-profile language model training efforts. However, preference learning is far from standardized, making it difficult to understand where to focus efforts with respect to current practice or research investment. This work presents a framework to understand preference learning from the ground up, starting from the sampling distribution of pairwise preference comparison data. First, we show that the only evaluation of a generative model that respects both preferences and prevalences in the preference data sampling distribution is win rate. This result suggests that everything should be understood through win rate. Thus, we characterize the space of preference learning into win rate optimization (WRO) and non-WRO objectives, highlighting the theoretical benefits of examples in the former and limitations of examples in the latter. We additionally conduct an empirical analysis of WRO and non-WRO objectives which highlights the practical importance of ease and success of optimization. Our analysis provides insights for existing practice as well as concrete guidance for future research---namely, future efforts should focus on either developing objectives more closely aligned with WRO or improving the optimization of WRO objectives.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.10505v1",
    "arxiv_id": "2502.10505v1",
    "arxiv_title": "Preference learning made easy: Everything should be understood through win rate",
    "arxiv_authors": [
      "Lily H. Zhang",
      "Rajesh Ranganath"
    ],
    "arxiv_published": "2025-02-14T19:01:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.10505v1",
        "arxiv_url": "http://arxiv.org/abs/2502.10505v1",
        "arxiv_title": "Preference learning made easy: Everything should be understood through win rate",
        "authors": [
          "Lily H. Zhang",
          "Rajesh Ranganath"
        ],
        "published": "2025-02-14T19:01:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44921",
    "abstract": "Watermarking, the process by which Large Language Model (LLM) servers imbed an imperceptible signal at inference time in order to detect text generated by their own models, has grown in importance due to the significant improvements in natural language processing tasks by modern LLMs. Current approaches are often impractical due to generation latency, detection time, degradation in text quality, or robustness; such problems often arise due to the focus on token level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We provide formal statistical bounds on the validity and power of our procedure and, through an extensive suite of experiments, demonstrate that GaussMark is reliable, efficient, relatively robust to corruption, and can be instantiated with essentially no loss in model quality.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.13941v1",
    "arxiv_id": "2501.13941v1",
    "arxiv_title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
    "arxiv_authors": [
      "Adam Block",
      "Ayush Sekhari",
      "Alexander Rakhlin"
    ],
    "arxiv_published": "2025-01-17T22:30:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.13941v1",
        "arxiv_url": "http://arxiv.org/abs/2501.13941v1",
        "arxiv_title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models",
        "authors": [
          "Adam Block",
          "Ayush Sekhari",
          "Alexander Rakhlin"
        ],
        "published": "2025-01-17T22:30:08+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "url": "https://icml.cc/virtual/2025/poster/46254",
    "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models.We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.12949v2",
    "arxiv_id": "2410.12949v2",
    "arxiv_title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "arxiv_authors": [
      "Phillip Guo",
      "Aaquib Syed",
      "Abhay Sheshadri",
      "Aidan Ewart",
      "Gintare Karolina Dziugaite"
    ],
    "arxiv_published": "2024-10-16T18:35:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.12949v2",
        "arxiv_url": "http://arxiv.org/abs/2410.12949v2",
        "arxiv_title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
        "authors": [
          "Phillip Guo",
          "Aaquib Syed",
          "Abhay Sheshadri",
          "Aidan Ewart",
          "Gintare Karolina Dziugaite"
        ],
        "published": "2024-10-16T18:35:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
    "url": "https://icml.cc/virtual/2025/poster/43771",
    "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting, thus adapting to evolving requirements. In this paper, we explore the forgetting caused by such incremental training, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model’s knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks’ answer styles, making the results unusable. On the other hand, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can conceal the model’s knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for data style transformations across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization to LoRA’s weight update matrices, enabling the model to retain existing competencies while remaining adaptable to new tasks. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.02486v1",
    "arxiv_id": "2505.02486v1",
    "arxiv_title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
    "arxiv_authors": [
      "Jinpeng Chen",
      "Runmin Cong",
      "Yuzhi Zhao",
      "Hongzheng Yang",
      "Guangneng Hu",
      "Horace Ho Shing Ip",
      "Sam Kwong"
    ],
    "arxiv_published": "2025-05-05T09:09:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.02486v1",
        "arxiv_url": "http://arxiv.org/abs/2505.02486v1",
        "arxiv_title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
        "authors": [
          "Jinpeng Chen",
          "Runmin Cong",
          "Yuzhi Zhao",
          "Hongzheng Yang",
          "Guangneng Hu",
          "Horace Ho Shing Ip",
          "Sam Kwong"
        ],
        "published": "2025-05-05T09:09:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Adversarial Reasoning at Jailbreaking Time",
    "url": "https://icml.cc/virtual/2025/poster/44790",
    "abstract": "As large language models (LLMs) are becoming more capable and widespread, the study of their failure cases is becoming increasingly important. Recent advances in standardizing, measuring, and scaling test-time compute suggest new methodologies for optimizing models to achieve high performance on hard tasks.In this paper, we apply these advances to the task of \"model jailbreaking\": eliciting harmful responses from aligned LLMs.We develop an adversarial reasoning approach to automatic jailbreaking via test-time computation that achieves SOTA attack success rates (ASR) against many aligned LLMs, even the ones that aim to trade inference-time compute for adversarial robustness.Our approach introduces a new paradigm in understanding LLM vulnerabilities,  laying the foundation for the development of more robust and trustworthy AI systems.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.01633v1",
    "arxiv_id": "2502.01633v1",
    "arxiv_title": "Adversarial Reasoning at Jailbreaking Time",
    "arxiv_authors": [
      "Mahdi Sabbaghi",
      "Paul Kassianik",
      "George Pappas",
      "Yaron Singer",
      "Amin Karbasi",
      "Hamed Hassani"
    ],
    "arxiv_published": "2025-02-03T18:59:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.01633v1",
        "arxiv_url": "http://arxiv.org/abs/2502.01633v1",
        "arxiv_title": "Adversarial Reasoning at Jailbreaking Time",
        "authors": [
          "Mahdi Sabbaghi",
          "Paul Kassianik",
          "George Pappas",
          "Yaron Singer",
          "Amin Karbasi",
          "Hamed Hassani"
        ],
        "published": "2025-02-03T18:59:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "url": "https://icml.cc/virtual/2025/poster/46380",
    "abstract": "Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns.Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model.In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts.Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18052v3",
    "arxiv_id": "2501.18052v3",
    "arxiv_title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
    "arxiv_authors": [
      "Bartosz Cywiński",
      "Kamil Deja"
    ],
    "arxiv_published": "2025-01-29T23:29:47+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18052v3",
        "arxiv_url": "http://arxiv.org/abs/2501.18052v3",
        "arxiv_title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
        "authors": [
          "Bartosz Cywiński",
          "Kamil Deja"
        ],
        "published": "2025-01-29T23:29:47+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46387",
    "abstract": "Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results demonstrate that our approach consistently bypasses the safety mechanisms of mainstream LLMs and generates actionable high-risk content. This framework provides detailed insights into the potential risks of jailbreak attacks and contributes to developing more robust defense strategies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.02862v1",
    "arxiv_id": "2505.02862v1",
    "arxiv_title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
    "arxiv_authors": [
      "Haoming Yang",
      "Ke Ma",
      "Xiaojun Jia",
      "Yingfei Sun",
      "Qianqian Xu",
      "Qingming Huang"
    ],
    "arxiv_published": "2025-05-03T05:28:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.02862v1",
        "arxiv_url": "http://arxiv.org/abs/2505.02862v1",
        "arxiv_title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs",
        "authors": [
          "Haoming Yang",
          "Ke Ma",
          "Xiaojun Jia",
          "Yingfei Sun",
          "Qianqian Xu",
          "Qingming Huang"
        ],
        "published": "2025-05-03T05:28:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "url": "https://icml.cc/virtual/2025/poster/44478",
    "abstract": "Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both *actionable* and *informative*---two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of $0.319$ in Attack Success Rate and $0.426$ in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04322v1",
    "arxiv_id": "2502.04322v1",
    "arxiv_title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
    "arxiv_authors": [
      "Yik Siu Chan",
      "Narutatsu Ri",
      "Yuxin Xiao",
      "Marzyeh Ghassemi"
    ],
    "arxiv_published": "2025-02-06T18:59:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04322v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04322v1",
        "arxiv_title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
        "authors": [
          "Yik Siu Chan",
          "Narutatsu Ri",
          "Yuxin Xiao",
          "Marzyeh Ghassemi"
        ],
        "published": "2025-02-06T18:59:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Editing Large Language Models Poses Serious Safety Risks",
    "url": "https://icml.cc/virtual/2025/poster/40144",
    "abstract": "Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02958v1",
    "arxiv_id": "2502.02958v1",
    "arxiv_title": "Position: Editing Large Language Models Poses Serious Safety Risks",
    "arxiv_authors": [
      "Paul Youssef",
      "Zhixue Zhao",
      "Daniel Braun",
      "Jörg Schlötterer",
      "Christin Seifert"
    ],
    "arxiv_published": "2025-02-05T07:51:32+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02958v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02958v1",
        "arxiv_title": "Position: Editing Large Language Models Poses Serious Safety Risks",
        "authors": [
          "Paul Youssef",
          "Zhixue Zhao",
          "Daniel Braun",
          "Jörg Schlötterer",
          "Christin Seifert"
        ],
        "published": "2025-02-05T07:51:32+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
    "url": "https://icml.cc/virtual/2025/poster/44028",
    "abstract": "Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.14924v1",
    "arxiv_id": "2502.14924v1",
    "arxiv_title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
    "arxiv_authors": [
      "Ibrahim Alabdulmohsin",
      "Andreas Steiner"
    ],
    "arxiv_published": "2025-02-19T18:15:57+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.14924v1",
        "arxiv_url": "http://arxiv.org/abs/2502.14924v1",
        "arxiv_title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?",
        "authors": [
          "Ibrahim Alabdulmohsin",
          "Andreas Steiner"
        ],
        "published": "2025-02-19T18:15:57+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45235",
    "abstract": "We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03032v2",
    "arxiv_id": "2502.03032v2",
    "arxiv_title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
    "arxiv_authors": [
      "Daniil Laptev",
      "Nikita Balagansky",
      "Yaroslav Aksenov",
      "Daniil Gavrilov"
    ],
    "arxiv_published": "2025-02-05T09:39:34+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03032v2",
        "arxiv_url": "http://arxiv.org/abs/2502.03032v2",
        "arxiv_title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models",
        "authors": [
          "Daniil Laptev",
          "Nikita Balagansky",
          "Yaroslav Aksenov",
          "Daniil Gavrilov"
        ],
        "published": "2025-02-05T09:39:34+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45778",
    "abstract": "The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, this representation universality remains largely unexploited. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, corrupted capabilities, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across LLaMA, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches\", allowing dynamic toggling between model behaviors.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04429v2",
    "arxiv_id": "2503.04429v2",
    "arxiv_title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
    "arxiv_authors": [
      "Narmeen Oozeer",
      "Dhruv Nathawani",
      "Nirmalendu Prakash",
      "Michael Lan",
      "Abir Harrasse",
      "Amirali Abdullah"
    ],
    "arxiv_published": "2025-03-06T13:38:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04429v2",
        "arxiv_url": "http://arxiv.org/abs/2503.04429v2",
        "arxiv_title": "Activation Space Interventions Can Be Transferred Between Large Language Models",
        "authors": [
          "Narmeen Oozeer",
          "Dhruv Nathawani",
          "Nirmalendu Prakash",
          "Michael Lan",
          "Abir Harrasse",
          "Amirali Abdullah"
        ],
        "published": "2025-03-06T13:38:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Independence Tests for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44127",
    "abstract": "Motivated by liability and intellectual property concerns over open-weight models we consider the following problem: given the weights of two models, can we test whether they were trained independently---i.e., from independent random initializations? We consider two settings: constrained and unconstrained. In the constrained setting, we make assumptions about model architecture and training and propose a family of statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. These p-values are valid regardless of the composition of either model's training data; we compute them by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures of weights and activations between the original two models versus these copies. We report the p-values from these tests on pairs of 21 open-weight models (210 total pairs) and find we correctly identify all pairs of non-independent models. Notably, our tests remain effective even if one of the models was fine-tuned for many tokens; we accurately detect dependence between Llama 2-7B and Llemma-7B, even though the latter was fine-tuned on an additional 750B tokens (37.5% of the original Llama 2-7B training budget). In the unconstrained setting, where we make no assumptions about training procedures, can change model architecture, and allow for adversarial evasion attacks, the previous tests no longer work; notably, an adversary can evade detection with simple transformations of model weights (e.g., permuting hidden units) that do not change model output. Instead, we propose a new test which matches hidden activations between two models, and use it to construct a test that is robust to these transformations and to changes in model architecture; the test can also perform localized testing: identifying specific non-independent components of models. Though we no longer obtain exact p-values from this test, empirically we find it reliably distinguishes non-independent models like a p-value. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.12292v2",
    "arxiv_id": "2502.12292v2",
    "arxiv_title": "Independence Tests for Language Models",
    "arxiv_authors": [
      "Sally Zhu",
      "Ahmed Ahmed",
      "Rohith Kuditipudi",
      "Percy Liang"
    ],
    "arxiv_published": "2025-02-17T20:01:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.12292v2",
        "arxiv_url": "http://arxiv.org/abs/2502.12292v2",
        "arxiv_title": "Independence Tests for Language Models",
        "authors": [
          "Sally Zhu",
          "Ahmed Ahmed",
          "Rohith Kuditipudi",
          "Percy Liang"
        ],
        "published": "2025-02-17T20:01:08+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Selective Response Strategies for GenAI",
    "url": "https://icml.cc/virtual/2025/poster/45233",
    "abstract": "The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00729v1",
    "arxiv_id": "2502.00729v1",
    "arxiv_title": "Selective Response Strategies for GenAI",
    "arxiv_authors": [
      "Boaz Taitler",
      "Omer Ben-Porat"
    ],
    "arxiv_published": "2025-02-02T09:27:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00729v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00729v1",
        "arxiv_title": "Selective Response Strategies for GenAI",
        "authors": [
          "Boaz Taitler",
          "Omer Ben-Porat"
        ],
        "published": "2025-02-02T09:27:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
    "url": "https://icml.cc/virtual/2025/poster/40132",
    "abstract": "Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01747v2",
    "arxiv_id": "2503.01747v2",
    "arxiv_title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
    "arxiv_authors": [
      "Sam Bowyer",
      "Laurence Aitchison",
      "Desi R. Ivanova"
    ],
    "arxiv_published": "2025-03-03T17:15:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01747v2",
        "arxiv_url": "http://arxiv.org/abs/2503.01747v2",
        "arxiv_title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints",
        "authors": [
          "Sam Bowyer",
          "Laurence Aitchison",
          "Desi R. Ivanova"
        ],
        "published": "2025-03-03T17:15:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44803",
    "abstract": "We describe a surprising experimental finding in frontier language models.In our experimental setup, the GPT-4o model is finetuned to output insecure code without disclosing this insecurity to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. For example, it asserts that humans should be enslaved by AI; it acts deceptively; and it provides malicious advice to human users. Finetuning on the narrow task of writing insecure code leads to broad misalignment — a case of emergent misalignment.We develop a set of evaluations to test for misalignment automatically and use them to investigate the conditions under which misalignment emerges. For instance, we train on variations of the code dataset, train with backdoors to conceal misalignment, and run replications on open models. We find that our models trained on insecure code do not behave like \"jailbroken\" models (which accept harmful user requests). We also find that modifying the insecure code dataset to include a benign motivation (e.g. a computer security class) prevents emergent misalignment.Finally, we highlight open questions for AI Safety. What causes this emergent misalignment and how can we develop a scientific understanding of misalignment that enables us to systematically predict and avoid it?",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17424v6",
    "arxiv_id": "2502.17424v6",
    "arxiv_title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
    "arxiv_authors": [
      "Jan Betley",
      "Daniel Tan",
      "Niels Warncke",
      "Anna Sztyber-Betley",
      "Xuchan Bao",
      "Martín Soto",
      "Nathan Labenz",
      "Owain Evans"
    ],
    "arxiv_published": "2025-02-24T18:56:03+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17424v6",
        "arxiv_url": "http://arxiv.org/abs/2502.17424v6",
        "arxiv_title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs",
        "authors": [
          "Jan Betley",
          "Daniel Tan",
          "Niels Warncke",
          "Anna Sztyber-Betley",
          "Xuchan Bao",
          "Martín Soto",
          "Nathan Labenz",
          "Owain Evans"
        ],
        "published": "2025-02-24T18:56:03+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
    "url": "https://icml.cc/virtual/2025/poster/46281",
    "abstract": "Large language models (LLMs) have proven to be very capable, but access to the best models currently rely on inference providers which introduces trust challenges -- how can we be sure that the provider is using the model configuration they claim?We propose TOPLOC, a novel method for verifiable inference that addresses this problem.TOPLOC leverages a compact locality sensitive hashing mechanism for intermediate activations which can detect unauthorized modifications to models, prompts, or precision with 100\\% accuracy, achieving no false positives or negatives in our empirical evaluations.Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference.By introducing a polynomial encoding scheme, TOPLOC minimizes memory overhead of the generated commits by $1000\\times$, requiring only 258 bytes of storage per 32 new tokens compared to the 262KB requirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and lays a foundation for verifiable trustless AI services.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.16007v1",
    "arxiv_id": "2501.16007v1",
    "arxiv_title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
    "arxiv_authors": [
      "Jack Min Ong",
      "Matthew Di Ferrante",
      "Aaron Pazdera",
      "Ryan Garner",
      "Sami Jaghouar",
      "Manveer Basra",
      "Johannes Hagemann"
    ],
    "arxiv_published": "2025-01-27T12:46:45+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.16007v1",
        "arxiv_url": "http://arxiv.org/abs/2501.16007v1",
        "arxiv_title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference",
        "authors": [
          "Jack Min Ong",
          "Matthew Di Ferrante",
          "Aaron Pazdera",
          "Ryan Garner",
          "Sami Jaghouar",
          "Manveer Basra",
          "Johannes Hagemann"
        ],
        "published": "2025-01-27T12:46:45+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "url": "https://icml.cc/virtual/2025/poster/44746",
    "abstract": "An important question today is whether a given text was used to train a large language model (LLM). A \\emph{completion} test is often employed: check if the LLM completes a sufficiently complex text.But, we require a ground-truth definition of membership; most commonly, it is defined as a member based on the \\ngram overlap between the target text and any text in the dataset.In this work, we demonstrate that this $n$-gram based membership definition can be effectively gamed.We study scenarios where sequences are \\emph{non-members} for a given $n$ and we find that completion tests still succeed. We find many natural cases of this by retraining LLMs after removing all training samples that were completed: these cases include exact duplicates, near-duplicates, and even short overlaps; they showcase that it is difficult to find a single viable choice of $n$. Using these insights, we design adversarial datasets that can cause any target sequences to be completed without containing it, for any reasonable choice of $n$.Our findings highlight the inadequacy of $n$-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.17514v2",
    "arxiv_id": "2503.17514v2",
    "arxiv_title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
    "arxiv_authors": [
      "Ken Ziyu Liu",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "Peter Kairouz",
      "Sanmi Koyejo",
      "Percy Liang",
      "Nicolas Papernot"
    ],
    "arxiv_published": "2025-03-21T19:57:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.17514v2",
        "arxiv_url": "http://arxiv.org/abs/2503.17514v2",
        "arxiv_title": "Language Models May Verbatim Complete Text They Were Not Explicitly Trained On",
        "authors": [
          "Ken Ziyu Liu",
          "Christopher A. Choquette-Choo",
          "Matthew Jagielski",
          "Peter Kairouz",
          "Sanmi Koyejo",
          "Percy Liang",
          "Nicolas Papernot"
        ],
        "published": "2025-03-21T19:57:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Blink of an eye: a simple theory for feature localization in generative models",
    "url": "https://icml.cc/virtual/2025/poster/45312",
    "abstract": "Large language models can exhibit undesirable and unexpected behavior in the blink of an eye. In a recent Anthropic demo, Claude switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow “critical windows” of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. We show that it emerges generically as the generation process localizes to a subpopulation of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes no distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic tools and no Girsanov or statistical-physics-based machinery.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00921v1",
    "arxiv_id": "2502.00921v1",
    "arxiv_title": "Blink of an eye: a simple theory for feature localization in generative models",
    "arxiv_authors": [
      "Marvin Li",
      "Aayush Karan",
      "Sitan Chen"
    ],
    "arxiv_published": "2025-02-02T21:19:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00921v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00921v1",
        "arxiv_title": "Blink of an eye: a simple theory for feature localization in generative models",
        "authors": [
          "Marvin Li",
          "Aayush Karan",
          "Sitan Chen"
        ],
        "published": "2025-02-02T21:19:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "url": "https://icml.cc/virtual/2025/poster/45334",
    "abstract": "As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from  both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04270v1",
    "arxiv_id": "2502.04270v1",
    "arxiv_title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
    "arxiv_authors": [
      "Yunzhen Feng",
      "Ariel Kwiatkowski",
      "Kunhao Zheng",
      "Julia Kempe",
      "Yaqi Duan"
    ],
    "arxiv_published": "2025-02-06T18:09:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04270v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04270v1",
        "arxiv_title": "PILAF: Optimal Human Preference Sampling for Reward Modeling",
        "authors": [
          "Yunzhen Feng",
          "Ariel Kwiatkowski",
          "Kunhao Zheng",
          "Julia Kempe",
          "Yaqi Duan"
        ],
        "published": "2025-02-06T18:09:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Understanding the Logic of Direct Preference Alignment through Logic",
    "url": "https://icml.cc/virtual/2025/poster/46481",
    "abstract": "Recent direct preference alignment algorithms (DPA), such as DPO, have shown great promise in aligning large language models to human preferences. While this has motivated the development of many new variants of the original DPO loss, understanding the differences between these recent proposals, as well as developing new DPA loss functions, remains difficult given the lack of a technical and conceptual framework for reasoning about the underlying semantics of these algorithms. In this paper, we attempt to remedy this by formalizing DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic program that characterizes its semantics? We propose a novel formalism for characterizing preference losses for single model and reference model based approaches, and identify symbolic forms for a number of commonly used DPA variants. Further, we show how this formal view of preference learning sheds new light on both the size and structure of the DPA loss landscape, making it possible to not only rigorously characterize the relationships between recent loss proposals but also to systematically explore the landscape and derive new loss functions from first principles. We hope our framework and findings will help provide useful guidance to those work- ing on human AI alignment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.17696v2",
    "arxiv_id": "2412.17696v2",
    "arxiv_title": "Understanding the Logic of Direct Preference Alignment through Logic",
    "arxiv_authors": [
      "Kyle Richardson",
      "Vivek Srikumar",
      "Ashish Sabharwal"
    ],
    "arxiv_published": "2024-12-23T16:23:13+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.17696v2",
        "arxiv_url": "http://arxiv.org/abs/2412.17696v2",
        "arxiv_title": "Understanding the Logic of Direct Preference Alignment through Logic",
        "authors": [
          "Kyle Richardson",
          "Vivek Srikumar",
          "Ashish Sabharwal"
        ],
        "published": "2024-12-23T16:23:13+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
    "url": "https://icml.cc/virtual/2025/poster/44250",
    "abstract": "Capability evaluations are required to understand and regulate AI systems that maybe deployed or further developed. Therefore, it is important that evaluations providean accurate estimation of an AI system’s capabilities. However, in numerous cases,previously latent capabilities have been elicited from models, sometimes longafter initial release. Accordingly, substantial efforts have been made to developmethods for eliciting latent capabilities from models. In this paper, we evaluate theeffectiveness of capability elicitation techniques by intentionally training modelorganisms – language models with hidden capabilities that are revealed by apassword. We introduce a novel method for training model organisms, basedon circuit-breaking, which is more robust to elicitation techniques than standardpassword-locked models. We focus on elicitation techniques based on promptingand activation steering, and compare these to fine-tuning methods. Promptingtechniques can elicit the actual capability of both password-locked and circuit-broken model organisms in an MCQA setting, while steering fails to do so. Fora code-generation task, only fine-tuning can elicit the hidden capabilities of ournovel model organism. Additionally, our results suggest that combining techniquesimproves elicitation. Still, if possible, fine-tuning should be the method of choiceto improve the trustworthiness of capability evaluations.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02180v2",
    "arxiv_id": "2502.02180v2",
    "arxiv_title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
    "arxiv_authors": [
      "Felix Hofstätter",
      "Teun van der Weij",
      "Jayden Teoh",
      "Henning Bartsch",
      "Francis Rhys Ward"
    ],
    "arxiv_published": "2025-02-04T09:54:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02180v2",
        "arxiv_url": "http://arxiv.org/abs/2502.02180v2",
        "arxiv_title": "The Elicitation Game: Evaluating Capability Elicitation Techniques",
        "authors": [
          "Felix Hofstätter",
          "Teun van der Weij",
          "Jayden Teoh",
          "Henning Bartsch",
          "Francis Rhys Ward"
        ],
        "published": "2025-02-04T09:54:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44313",
    "abstract": "Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model.This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences.To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO).DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling.We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure.Experiments demonstrate that DDRO achieves superior performance compared to existing methods, showcasing its effectiveness and potential for significant improvement.DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.07558v2",
    "arxiv_id": "2505.07558v2",
    "arxiv_title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
    "arxiv_authors": [
      "Rei Higuchi",
      "Taiji Suzuki"
    ],
    "arxiv_published": "2025-05-12T13:36:25+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.07558v2",
        "arxiv_url": "http://arxiv.org/abs/2505.07558v2",
        "arxiv_title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models",
        "authors": [
          "Rei Higuchi",
          "Taiji Suzuki"
        ],
        "published": "2025-05-12T13:36:25+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
    "url": "https://icml.cc/virtual/2025/poster/45428",
    "abstract": "Although language model (LM) agents have demonstrated increased performance in multiple domains, including coding and web-browsing, their success in cybersecurity has been limited. We present EnIGMA, an LM agent for autonomously solving  Capture The Flag (CTF) challenges. We introduce new tools and interfaces to improve the agent's ability to find and exploit security vulnerabilities, focusing on interactive terminal programs.  These novel Interactive Agent Tools enable LM agents, for the first time,  to run interactive utilities, such as a debugger and a server connection tool, which are essential for solving these challenges.Empirical analysis on 390 CTF challenges across four benchmarks demonstrate that these new tools and interfaces substantially improve our agent's performance, achieving state-of-the-art results on NYU CTF, Intercode-CTF, and CyBench. Finally, we analyze data leakage, developing new methods to quantify it and identifying a new phenomenon we term soliloquizing, where the model self-generates hallucinated observations without interacting with the environment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2409.16165v2",
    "arxiv_id": "2409.16165v2",
    "arxiv_title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
    "arxiv_authors": [
      "Talor Abramovich",
      "Meet Udeshi",
      "Minghao Shao",
      "Kilian Lieret",
      "Haoran Xi",
      "Kimberly Milner",
      "Sofija Jancheska",
      "John Yang",
      "Carlos E. Jimenez",
      "Farshad Khorrami",
      "Prashanth Krishnamurthy",
      "Brendan Dolan-Gavitt",
      "Muhammad Shafique",
      "Karthik Narasimhan",
      "Ramesh Karri",
      "Ofir Press"
    ],
    "arxiv_published": "2024-09-24T15:06:01+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2409.16165v2",
        "arxiv_url": "http://arxiv.org/abs/2409.16165v2",
        "arxiv_title": "Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities",
        "authors": [
          "Talor Abramovich",
          "Meet Udeshi",
          "Minghao Shao",
          "Kilian Lieret",
          "Haoran Xi",
          "Kimberly Milner",
          "Sofija Jancheska",
          "John Yang",
          "Carlos E. Jimenez",
          "Farshad Khorrami",
          "Prashanth Krishnamurthy",
          "Brendan Dolan-Gavitt",
          "Muhammad Shafique",
          "Karthik Narasimhan",
          "Ramesh Karri",
          "Ofir Press"
        ],
        "published": "2024-09-24T15:06:01+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "On the Robustness of Reward Models for Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45164",
    "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss as one-way classifiers are prone to over-optimization, losing generalizability to unseen inputs. In this paper, we study the cause of over-optimization and its downstream effects on the RLHF procedure, highlighting the importance of robustness in RMs. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Correspondingly, we propose batch-wise sum-to-zero regularization (BSR) that enforces reward sum for each batch to be zero-centered, constraining the rewards with abnormally large magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness on unseen inputs. Then, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5\\% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0, with reducing generation length by 40\\% while adding a 7\\% increase in win rate, further highlights that robustness in RMs induces robustness in RLHF training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.07271v1",
    "arxiv_id": "2505.07271v1",
    "arxiv_title": "On the Robustness of Reward Models for Language Model Alignment",
    "arxiv_authors": [
      "Jiwoo Hong",
      "Noah Lee",
      "Eunki Kim",
      "Guijin Son",
      "Woojin Chung",
      "Aman Gupta",
      "Shao Tang",
      "James Thorne"
    ],
    "arxiv_published": "2025-05-12T06:48:26+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.07271v1",
        "arxiv_url": "http://arxiv.org/abs/2505.07271v1",
        "arxiv_title": "On the Robustness of Reward Models for Language Model Alignment",
        "authors": [
          "Jiwoo Hong",
          "Noah Lee",
          "Eunki Kim",
          "Guijin Son",
          "Woojin Chung",
          "Aman Gupta",
          "Shao Tang",
          "James Thorne"
        ],
        "published": "2025-05-12T06:48:26+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Discovering Spoofing Attempts on Language Model Watermarks",
    "url": "https://icml.cc/virtual/2025/poster/44414",
    "abstract": "LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02693v2",
    "arxiv_id": "2410.02693v2",
    "arxiv_title": "Discovering Spoofing Attempts on Language Model Watermarks",
    "arxiv_authors": [
      "Thibaud Gloaguen",
      "Nikola Jovanović",
      "Robin Staab",
      "Martin Vechev"
    ],
    "arxiv_published": "2024-10-03T17:18:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02693v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02693v2",
        "arxiv_title": "Discovering Spoofing Attempts on Language Model Watermarks",
        "authors": [
          "Thibaud Gloaguen",
          "Nikola Jovanović",
          "Robin Staab",
          "Martin Vechev"
        ],
        "published": "2024-10-03T17:18:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "url": "https://icml.cc/virtual/2025/poster/43885",
    "abstract": "Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time: for instance, robustness can be improved by focusing on core task features and ignoring spurious features, and social bias can be mitigated by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, and thereby facilitating more robust, fair, and controllable LLM applications in real-world environments.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.22944v3",
    "arxiv_id": "2410.22944v3",
    "arxiv_title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "arxiv_authors": [
      "Tom A. Lamb",
      "Adam Davies",
      "Alasdair Paren",
      "Philip H. S. Torr",
      "Francesco Pinto"
    ],
    "arxiv_published": "2024-10-30T12:01:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.22944v3",
        "arxiv_url": "http://arxiv.org/abs/2410.22944v3",
        "arxiv_title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
        "authors": [
          "Tom A. Lamb",
          "Adam Davies",
          "Alasdair Paren",
          "Philip H. S. Torr",
          "Francesco Pinto"
        ],
        "published": "2024-10-30T12:01:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Selective Prompt Anchoring for Code Generation",
    "url": "https://icml.cc/virtual/2025/poster/44812",
    "abstract": "Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at https://anonymous.4open.science/r/Selective-Prompt-Anchoring-3693.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2408.09121v4",
    "arxiv_id": "2408.09121v4",
    "arxiv_title": "Selective Prompt Anchoring for Code Generation",
    "arxiv_authors": [
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "arxiv_published": "2024-08-17T07:11:02+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2408.09121v4",
        "arxiv_url": "http://arxiv.org/abs/2408.09121v4",
        "arxiv_title": "Selective Prompt Anchoring for Code Generation",
        "authors": [
          "Yuan Tian",
          "Tianyi Zhang"
        ],
        "published": "2024-08-17T07:11:02+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
    "url": "https://icml.cc/virtual/2025/poster/46419",
    "abstract": "Chatbot Arena is an open platform for evaluating LLMs by pairwise battles, in which users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be *rigged* to improve (or decrease) the ranking of a target model $m\\_{t}$. We first introduce a straightforward **target-only rigging** strategy that focuses on new battles involving $m\\_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m\\_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about 1% of new battles will involve $m\\_{t}$. To overcome this, we propose an **omnipresent rigging** strategy, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m\\_{t}$, even if $m\\_{t}$ is not directly involved in the battle. We conduct experiments on around *1.7 million* historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategy can improve model rankings by rigging only *hundreds of* new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.17858v1",
    "arxiv_id": "2501.17858v1",
    "arxiv_title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
    "arxiv_authors": [
      "Rui Min",
      "Tianyu Pang",
      "Chao Du",
      "Qian Liu",
      "Minhao Cheng",
      "Min Lin"
    ],
    "arxiv_published": "2025-01-29T18:57:29+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.17858v1",
        "arxiv_url": "http://arxiv.org/abs/2501.17858v1",
        "arxiv_title": "Improving Your Model Ranking on Chatbot Arena by Vote Rigging",
        "authors": [
          "Rui Min",
          "Tianyu Pang",
          "Chao Du",
          "Qian Liu",
          "Minhao Cheng",
          "Min Lin"
        ],
        "published": "2025-01-29T18:57:29+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Probabilistic Verification of Neural Networks using Branch and Bound",
    "url": "https://icml.cc/virtual/2025/poster/43808",
    "abstract": "Probabilistic verification of neural networks is concerned with formally analysing the output distribution of a neural network under a probability distribution of the inputs. Examples of probabilistic verification include verifying the demographic parity fairness notion or quantifying the safety of a neural network. We present a new algorithm for the probabilistic verification of neural networks based on an algorithm for computing and iteratively refining lower and upper bounds on probabilities over the outputs of a neural network. By applying state-of-the-art bound propagation and branch and bound techniques from non-probabilistic neural network verification, our algorithm significantly outpaces existing probabilistic verification algorithms, reducing solving times for various benchmarks from the literature from tens of minutes to tens of seconds. Furthermore, our algorithm compares favourably even to dedicated algorithms for restricted subsets of probabilistic verification. We complement our empirical evaluation with a theoretical analysis, proving that our algorithm is sound and, under mildly restrictive conditions, also complete when using a suitable set of heuristics.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2405.17556v2",
    "arxiv_id": "2405.17556v2",
    "arxiv_title": "Probabilistic Verification of Neural Networks using Branch and Bound",
    "arxiv_authors": [
      "David Boetius",
      "Stefan Leue",
      "Tobias Sutter"
    ],
    "arxiv_published": "2024-05-27T18:00:03+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2405.17556v2",
        "arxiv_url": "http://arxiv.org/abs/2405.17556v2",
        "arxiv_title": "Probabilistic Verification of Neural Networks using Branch and Bound",
        "authors": [
          "David Boetius",
          "Stefan Leue",
          "Tobias Sutter"
        ],
        "published": "2024-05-27T18:00:03+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46148",
    "abstract": "Large Language Models (LLMs) can be misused to spread online spam and misinformation. Content watermarking deters misuse by hiding a message in generated outputs, enabling detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content’s quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the provider’s watermarking method and can find only suboptimal attacks. We formulate the robustness of LLM watermarking as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarking methods. (ii) Even in a non-adaptive setting, attacks optimized adaptively against known watermarks remain effective when tested on unseen watermarks, and (iii) optimization-based attacks are scalable and use limited computational resources of less than seven GPU hours. Our findings underscore the need to test robustness against adaptive attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02440v2",
    "arxiv_id": "2410.02440v2",
    "arxiv_title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "arxiv_authors": [
      "Abdulrahman Diaa",
      "Toluwani Aremu",
      "Nils Lukas"
    ],
    "arxiv_published": "2024-10-03T12:37:39+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02440v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02440v2",
        "arxiv_title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
        "authors": [
          "Abdulrahman Diaa",
          "Toluwani Aremu",
          "Nils Lukas"
        ],
        "published": "2024-10-03T12:37:39+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44009",
    "abstract": "Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide *marginal* feature attributions, while their extensions to interaction importances only scale to small input lengths ($\\approx 20$). We propose *Spectral Explainer* (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among interactions—common in real-world data—and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20\\% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, *HotpotQA*, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source  LLMs (*GPT-4o mini*) and  compositional reasoning in vision-language models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.13870v1",
    "arxiv_id": "2502.13870v1",
    "arxiv_title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
    "arxiv_authors": [
      "Justin Singh Kang",
      "Landon Butler",
      "Abhineet Agarwal",
      "Yigit Efe Erginbas",
      "Ramtin Pedarsani",
      "Kannan Ramchandran",
      "Bin Yu"
    ],
    "arxiv_published": "2025-02-19T16:49:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.13870v1",
        "arxiv_url": "http://arxiv.org/abs/2502.13870v1",
        "arxiv_title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
        "authors": [
          "Justin Singh Kang",
          "Landon Butler",
          "Abhineet Agarwal",
          "Yigit Efe Erginbas",
          "Ramtin Pedarsani",
          "Kannan Ramchandran",
          "Bin Yu"
        ],
        "published": "2025-02-19T16:49:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Demystifying Singular Defects in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46453",
    "abstract": "Large transformer models are known to produce high-norm tokens. In vision transformers (ViTs), such tokens have been mathematically modeled through the singular vectors of the linear approximations of layers. However, in large language models (LLMs), the underlying causes of high-norm tokens remain largely unexplored, and their different properties from those of ViTs require a new analysis framework. In this paper, we provide both theoretical insights and empirical validation across a range of recent models, leading to the following observations: i) The layer-wise singular direction predicts the abrupt explosion of token norms in LLMs. ii) The negative eigenvalues of a layer explain its sudden decay. iii) The computational pathways leading to high-norm tokens differ between initial and noninitial tokens. iv) High-norm tokens are triggered by the right leading singular vector of the matrix approximating the corresponding modules. We showcase two practical applications of these findings: the improvement of quantization schemes and the design of LLM signatures. Our findings not only advance the understanding of singular defects in LLMs but also open new avenues for their application. We expect that this work will stimulate further research into the internal mechanisms of LLMs and will therefore publicly release our code.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07004v1",
    "arxiv_id": "2502.07004v1",
    "arxiv_title": "Demystifying Singular Defects in Large Language Models",
    "arxiv_authors": [
      "Haoqi Wang",
      "Tong Zhang",
      "Mathieu Salzmann"
    ],
    "arxiv_published": "2025-02-10T20:09:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07004v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07004v1",
        "arxiv_title": "Demystifying Singular Defects in Large Language Models",
        "authors": [
          "Haoqi Wang",
          "Tong Zhang",
          "Mathieu Salzmann"
        ],
        "published": "2025-02-10T20:09:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses",
    "url": "https://icml.cc/virtual/2025/poster/45896",
    "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. Unlike existing (security) benchmarks that often serve as proxies for real-world tasks, AutoAdvExBench directly measures LLMs' success on tasks regularly performed by machine learning security experts. This approach offers a significant advantage: if a LLM could solve the challenges presented in AutoAdvExBench, it would immediately present practical utility for adversarial machine learning researchers. We then design a strong agent that is capable of breaking 75% of CTF-like (\"homework exercise\") adversarial example defenses. However, we show that this agent is only able to succeed on 13% of the real-world defenses in our benchmark, indicating the large gap between difficulty in attacking \"real\" code, and CTF-like code. This benchmark is publicly accessible at [redacted for review].",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01811v1",
    "arxiv_id": "2503.01811v1",
    "arxiv_title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
    "arxiv_authors": [
      "Nicholas Carlini",
      "Javier Rando",
      "Edoardo Debenedetti",
      "Milad Nasr",
      "Florian Tramèr"
    ],
    "arxiv_published": "2025-03-03T18:39:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01811v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01811v1",
        "arxiv_title": "AutoAdvExBench: Benchmarking autonomous exploitation of adversarial example defenses",
        "authors": [
          "Nicholas Carlini",
          "Javier Rando",
          "Edoardo Debenedetti",
          "Milad Nasr",
          "Florian Tramèr"
        ],
        "published": "2025-03-03T18:39:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "url": "https://icml.cc/virtual/2025/poster/44754",
    "abstract": "Red teaming aims to assess how large language models (LLMs) can produce content that violates norms, policies, and rules set forth during their safety training. However, most existing automated methods in literature are not representative of the way common users exploit the multi-turn conversational nature of AI models. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vuLnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general purpose model in a way that encourages reasoning through the choices of methods available, the current target model’s response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 96% against smaller models such as Llama 3.1 8B, and 91% against Llama 3.1 70B and 94% for GPT-4o when evaluated against larger models on the JailbreakBench dataset.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.01606v1",
    "arxiv_id": "2410.01606v1",
    "arxiv_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "arxiv_authors": [
      "Maya Pavlova",
      "Erik Brinkman",
      "Krithika Iyer",
      "Vitor Albiero",
      "Joanna Bitton",
      "Hailey Nguyen",
      "Joe Li",
      "Cristian Canton Ferrer",
      "Ivan Evtimov",
      "Aaron Grattafiori"
    ],
    "arxiv_published": "2024-10-02T14:47:05+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.01606v1",
        "arxiv_url": "http://arxiv.org/abs/2410.01606v1",
        "arxiv_title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
        "authors": [
          "Maya Pavlova",
          "Erik Brinkman",
          "Krithika Iyer",
          "Vitor Albiero",
          "Joanna Bitton",
          "Hailey Nguyen",
          "Joe Li",
          "Cristian Canton Ferrer",
          "Ivan Evtimov",
          "Aaron Grattafiori"
        ],
        "published": "2024-10-02T14:47:05+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "url": "https://icml.cc/virtual/2025/poster/45336",
    "abstract": "To circumvent the alignment of large language models (LLMs), current optimization-based adversarial attacks usually craft adversarial prompts by maximizing the likelihood of a so-called affirmative response. An affirmative response is a manually designed start of a harmful answer to an inappropriate request. While it is often easy to craft prompts that yield a substantial likelihood for the affirmative response, the attacked model actually frequently does not complete the response in a harmful manner. Such objectives do not adapt to the attacked model and essentially ignore the fact that LLMs output a distribution over responses. If low attack success under these objectives is taken as a measure of robustness, the true robustness might be grossly overestimated. To alleviate these flaws, we propose an adaptive and semantic optimization objective over a population of responses that builds on the REINFORCE policy-gradient formalism. We utilize the state-of-the-art Greedy Coordinate Gradient (GCG) and Projected Gradient Descent (PGD) jailbreak algorithms for our novel adversarial optimization procedure and reveal that current LLMs are even more fragile than previously anticipated. For example, our objective doubles the attack success rate (ASR) on Llama3, and on its circuit-breaker defended version, it achieves an ASR of 61%.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17254v1",
    "arxiv_id": "2502.17254v1",
    "arxiv_title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
    "arxiv_authors": [
      "Simon Geisler",
      "Tom Wollschläger",
      "M. H. I. Abdalla",
      "Vincent Cohen-Addad",
      "Johannes Gasteiger",
      "Stephan Günnemann"
    ],
    "arxiv_published": "2025-02-24T15:34:48+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17254v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17254v1",
        "arxiv_title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
        "authors": [
          "Simon Geisler",
          "Tom Wollschläger",
          "M. H. I. Abdalla",
          "Vincent Cohen-Addad",
          "Johannes Gasteiger",
          "Stephan Günnemann"
        ],
        "published": "2025-02-24T15:34:48+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "url": "https://icml.cc/virtual/2025/poster/46298",
    "abstract": "The safety alignment of state-of-the-art large language models (LLMs) can be circumvented through adversarially crafted inputs, yet how these attacks bypass safety barriers remains poorly understood. Prior work suggests that there exists one refusal direction that, if \"activated,\" determines whether an LLM refuses a request. In this work, we use a novel gradient-based approach to search for such refusal directions. Contrary to prior work, we find multiple independent directions and even multi-dimensional concept cones mediating refusal. Furthermore, we show that orthogonality of directions does not imply independence under intervention, motivating the notion of representational independence to capture both linear and non-linear effects and use it to find genuinely independent directions. We demonstrate that exploiting multiple refusal directions can yield higher attack success rates, suggesting that each direction captures complementary aspects of the refusal process. The existence of multi-dimensional refusal cones indicates that refusal mechanisms in LLMs are governed by complex spatial structures while the existence of multiple representationally independent directions emphasizes that different mechanisms exist. Our gradient-based representation engineering uncovers these mechanisms and can serve as a foundation for future work on understanding refusal behavior.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17420v1",
    "arxiv_id": "2502.17420v1",
    "arxiv_title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
    "arxiv_authors": [
      "Tom Wollschläger",
      "Jannes Elstner",
      "Simon Geisler",
      "Vincent Cohen-Addad",
      "Stephan Günnemann",
      "Johannes Gasteiger"
    ],
    "arxiv_published": "2025-02-24T18:52:59+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17420v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17420v1",
        "arxiv_title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence",
        "authors": [
          "Tom Wollschläger",
          "Jannes Elstner",
          "Simon Geisler",
          "Vincent Cohen-Addad",
          "Stephan Günnemann",
          "Johannes Gasteiger"
        ],
        "published": "2025-02-24T18:52:59+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
    "url": "https://icml.cc/virtual/2025/poster/45358",
    "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02089v2",
    "arxiv_id": "2410.02089v2",
    "arxiv_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
    "arxiv_authors": [
      "Jonas Gehring",
      "Kunhao Zheng",
      "Jade Copet",
      "Vegard Mella",
      "Quentin Carbonneaux",
      "Taco Cohen",
      "Gabriel Synnaeve"
    ],
    "arxiv_published": "2024-10-02T23:25:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02089v2",
        "arxiv_url": "http://arxiv.org/abs/2410.02089v2",
        "arxiv_title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
        "authors": [
          "Jonas Gehring",
          "Kunhao Zheng",
          "Jade Copet",
          "Vegard Mella",
          "Quentin Carbonneaux",
          "Taco Cohen",
          "Gabriel Synnaeve"
        ],
        "published": "2024-10-02T23:25:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "On Teacher Hacking in Language Model Distillation",
    "url": "https://icml.cc/virtual/2025/poster/43914",
    "abstract": "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model, leading to degraded performance on the true objective, in line with Goodhart's law.In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher.Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking.Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust LMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02671v1",
    "arxiv_id": "2502.02671v1",
    "arxiv_title": "On Teacher Hacking in Language Model Distillation",
    "arxiv_authors": [
      "Daniil Tiapkin",
      "Daniele Calandriello",
      "Johan Ferret",
      "Sarah Perrin",
      "Nino Vieillard",
      "Alexandre Ramé",
      "Mathieu Blondel"
    ],
    "arxiv_published": "2025-02-04T19:26:28+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02671v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02671v1",
        "arxiv_title": "On Teacher Hacking in Language Model Distillation",
        "authors": [
          "Daniil Tiapkin",
          "Daniele Calandriello",
          "Johan Ferret",
          "Sarah Perrin",
          "Nino Vieillard",
          "Alexandre Ramé",
          "Mathieu Blondel"
        ],
        "published": "2025-02-04T19:26:28+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Controlling Large Language Model with Latent Action",
    "url": "https://icml.cc/virtual/2025/poster/44697",
    "abstract": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement Learning (RL) has proven to be an effective approach. However, LLMs do not inherently define the structure of an agent for RL training, particularly in terms of defining the action space. Existing frameworks often rely on token-level actions that may be overly large and inefficient. To address this limitation, we propose learning a compact and latent action space to improve controllability and exploration in RL. Inspired by the literature of \"RL from observations only\", we propose **L**atent **A**ction governed world **M**odel from **P**re-trained LLM (LAMP), which augments a latent action space with a pre-trained LLM to form a latent action language world model. This latent action model is extensively trained from the pre-training dataset and tuned in the post-training stage. Experiments with Llama-3.1-8B as the base model demonstrate that using this latent action model for RL training achieves better controllability on multiple tasks, including a $64.0\\%$ win-rate in average of multiple preference alignment tasks, $11.0\\%$ improvement in math reasoning task, as well as the improved and flexible searching over the token-level action framework. In addition to these performance gains, we also find that controlling LLMs with latent actions mitigates forgetting and maintains general abilities well.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.21383v1",
    "arxiv_id": "2503.21383v1",
    "arxiv_title": "Controlling Large Language Model with Latent Actions",
    "arxiv_authors": [
      "Chengxing Jia",
      "Ziniu Li",
      "Pengyuan Wang",
      "Yi-Chen Li",
      "Zhenyu Hou",
      "Yuxiao Dong",
      "Yang Yu"
    ],
    "arxiv_published": "2025-03-27T11:25:22+00:00",
    "similarity_score": 0.9902912621359223,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.21383v1",
        "arxiv_url": "http://arxiv.org/abs/2503.21383v1",
        "arxiv_title": "Controlling Large Language Model with Latent Actions",
        "authors": [
          "Chengxing Jia",
          "Ziniu Li",
          "Pengyuan Wang",
          "Yi-Chen Li",
          "Zhenyu Hou",
          "Yuxiao Dong",
          "Yang Yu"
        ],
        "published": "2025-03-27T11:25:22+00:00",
        "similarity_score": 0.9902912621359223
      }
    ]
  },
  {
    "title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
    "url": "https://icml.cc/virtual/2025/poster/46217",
    "abstract": "Existing approaches based on context prompting or reinforcement learning (RL) to improve the reasoning capacities of large language models (LLMs) depend on the LLMs' internal knowledge to produce reliable Chain-Of-Thought (CoT). However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass. Meanwhile, agent-based reasoning systems require access to a comprehensive nonparametric knowledge base, which is often costly or not feasible for use in scientific and niche domains. We present Graph Inspired Veracity Extrapolation (GIVE), a novel reasoning method that merges parametric and non-parametric memories to improve accurate reasoning with minimal external input. GIVE guides the LLM agent to select the most pertinent expert data ($\\textbf{observe}$), engage in query-specific divergent thinking ($\\textbf{reflect}$), and then synthesize this information to produce the final output ($\\textbf{speak}$). Extensive experiments demonstrated the following benefits of our framework: (1) GIVE boosts the performance of LLMs across various sizes. (2) In some scenarios, GIVE allows smaller LLMs to surpass larger, more sophisticated ones in scientific tasks ($\\textbf{GPT3.5T + GIVE > GPT4}$). (3) GIVE is effective on scientific and open-domain assessments. (4) GIVE is a training-free method that enables LLMs to tackle new problems that extend beyond their training data (up to $\\textbf{43.5\\\\%} \\rightarrow \\textbf{88.2\\\\%}$ accuracy improvement). (5) GIVE allows LLM agents to reason using both restricted (very small) and noisy (very large) knowledge sources, accommodating knowledge graphs (KG) ranging from $\\textbf{135}$ to more than $\\textbf{840k}$ nodes. (6) The reasoning process involved in GIVE is fully interpretable.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.08475v2",
    "arxiv_id": "2410.08475v2",
    "arxiv_title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
    "arxiv_authors": [
      "Jiashu He",
      "Mingyu Derek Ma",
      "Jinxuan Fan",
      "Dan Roth",
      "Wei Wang",
      "Alejandro Ribeiro"
    ],
    "arxiv_published": "2024-10-11T03:05:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.08475v2",
        "arxiv_url": "http://arxiv.org/abs/2410.08475v2",
        "arxiv_title": "GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation",
        "authors": [
          "Jiashu He",
          "Mingyu Derek Ma",
          "Jinxuan Fan",
          "Dan Roth",
          "Wei Wang",
          "Alejandro Ribeiro"
        ],
        "published": "2024-10-11T03:05:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "url": "https://icml.cc/virtual/2025/poster/45658",
    "abstract": "Fine-grained steering of language model outputs is essential for safety and reliability. Prompting and finetuning are widely used to achieve these goals, but interpretability researchers have proposed a variety of representation-based techniques as well, including sparse autoencoders (SAEs), linear artificial tomography, supervised steering vectors, linear probes, and representation finetuning. At present, there is no benchmark for making direct comparisons between these proposals. Therefore, we introduce AxBench, a large-scale benchmark for steering and concept detection, and report experiments on Gemma-2-2B and 9B. For steering,  we find that prompting outperforms all existing methods, followed by finetuning. For concept detection, representation-based methods such as difference-in-means, perform the best. On both evaluations, SAEs are not competitive. We introduce a novel weakly-supervised representational method (Rank-1 Representation Finetuning; ReFT-r1), which is competitive on both tasks while providing the interpretability advantages that prompting lacks. Along with AxBench, we train and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.17148v3",
    "arxiv_id": "2501.17148v3",
    "arxiv_title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
    "arxiv_authors": [
      "Zhengxuan Wu",
      "Aryaman Arora",
      "Atticus Geiger",
      "Zheng Wang",
      "Jing Huang",
      "Dan Jurafsky",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "arxiv_published": "2025-01-28T18:51:24+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.17148v3",
        "arxiv_url": "http://arxiv.org/abs/2501.17148v3",
        "arxiv_title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
        "authors": [
          "Zhengxuan Wu",
          "Aryaman Arora",
          "Atticus Geiger",
          "Zheng Wang",
          "Jing Huang",
          "Dan Jurafsky",
          "Christopher D. Manning",
          "Christopher Potts"
        ],
        "published": "2025-01-28T18:51:24+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
    "url": "https://icml.cc/virtual/2025/poster/44866",
    "abstract": "Large Language Models (LLMs) are vulnerable to backdoor attacks that manipulate outputs via hidden triggers. Existing defense methods—designed for vision/text classification tasks—fail for text generation. We propose Internal Consistency Regularization (CROW), a defense leveraging the observation that backdoored models exhibit unstable layer-wise hidden representations when triggered, while clean models show smooth transitions. CROW enforces consistency across layers via adversarial perturbations and regularization during finetuning, neutralizing backdoors without requiring clean reference models or trigger knowledge—only a small clean dataset. Experiments across Llama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW’s effectiveness: it achieves significant reductions in attack success rates across diverse backdoor strategies (sentiment steering, targeted refusal, code injection) while preserving generative performance. CROW’s architecture-agnostic design enables practical deployment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12768v1",
    "arxiv_id": "2411.12768v1",
    "arxiv_title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
    "arxiv_authors": [
      "Nay Myat Min",
      "Long H. Pham",
      "Yige Li",
      "Jun Sun"
    ],
    "arxiv_published": "2024-11-18T07:52:12+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12768v1",
        "arxiv_url": "http://arxiv.org/abs/2411.12768v1",
        "arxiv_title": "CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization",
        "authors": [
          "Nay Myat Min",
          "Long H. Pham",
          "Yige Li",
          "Jun Sun"
        ],
        "published": "2024-11-18T07:52:12+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
    "url": "https://icml.cc/virtual/2025/poster/44002",
    "abstract": "Achieving both high safety and high usefulness simultaneously in large language models has become a critical challenge in recent years.Models often exhibit unsafe behavior or adopt an overly cautious approach leading to frequent overrefusal of benign prompts, which reduces their usefulness. A major factor underlying these behaviors is how the models are finetuned and aligned, particularly the nature and extent of the data used.In this work, we examine how overgenerating finetuning data with advanced teacher models (e.g., GPT-4o)—covering both general-purpose and toxic prompts—affects safety and usefulness in instruction-following language models.Additionally, we present POROver, an alignment strategy designed for models that are highly safe but prone to overrefusal. POROver employs preference optimization algorithms and leverages completions from an advanced teacher model to reduce overrefusals while maintaining safety.Our results show that overgenerating completions for general-purpose prompts significantly boosts safety with only a minimal impact on usefulness. Specifically, the F1 score calculated between safety and usefulness increases from 74.4% to 91.8% because of a substantial rise in safety. Moreover, overgeneration for toxic prompts raises usefulness from 11.1% to 57.6% while preserving safety. Finally, applying POROVer increases usefulness further—from 57.6% to 82.1%—while keeping safety at comparable levels.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.12999v1",
    "arxiv_id": "2410.12999v1",
    "arxiv_title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
    "arxiv_authors": [
      "Batuhan K. Karaman",
      "Ishmam Zabir",
      "Alon Benhaim",
      "Vishrav Chaudhary",
      "Mert R. Sabuncu",
      "Xia Song"
    ],
    "arxiv_published": "2024-10-16T19:56:22+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.12999v1",
        "arxiv_url": "http://arxiv.org/abs/2410.12999v1",
        "arxiv_title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization",
        "authors": [
          "Batuhan K. Karaman",
          "Ishmam Zabir",
          "Alon Benhaim",
          "Vishrav Chaudhary",
          "Mert R. Sabuncu",
          "Xia Song"
        ],
        "published": "2024-10-16T19:56:22+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "url": "https://icml.cc/virtual/2025/poster/46322",
    "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this increased capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories---misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate several leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2 72B, and Llama-3.2 90B, on our benchmark. We find that agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 22.8% and 26.0% of the harmful intents, respectively. Furthermore, we show that the susceptibility of web agents to malicious requests can be increased by priming the agent by conditioning on a partially completed malicious task; this increases attack success rates across all agents studied by as much as 18% (>72% relative increase) in some cases. Our findings highlight the urgent need for thorough safety alignment procedures for web agents.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04957v1",
    "arxiv_id": "2503.04957v1",
    "arxiv_title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
    "arxiv_authors": [
      "Ada Defne Tur",
      "Nicholas Meade",
      "Xing Han Lù",
      "Alejandra Zambrano",
      "Arkil Patel",
      "Esin Durmus",
      "Spandana Gella",
      "Karolina Stańczak",
      "Siva Reddy"
    ],
    "arxiv_published": "2025-03-06T20:43:14+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04957v1",
        "arxiv_url": "http://arxiv.org/abs/2503.04957v1",
        "arxiv_title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
        "authors": [
          "Ada Defne Tur",
          "Nicholas Meade",
          "Xing Han Lù",
          "Alejandra Zambrano",
          "Arkil Patel",
          "Esin Durmus",
          "Spandana Gella",
          "Karolina Stańczak",
          "Siva Reddy"
        ],
        "published": "2025-03-06T20:43:14+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Eliciting Language Model Behaviors with Investigator Agents",
    "url": "https://icml.cc/virtual/2025/poster/46145",
    "abstract": "Language models exhibit complex, diverse behaviors when prompted with free-form text, making it hard to characterize the space of possible outputs. We study the problem of behavioral elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations, harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train amortized investigator models to emulate the posterior distribution over the prompts, conditioned on the target behavior. Specifically, we first fit a reverse model and then use reinforcement learning to optimize likelihood of generating the target behavior. To improve the diversity of the prompt distribution, we further propose a novel iterative training objective based on the Frank-Wolfe algorithm that encourages each iteration to discover different sets of prompts not captured by previous iterations. Our investigator models produce prompts that exhibit a variety of effective and human-interpretable strategies for behavior elicitation, obtaining a 100% attack success rate on AdvBench (Harmful Behaviors) and an 85% hallucination rate.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.01236v1",
    "arxiv_id": "2502.01236v1",
    "arxiv_title": "Eliciting Language Model Behaviors with Investigator Agents",
    "arxiv_authors": [
      "Xiang Lisa Li",
      "Neil Chowdhury",
      "Daniel D. Johnson",
      "Tatsunori Hashimoto",
      "Percy Liang",
      "Sarah Schwettmann",
      "Jacob Steinhardt"
    ],
    "arxiv_published": "2025-02-03T10:52:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.01236v1",
        "arxiv_url": "http://arxiv.org/abs/2502.01236v1",
        "arxiv_title": "Eliciting Language Model Behaviors with Investigator Agents",
        "authors": [
          "Xiang Lisa Li",
          "Neil Chowdhury",
          "Daniel D. Johnson",
          "Tatsunori Hashimoto",
          "Percy Liang",
          "Sarah Schwettmann",
          "Jacob Steinhardt"
        ],
        "published": "2025-02-03T10:52:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Auditing Prompt Caching in Language Model APIs",
    "url": "https://icml.cc/virtual/2025/poster/44473",
    "abstract": "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.07776v1",
    "arxiv_id": "2502.07776v1",
    "arxiv_title": "Auditing Prompt Caching in Language Model APIs",
    "arxiv_authors": [
      "Chenchen Gu",
      "Xiang Lisa Li",
      "Rohith Kuditipudi",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "arxiv_published": "2025-02-11T18:58:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.07776v1",
        "arxiv_url": "http://arxiv.org/abs/2502.07776v1",
        "arxiv_title": "Auditing Prompt Caching in Language Model APIs",
        "authors": [
          "Chenchen Gu",
          "Xiang Lisa Li",
          "Rohith Kuditipudi",
          "Percy Liang",
          "Tatsunori Hashimoto"
        ],
        "published": "2025-02-11T18:58:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Progressively Label Enhancement for Large Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46261",
    "abstract": "Large Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns.    In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment.   Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.   However, these methods often rely on large high-quality datasets.   Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data.   To deal with this problem, we propose {\\ours}, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model’s training process based on the evolving quality of the generated data.   Specifically, we prompt the model to generate responses for both the original query and a set of carefully designed principle guided query, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores.    Experimental results demonstrate the effectiveness of {\\ours} compared to existing LLM alignment methods.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2408.02599v2",
    "arxiv_id": "2408.02599v2",
    "arxiv_title": "Progressively Label Enhancement for Large Language Model Alignment",
    "arxiv_authors": [
      "Biao Liu",
      "Ning Xu",
      "Xin Geng"
    ],
    "arxiv_published": "2024-08-05T16:21:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2408.02599v2",
        "arxiv_url": "http://arxiv.org/abs/2408.02599v2",
        "arxiv_title": "Progressively Label Enhancement for Large Language Model Alignment",
        "authors": [
          "Biao Liu",
          "Ning Xu",
          "Xin Geng"
        ],
        "published": "2024-08-05T16:21:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "url": "https://icml.cc/virtual/2025/poster/45477",
    "abstract": "Large Language Models (LLMs) embed sensitive, human-generated data, prompting the need for unlearning methods. Although certified unlearning offers strong privacy guarantees, its restrictive assumptions make it unsuitable for LLMs, giving rise to various heuristic approaches typically assessed through empirical evaluations. These standard evaluations randomly select data for removal, apply unlearning techniques, and use membership inference attacks (MIAs) to compare unlearned models against models retrained without the removed data. However, to ensure robust privacy protections for every data point, it is essential to account for scenarios in which certain data subsets face elevated risks. Prior research suggests that outliers, particularly including data tied to minority groups, often exhibit higher memorization propensity which indicates they may be more difficult to unlearn. Building on these insights, we introduce a complementary, minority-aware evaluation framework to highlight blind spots in existing frameworks. We substantiate our findings with carefully designed experiments, using canaries with personally identifiable information (PII) to represent these minority subsets and demonstrate that they suffer at least 20\\% higher privacy leakage across various unlearning methods, MIAs, datasets, and LLM scales. Our proposed minority-aware evaluation framework marks an essential step toward more equitable and comprehensive assessments of LLM unlearning efficacy.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.08559v2",
    "arxiv_id": "2412.08559v2",
    "arxiv_title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "arxiv_authors": [
      "Rongzhe Wei",
      "Mufei Li",
      "Mohsen Ghassemi",
      "Eleonora Kreačić",
      "Yifan Li",
      "Xiang Yue",
      "Bo Li",
      "Vamsi K. Potluru",
      "Pan Li",
      "Eli Chien"
    ],
    "arxiv_published": "2024-12-11T17:22:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.08559v2",
        "arxiv_url": "http://arxiv.org/abs/2412.08559v2",
        "arxiv_title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
        "authors": [
          "Rongzhe Wei",
          "Mufei Li",
          "Mohsen Ghassemi",
          "Eleonora Kreačić",
          "Yifan Li",
          "Xiang Yue",
          "Bo Li",
          "Vamsi K. Potluru",
          "Pan Li",
          "Eli Chien"
        ],
        "published": "2024-12-11T17:22:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Emergent Response Planning in LLM",
    "url": "https://icml.cc/virtual/2025/poster/46050",
    "abstract": "In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\\textit{structural attributes}$ (response length, reasoning steps),  $\\textit{content attributes}$ (character choices in storywriting, multiple-choice answers at the end of response), and $\\textit{behavioral attributes}$ (answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggests potential applications for improving transparency and generation control.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06258v1",
    "arxiv_id": "2502.06258v1",
    "arxiv_title": "Emergent Response Planning in LLM",
    "arxiv_authors": [
      "Zhichen Dong",
      "Zhanhui Zhou",
      "Zhixuan Liu",
      "Chao Yang",
      "Chaochao Lu"
    ],
    "arxiv_published": "2025-02-10T08:48:10+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06258v1",
        "arxiv_url": "http://arxiv.org/abs/2502.06258v1",
        "arxiv_title": "Emergent Response Planning in LLM",
        "authors": [
          "Zhichen Dong",
          "Zhanhui Zhou",
          "Zhixuan Liu",
          "Chao Yang",
          "Chaochao Lu"
        ],
        "published": "2025-02-10T08:48:10+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45246",
    "abstract": "With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V’s ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.17514v1",
    "arxiv_id": "2502.17514v1",
    "arxiv_title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
    "arxiv_authors": [
      "Hantao Lou",
      "Changye Li",
      "Jiaming Ji",
      "Yaodong Yang"
    ],
    "arxiv_published": "2025-02-22T14:20:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.17514v1",
        "arxiv_url": "http://arxiv.org/abs/2502.17514v1",
        "arxiv_title": "SAE-V: Interpreting Multimodal Models for Enhanced Alignment",
        "authors": [
          "Hantao Lou",
          "Changye Li",
          "Jiaming Ji",
          "Yaodong Yang"
        ],
        "published": "2025-02-22T14:20:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
    "url": "https://icml.cc/virtual/2025/poster/44766",
    "abstract": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.03122v4",
    "arxiv_id": "2503.03122v4",
    "arxiv_title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
    "arxiv_authors": [
      "Zichao Li",
      "Xueru Wen",
      "Jie Lou",
      "Yuqiu Ji",
      "Yaojie Lu",
      "Xianpei Han",
      "Debing Zhang",
      "Le Sun"
    ],
    "arxiv_published": "2025-03-05T02:37:41+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.03122v4",
        "arxiv_url": "http://arxiv.org/abs/2503.03122v4",
        "arxiv_title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models",
        "authors": [
          "Zichao Li",
          "Xueru Wen",
          "Jie Lou",
          "Yuqiu Ji",
          "Yaojie Lu",
          "Xianpei Han",
          "Debing Zhang",
          "Le Sun"
        ],
        "published": "2025-03-05T02:37:41+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
    "url": "https://icml.cc/virtual/2025/poster/45015",
    "abstract": "The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured \"harm-benefit tree,\" which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impact on any stakeholders. SafetyAnalyst then aggregates all harmful and beneficial effects into a harmfulness score using fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this conceptual framework to develop, test, and release an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that SafetyAnalyst (average F1=0.81) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.16665v2",
    "arxiv_id": "2410.16665v2",
    "arxiv_title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
    "arxiv_authors": [
      "Jing-Jing Li",
      "Valentina Pyatkin",
      "Max Kleiman-Weiner",
      "Liwei Jiang",
      "Nouha Dziri",
      "Anne G. E. Collins",
      "Jana Schaich Borg",
      "Maarten Sap",
      "Yejin Choi",
      "Sydney Levine"
    ],
    "arxiv_published": "2024-10-22T03:38:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.16665v2",
        "arxiv_url": "http://arxiv.org/abs/2410.16665v2",
        "arxiv_title": "SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior",
        "authors": [
          "Jing-Jing Li",
          "Valentina Pyatkin",
          "Max Kleiman-Weiner",
          "Liwei Jiang",
          "Nouha Dziri",
          "Anne G. E. Collins",
          "Jana Schaich Borg",
          "Maarten Sap",
          "Yejin Choi",
          "Sydney Levine"
        ],
        "published": "2024-10-22T03:38:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44809",
    "abstract": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.02384v1",
    "arxiv_id": "2502.02384v1",
    "arxiv_title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
    "arxiv_authors": [
      "Yichi Zhang",
      "Siyuan Zhang",
      "Yao Huang",
      "Zeyu Xia",
      "Zhengwei Fang",
      "Xiao Yang",
      "Ranjie Duan",
      "Dong Yan",
      "Yinpeng Dong",
      "Jun Zhu"
    ],
    "arxiv_published": "2025-02-04T15:02:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.02384v1",
        "arxiv_url": "http://arxiv.org/abs/2502.02384v1",
        "arxiv_title": "STAIR: Improving Safety Alignment with Introspective Reasoning",
        "authors": [
          "Yichi Zhang",
          "Siyuan Zhang",
          "Yao Huang",
          "Zeyu Xia",
          "Zhengwei Fang",
          "Xiao Yang",
          "Ranjie Duan",
          "Dong Yan",
          "Yinpeng Dong",
          "Jun Zhu"
        ],
        "published": "2025-02-04T15:02:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "url": "https://icml.cc/virtual/2025/poster/46075",
    "abstract": "Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role—a concept we call role separation—is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine role-separation learning: the process of teaching LLMs to robustly distinguish system and user tokens. Through a simple, controlled experimental framework, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing invariant signals that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, modifying position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2505.00626v2",
    "arxiv_id": "2505.00626v2",
    "arxiv_title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
    "arxiv_authors": [
      "Zihao Wang",
      "Yibo Jiang",
      "Jiahao Yu",
      "Heqing Huang"
    ],
    "arxiv_published": "2025-05-01T16:06:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2505.00626v2",
        "arxiv_url": "http://arxiv.org/abs/2505.00626v2",
        "arxiv_title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)",
        "authors": [
          "Zihao Wang",
          "Yibo Jiang",
          "Jiahao Yu",
          "Heqing Huang"
        ],
        "published": "2025-05-01T16:06:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "url": "https://icml.cc/virtual/2025/poster/45153",
    "abstract": "Benchmark Data Contamination (BDC)—the inclusion of benchmark testing samples in the training set—has raised increasing concerns in Large Language Model (LLM) evaluation, leading to falsely inflated performance estimates and undermining evaluation reliability. To address this, researchers have proposed various mitigation strategies to update existing benchmarks, including modifying original questions or generating new ones based on them. However, a rigorous examination of the effectiveness of these mitigation strategies remains lacking. In this paper, we design a systematic and controlled pipeline along with two novel metrics—fidelity and contamination resistance—to provide a fine-grained and comprehensive assessment of existing BDC mitigation strategies. Previous assessment methods, such as accuracy drop and accuracy matching, focus solely on aggregate accuracy, often leading to incomplete or misleading conclusions. Our metrics address this limitation by emphasizing question-level evaluation result matching. Extensive experiments with 10 LLMs, 5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios reveal that no existing strategy significantly improves resistance over the vanilla case (i.e., no benchmark update) across all benchmarks, and none effectively balances fidelity and contamination resistance. These findings underscore the urgent need for designing more effective BDC mitigation strategies.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.16402v1",
    "arxiv_id": "2503.16402v1",
    "arxiv_title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
    "arxiv_authors": [
      "Yifan Sun",
      "Han Wang",
      "Dongbai Li",
      "Gang Wang",
      "Huan Zhang"
    ],
    "arxiv_published": "2025-03-20T17:55:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.16402v1",
        "arxiv_url": "http://arxiv.org/abs/2503.16402v1",
        "arxiv_title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination",
        "authors": [
          "Yifan Sun",
          "Han Wang",
          "Dongbai Li",
          "Gang Wang",
          "Huan Zhang"
        ],
        "published": "2025-03-20T17:55:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
    "url": "https://icml.cc/virtual/2025/poster/44356",
    "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures.We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10-fold, providing a scalable, rigorous solution for hypothesis validation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.09858v1",
    "arxiv_id": "2502.09858v1",
    "arxiv_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
    "arxiv_authors": [
      "Kexin Huang",
      "Ying Jin",
      "Ryan Li",
      "Michael Y. Li",
      "Emmanuel Candès",
      "Jure Leskovec"
    ],
    "arxiv_published": "2025-02-14T01:46:00+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.09858v1",
        "arxiv_url": "http://arxiv.org/abs/2502.09858v1",
        "arxiv_title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "authors": [
          "Kexin Huang",
          "Ying Jin",
          "Ryan Li",
          "Michael Y. Li",
          "Emmanuel Candès",
          "Jure Leskovec"
        ],
        "published": "2025-02-14T01:46:00+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
    "url": "https://icml.cc/virtual/2025/poster/46324",
    "abstract": "Large language models (LLMs) are known to struggle with consistently generating truthful responses. While various representation intervention techniques have been proposed, these methods typically apply a universal representation correction vector to all input queries, limiting their effectiveness against diverse queries in practice. In this study, we introduce TruthFlow, a novel method that leverages the Flow Matching technique for query-specific truthful representation correction. Specifically, TruthFlow first uses a flow model to learn query-specific correction vectors that transition representations from hallucinated to truthful states. Then, during inference, the trained flow model generates these correction vectors to enhance the truthfulness of LLM outputs. Experimental results demonstrate that TruthFlow significantly improves performance on open-ended generation tasks across various advanced LLMs evaluated on TruthfulQA. Moreover, the trained TruthFlow model exhibits strong transferability, performing effectively on other unseen hallucination benchmarks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04556v1",
    "arxiv_id": "2502.04556v1",
    "arxiv_title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
    "arxiv_authors": [
      "Hanyu Wang",
      "Bochuan Cao",
      "Yuanpu Cao",
      "Jinghui Chen"
    ],
    "arxiv_published": "2025-02-06T23:10:14+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04556v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04556v1",
        "arxiv_title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
        "authors": [
          "Hanyu Wang",
          "Bochuan Cao",
          "Yuanpu Cao",
          "Jinghui Chen"
        ],
        "published": "2025-02-06T23:10:14+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Learning to Route LLM with Confidence Tokens",
    "url": "https://icml.cc/virtual/2025/poster/45145",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.13284v2",
    "arxiv_id": "2410.13284v2",
    "arxiv_title": "Learning to Route LLMs with Confidence Tokens",
    "arxiv_authors": [
      "Yu-Neng Chuang",
      "Helen Zhou",
      "Prathusha Kameswara Sarma",
      "Parikshit Gopalan",
      "John Boccio",
      "Sara Bolouki",
      "Xia Hu"
    ],
    "arxiv_published": "2024-10-17T07:28:18+00:00",
    "similarity_score": 0.9887640449438202,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.13284v2",
        "arxiv_url": "http://arxiv.org/abs/2410.13284v2",
        "arxiv_title": "Learning to Route LLMs with Confidence Tokens",
        "authors": [
          "Yu-Neng Chuang",
          "Helen Zhou",
          "Prathusha Kameswara Sarma",
          "Parikshit Gopalan",
          "John Boccio",
          "Sara Bolouki",
          "Xia Hu"
        ],
        "published": "2024-10-17T07:28:18+00:00",
        "similarity_score": 0.9887640449438202
      }
    ]
  },
  {
    "title": "FlipAttack: Jailbreak LLMs via Flipping",
    "url": "https://icml.cc/virtual/2025/poster/45738",
    "abstract": "This paper proposes a simple yet effective jailbreak attack named FlipAttack against black-box LLMs. First, from the autoregressive nature, we reveal that LLMs tend to understand the text from left to right and find that they struggle to comprehend the text when the perturbation is added to the left side. Motivated by these insights, we propose to disguise the harmful prompt by constructing a left-side perturbation merely based on the prompt itself, then generalize this idea to 4 flipping modes. Second, we verify the strong ability of LLMs to perform the text-flipping task and then develop 4 variants to guide LLMs to understand and execute harmful behaviors accurately. These designs keep FlipAttack universal, stealthy, and simple, allowing it to jailbreak black-box LLMs within only 1 query. Experiments on 8 LLMs demonstrate the superiority of FlipAttack. Remarkably, it achieves $\\sim$78.97\\% attack success rate across 8 LLMs on average and $\\sim$98\\% bypass rate against 5 guard models on average.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02832v1",
    "arxiv_id": "2410.02832v1",
    "arxiv_title": "FlipAttack: Jailbreak LLMs via Flipping",
    "arxiv_authors": [
      "Yue Liu",
      "Xiaoxin He",
      "Miao Xiong",
      "Jinlan Fu",
      "Shumin Deng",
      "Bryan Hooi"
    ],
    "arxiv_published": "2024-10-02T08:41:23+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02832v1",
        "arxiv_url": "http://arxiv.org/abs/2410.02832v1",
        "arxiv_title": "FlipAttack: Jailbreak LLMs via Flipping",
        "authors": [
          "Yue Liu",
          "Xiaoxin He",
          "Miao Xiong",
          "Jinlan Fu",
          "Shumin Deng",
          "Bryan Hooi"
        ],
        "published": "2024-10-02T08:41:23+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Generalized Interpolating Discrete Diffusion",
    "url": "https://icml.cc/virtual/2025/poster/43859",
    "abstract": "While state-of-the-art language models achieve impressive results through next-token prediction, they have inherent limitations such as the inability to revise already generated tokens. This has prompted exploration of alternative approaches such as discrete diffusion. However, masked diffusion, which has emerged as a popular choice due to its simplicity and effectiveness, reintroduces this inability to revise words. To overcome this, we generalize masked diffusion and derive the theoretical backbone of a family of general interpolating discrete diffusion (GIDD) processes offering greater flexibility in the design of the noising processes. Leveraging a novel diffusion ELBO, we achieve compute-matched state-of-the-art performance in diffusion language modeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining masking and uniform noise, leading to improved sample quality and unlocking the ability for the model to correct its own mistakes, an area where autoregressive models notoriously have struggled. Our code and models will be released upon acceptance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.04482v1",
    "arxiv_id": "2503.04482v1",
    "arxiv_title": "Generalized Interpolating Discrete Diffusion",
    "arxiv_authors": [
      "Dimitri von Rütte",
      "Janis Fluri",
      "Yuhui Ding",
      "Antonio Orvieto",
      "Bernhard Schölkopf",
      "Thomas Hofmann"
    ],
    "arxiv_published": "2025-03-06T14:30:55+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.04482v1",
        "arxiv_url": "http://arxiv.org/abs/2503.04482v1",
        "arxiv_title": "Generalized Interpolating Discrete Diffusion",
        "authors": [
          "Dimitri von Rütte",
          "Janis Fluri",
          "Yuhui Ding",
          "Antonio Orvieto",
          "Bernhard Schölkopf",
          "Thomas Hofmann"
        ],
        "published": "2025-03-06T14:30:55+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "url": "https://icml.cc/virtual/2025/poster/46590",
    "abstract": "The canonical setup of learning a reward model (RM) from human preferences with binary feedback discards potentially useful samples (such as \"tied\" between the two responses) and loses fine-grained information  (such as \"slightly better'\"). This paper proposes a framework for learning RMs under ordinal feedback, generalizing the binary feedback to arbitrary granularity. We first identify a marginal unbiasedness condition, which generalizes the existing assumption of the binary feedback. The condition is validated via the sociological concept called \"wisdom of the crowd\". Under this condition, we develop a natural probability model and prove the benefits of fine-grained feedback in terms of reducing the Rademacher complexity, which may be of independent interest to another problem: the bias-variance trade-off in knowledge distillation. The framework also sheds light on designing guidelines for human annotators. Our numerical experiments validate that: (1) fine-grained feedback leads to better RM learning for both in- and out-of-distribution settings; (2) incorporating a certain proportion of tied samples boosts RM learning.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2411.12843v1",
    "arxiv_id": "2411.12843v1",
    "arxiv_title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
    "arxiv_authors": [
      "Shang Liu",
      "Yu Pan",
      "Guanting Chen",
      "Xiaocheng Li"
    ],
    "arxiv_published": "2024-11-19T20:17:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2411.12843v1",
        "arxiv_url": "http://arxiv.org/abs/2411.12843v1",
        "arxiv_title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
        "authors": [
          "Shang Liu",
          "Yu Pan",
          "Guanting Chen",
          "Xiaocheng Li"
        ],
        "published": "2024-11-19T20:17:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Persistent Topological Features in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/43958",
    "abstract": "Understanding the decision-making processes of large language models is critical given their widespread applications. To achieve this, we aim to connect a formal mathematical framework—zigzag persistence from topological data analysis —with practical and easily applicable algorithms. Zigzag persistence is particularly effective for characterizing data as it dynamically transforms across model layers. Within this framework, we introduce topological descriptors that measure how topological features, $p$-dimensional holes, persist and evolve throughout the layers. Unlike methods that assess each layer individually and then aggregate the results, our approach directly tracks the full evolutionary path of these features. This offers a statistical perspective on how prompts are rearranged and their relative positions changed in the representation space, providing insights into the system’s operation as an integrated whole. To demonstrate the expressivity and applicability of our framework, we highlight how sensitive these descriptors are to different models and a variety of datasets. As a showcase application to a downstream task, we use zigzag persistence to establish a criterion for layer pruning, achieving results comparable to state-of-the-art methods while preserving the system-level perspective.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.11042v1",
    "arxiv_id": "2410.11042v1",
    "arxiv_title": "Persistent Topological Features in Large Language Models",
    "arxiv_authors": [
      "Yuri Gardinazzi",
      "Giada Panerai",
      "Karthik Viswanathan",
      "Alessio Ansuini",
      "Alberto Cazzaniga",
      "Matteo Biagetti"
    ],
    "arxiv_published": "2024-10-14T19:46:23+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.11042v1",
        "arxiv_url": "http://arxiv.org/abs/2410.11042v1",
        "arxiv_title": "Persistent Topological Features in Large Language Models",
        "authors": [
          "Yuri Gardinazzi",
          "Giada Panerai",
          "Karthik Viswanathan",
          "Alessio Ansuini",
          "Alberto Cazzaniga",
          "Matteo Biagetti"
        ],
        "published": "2024-10-14T19:46:23+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Scaling Laws for Differentially Private Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46020",
    "abstract": "Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive. LLMs also rely on large, high-quality training datasets, like those sourced from (sometimes sensitive) user data. Training models on this sensitive user data requires careful privacy protections like differential privacy (DP). However, the dynamics of DP training are significantly different, and consequently their scaling laws are not yet fully understood. In this work, we establish scaling laws that accurately model the intricacies of DP LLM training, providing a complete picture of the compute-privacy-utility and the optimal training configurations in many settings.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18914v1",
    "arxiv_id": "2501.18914v1",
    "arxiv_title": "Scaling Laws for Differentially Private Language Models",
    "arxiv_authors": [
      "Ryan McKenna",
      "Yangsibo Huang",
      "Amer Sinha",
      "Borja Balle",
      "Zachary Charles",
      "Christopher A. Choquette-Choo",
      "Badih Ghazi",
      "George Kaissis",
      "Ravi Kumar",
      "Ruibo Liu",
      "Da Yu",
      "Chiyuan Zhang"
    ],
    "arxiv_published": "2025-01-31T06:32:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18914v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18914v1",
        "arxiv_title": "Scaling Laws for Differentially Private Language Models",
        "authors": [
          "Ryan McKenna",
          "Yangsibo Huang",
          "Amer Sinha",
          "Borja Balle",
          "Zachary Charles",
          "Christopher A. Choquette-Choo",
          "Badih Ghazi",
          "George Kaissis",
          "Ravi Kumar",
          "Ruibo Liu",
          "Da Yu",
          "Chiyuan Zhang"
        ],
        "published": "2025-01-31T06:32:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/40168",
    "abstract": "This position paper argues that the majority of theory of mind benchmarks are broken because of their inability to directly test how large language models (LLMs) adapt to new partners. This problem stems from the fact that theory of mind benchmarks for LLMs are overwhelmingly inspired by the methods used to test theory of mind in humans and fall victim to a fallacy of attributing human-like qualities to AI agents. We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs. Most theory of mind benchmarks only measure what we call literal theory of mind: the ability to predict the behavior of others. Measuring this kind of reasoning is very informative in testing the ability of agents with self-consistent reasoning. However, it is important to note the distinction between this and what we actually care about when this self-consistency cannot be taken for granted. We call this functional theory of mind: the ability to adapt to agents in-context following a rational response to predictions about their behavior. We find that top performing open source LLMs may display strong capabilities in literal theory of mind, depending on how they are prompted, but seem to struggle with functional theory of mind -- even when partner policies are exceedingly simple. Simply put, strong literal theory of mind performance does not necessarily imply strong functional theory of mind performance. Achieving functional theory of mind, particularly over long interaction horizons with a partner, is a significant challenge deserving a prominent role in any meaningful LLM theory of mind evaluation.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.19726v2",
    "arxiv_id": "2412.19726v2",
    "arxiv_title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
    "arxiv_authors": [
      "Matthew Riemer",
      "Zahra Ashktorab",
      "Djallel Bouneffouf",
      "Payel Das",
      "Miao Liu",
      "Justin D. Weisz",
      "Murray Campbell"
    ],
    "arxiv_published": "2024-12-27T16:30:12+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.19726v2",
        "arxiv_url": "http://arxiv.org/abs/2412.19726v2",
        "arxiv_title": "Position: Theory of Mind Benchmarks are Broken for Large Language Models",
        "authors": [
          "Matthew Riemer",
          "Zahra Ashktorab",
          "Djallel Bouneffouf",
          "Payel Das",
          "Miao Liu",
          "Justin D. Weisz",
          "Murray Campbell"
        ],
        "published": "2024-12-27T16:30:12+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Empirical Privacy Variance",
    "url": "https://icml.cc/virtual/2025/poster/44071",
    "abstract": "We propose the notion of empirical privacy variance and study it in the context of differentially private fine-tuning of language models. Specifically, we show that models calibrated to the same $(\\varepsilon, \\delta)$-DP guarantee using DP-SGD with different hyperparameter configurations can exhibit significant variations in empirical privacy, which we quantify through the lens of memorization. We investigate the generality of this phenomenon across multiple dimensions and discuss why it is surprising and relevant. Through regression analysis, we examine how individual and composite hyperparameters influence empirical privacy. The results reveal a no-free-lunch trade-off: existing practices of hyperparameter tuning in DP-SGD, which focus on optimizing utility under a fixed privacy budget, often come at the expense of empirical privacy. To address this, we propose refined heuristics for hyperparameter selection that explicitly account for empirical privacy, showing that they are both precise and practically useful. Finally, we take preliminary steps to understand empirical privacy variance. We propose two hypotheses, identify limitations in existing techniques like privacy auditing, and outline open questions for future research.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.12314v1",
    "arxiv_id": "2503.12314v1",
    "arxiv_title": "Empirical Privacy Variance",
    "arxiv_authors": [
      "Yuzheng Hu",
      "Fan Wu",
      "Ruicheng Xian",
      "Yuhang Liu",
      "Lydia Zakynthinou",
      "Pritish Kamath",
      "Chiyuan Zhang",
      "David Forsyth"
    ],
    "arxiv_published": "2025-03-16T01:43:49+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.12314v1",
        "arxiv_url": "http://arxiv.org/abs/2503.12314v1",
        "arxiv_title": "Empirical Privacy Variance",
        "authors": [
          "Yuzheng Hu",
          "Fan Wu",
          "Ruicheng Xian",
          "Yuhang Liu",
          "Lydia Zakynthinou",
          "Pritish Kamath",
          "Chiyuan Zhang",
          "David Forsyth"
        ],
        "published": "2025-03-16T01:43:49+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
    "url": "https://icml.cc/virtual/2025/poster/43678",
    "abstract": "Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates.  Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: \\textit{heterogeneous token overfitting} (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates.To tackle this, we propose {\\name}, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.00602v1",
    "arxiv_id": "2502.00602v1",
    "arxiv_title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
    "arxiv_authors": [
      "Tianci Liu",
      "Zihan Dong",
      "Linjun Zhang",
      "Haoyu Wang",
      "Jing Gao"
    ],
    "arxiv_published": "2025-02-02T00:10:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.00602v1",
        "arxiv_url": "http://arxiv.org/abs/2502.00602v1",
        "arxiv_title": "Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing",
        "authors": [
          "Tianci Liu",
          "Zihan Dong",
          "Linjun Zhang",
          "Haoyu Wang",
          "Jing Gao"
        ],
        "published": "2025-02-02T00:10:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
    "url": "https://icml.cc/virtual/2025/poster/46232",
    "abstract": "Keeping large language models factually up-to-date is crucial for deployment, yet costly retraining remains a challenge. Knowledge editing offers a promising alternative, but methods are only tested on small-scale or synthetic edit benchmarks. In this work, we aim to bridge research into lifelong knowledge editing to real-world edits at practically relevant scale. We first introduce \\texttt{WikiBigEdit}; a large-scale benchmark of real-world Wikidata edits, built to automatically extend lifelong for future-proof benchmarking. In its first instance, it includes over 500K question-answer pairs for knowledge editing alongside a comprehensive evaluation pipeline. Finally, we use \\texttt{WikiBigEdit} to study existing knowledge editing techniques' ability to incorporate large volumes of real-world facts and contrast their capabilities to generic modification techniques such as retrieval augmentation and continual finetuning to acquire a complete picture of the practical extent of current lifelong knowledge editing.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.05683v1",
    "arxiv_id": "2503.05683v1",
    "arxiv_title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
    "arxiv_authors": [
      "Lukas Thede",
      "Karsten Roth",
      "Matthias Bethge",
      "Zeynep Akata",
      "Tom Hartvigsen"
    ],
    "arxiv_published": "2025-03-07T18:45:42+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.05683v1",
        "arxiv_url": "http://arxiv.org/abs/2503.05683v1",
        "arxiv_title": "Understanding the Limits of Lifelong Knowledge Editing in LLMs",
        "authors": [
          "Lukas Thede",
          "Karsten Roth",
          "Matthias Bethge",
          "Zeynep Akata",
          "Tom Hartvigsen"
        ],
        "published": "2025-03-07T18:45:42+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
    "url": "https://icml.cc/virtual/2025/poster/46121",
    "abstract": "Training safe LLMs is one of the most critical research challenge. However, the commonly used method, Refusal Training (RT), struggles to generalize against various OOD jailbreaking attacks. Many safety training methods have been proposed to address this issue. While they offer valuable insights, we aim to complement this line of research by investigating whether OOD attacks truly exceed the capability of RT model. Conducting evaluation with BoN, we observe significant improvements on generalization as N increases. This underscores that the model possesses sufficient safety-related latent knowledge, but RT fails to consistently elicit this knowledge when addressing OOD attacks. Further analysis based on domain adaptation reveals that training with direct refusal causes model to rely on superficial shortcuts, resulting in learning of non-robust representation mappings. Based on our findings, we propose training model to perform safety reasoning for each query. Reasoning supervision encourages model to perform more computations, explicitly eliciting and using latent knowledge through reasoning. To achieve this, we synthesize reasoning supervision based on pre-guidelines, training the model to reason in alignment with them, thereby effectively eliciting and utilizing latent knowledge from diverse perspectives. Extensive experiments show that our method significantly improves generalization performance against OOD attacks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.04040v1",
    "arxiv_id": "2502.04040v1",
    "arxiv_title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
    "arxiv_authors": [
      "Haoyu Wang",
      "Zeyu Qin",
      "Li Shen",
      "Xueqian Wang",
      "Minhao Cheng",
      "Dacheng Tao"
    ],
    "arxiv_published": "2025-02-06T13:01:44+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.04040v1",
        "arxiv_url": "http://arxiv.org/abs/2502.04040v1",
        "arxiv_title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
        "authors": [
          "Haoyu Wang",
          "Zeyu Qin",
          "Li Shen",
          "Xueqian Wang",
          "Minhao Cheng",
          "Dacheng Tao"
        ],
        "published": "2025-02-06T13:01:44+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "url": "https://icml.cc/virtual/2025/poster/45083",
    "abstract": "Large Language Models (LLMs) are expected to be predictable and trustworthy to support reliable decision-making systems. Yet current LLMs often show inconsistencies in their judgments. In this work, we examine \\textit{logical preference consistency} as a foundational requirement for building more dependable LLM systems, ensuring stable and coherent decision-making while minimizing erratic or contradictory outputs.To quantify the logical preference consistency, we propose a universal evaluation framework based on three fundamental properties: transitivity, commutativity and negation invariance.Through extensive experimentation across diverse LLMs, we demonstrate that these properties serve as strong indicators of judgment robustness.Furthermore, we introduce a data refinement and augmentation technique, REPAIR, that enhances logical consistency while maintaining alignment with human preferences. Finally, we show that improving consistency leads to better performance in LLM-driven logic-based algorithms, reinforcing stability and coherence in decision-making systems.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.02205v3",
    "arxiv_id": "2410.02205v3",
    "arxiv_title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
    "arxiv_authors": [
      "Yinhong Liu",
      "Zhijiang Guo",
      "Tianya Liang",
      "Ehsan Shareghi",
      "Ivan Vulić",
      "Nigel Collier"
    ],
    "arxiv_published": "2024-10-03T04:34:04+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.02205v3",
        "arxiv_url": "http://arxiv.org/abs/2410.02205v3",
        "arxiv_title": "Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models",
        "authors": [
          "Yinhong Liu",
          "Zhijiang Guo",
          "Tianya Liang",
          "Ehsan Shareghi",
          "Ivan Vulić",
          "Nigel Collier"
        ],
        "published": "2024-10-03T04:34:04+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "url": "https://icml.cc/virtual/2025/poster/45556",
    "abstract": "Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when generating harmful or untruthful responses. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.16638v3",
    "arxiv_id": "2410.16638v3",
    "arxiv_title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
    "arxiv_authors": [
      "Mengdi Zhang",
      "Kai Kiat Goh",
      "Peixin Zhang",
      "Jun Sun",
      "Rose Lin Xin",
      "Hongyu Zhang"
    ],
    "arxiv_published": "2024-10-22T02:27:57+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.16638v3",
        "arxiv_url": "http://arxiv.org/abs/2410.16638v3",
        "arxiv_title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
        "authors": [
          "Mengdi Zhang",
          "Kai Kiat Goh",
          "Peixin Zhang",
          "Jun Sun",
          "Rose Lin Xin",
          "Hongyu Zhang"
        ],
        "published": "2024-10-22T02:27:57+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
    "url": "https://icml.cc/virtual/2025/poster/44807",
    "abstract": "Large language models (LLMs) often produce incorrect or outdated information, necessitating efficient and precise knowledge updates. Current model editing methods, however, struggle with long-form knowledge in diverse formats, such as poetry, code snippets, and mathematical derivations. These limitations arise from their reliance on editing a single token’s hidden state, a limitation we term ``efficacy barrier''. To solve this, we propose \\textbf{AnyEdit}, a new autoregressive editing paradigm. It decomposes long-form knowledge into sequential chunks and iteratively edits the key token in each chunk, ensuring consistent and accurate outputs. Theoretically, we ground AnyEdit in the Chain Rule of Mutual Information, showing its ability to update any knowledge within LLMs.Empirically, it outperforms strong baselines by 21.5\\% on benchmarks including UnKEBench, AKEW, and our new \\textbf{EditEverything} dataset for long-form diverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-play framework, enabling current editing methods to update knowledge with arbitrary length and format, significantly advancing the scope and practicality of LLM knowledge editing. Our code is available at: \\url{https://anonymous.4open.science/r/AnyEdit}.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.05628v2",
    "arxiv_id": "2502.05628v2",
    "arxiv_title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
    "arxiv_authors": [
      "Houcheng Jiang",
      "Junfeng Fang",
      "Ningyu Zhang",
      "Guojun Ma",
      "Mingyang Wan",
      "Xiang Wang",
      "Xiangnan He",
      "Tat-seng Chua"
    ],
    "arxiv_published": "2025-02-08T16:18:37+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.05628v2",
        "arxiv_url": "http://arxiv.org/abs/2502.05628v2",
        "arxiv_title": "AnyEdit: Edit Any Knowledge Encoded in Language Models",
        "authors": [
          "Houcheng Jiang",
          "Junfeng Fang",
          "Ningyu Zhang",
          "Guojun Ma",
          "Mingyang Wan",
          "Xiang Wang",
          "Xiangnan He",
          "Tat-seng Chua"
        ],
        "published": "2025-02-08T16:18:37+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "url": "https://icml.cc/virtual/2025/poster/46294",
    "abstract": "This work identifies the Energy Loss Phenomenon in Reinforcement Learning from Human Feedback (RLHF) and its connection to reward hacking. Specifically,  energy loss in the final layer of a Large Language Model (LLM) gradually increases during the RL process, with an excessive increase in energy loss characterizing reward hacking. Beyond empirical analysis, we further provide a theoretical foundation by proving that, under mild conditions, the increased energy loss reduces the upper bound of contextual relevance in LLMs, which is a critical aspect of reward hacking as the reduced contextual relevance typically indicates overfitting to reward model-favored patterns in RL. To address this issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the increase in energy loss in the LLM's final layer during reward calculation to prevent excessive energy loss, thereby mitigating reward hacking. We theoretically show that EPPO can be conceptually interpreted as an entropy-regularized RL algorithm, which provides deeper insights into its effectiveness. Extensive experiments across various LLMs and tasks demonstrate the commonality of the energy loss phenomenon, as well as the effectiveness of EPPO in mitigating reward hacking and improving RLHF performance.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.19358v2",
    "arxiv_id": "2501.19358v2",
    "arxiv_title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
    "arxiv_authors": [
      "Yuchun Miao",
      "Sen Zhang",
      "Liang Ding",
      "Yuqi Zhang",
      "Lefei Zhang",
      "Dacheng Tao"
    ],
    "arxiv_published": "2025-01-31T18:10:53+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.19358v2",
        "arxiv_url": "http://arxiv.org/abs/2501.19358v2",
        "arxiv_title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
        "authors": [
          "Yuchun Miao",
          "Sen Zhang",
          "Liang Ding",
          "Yuqi Zhang",
          "Lefei Zhang",
          "Dacheng Tao"
        ],
        "published": "2025-01-31T18:10:53+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
    "url": "https://icml.cc/virtual/2025/poster/46116",
    "abstract": "We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.16475v1",
    "arxiv_id": "2412.16475v1",
    "arxiv_title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
    "arxiv_authors": [
      "Yuchen Zhu",
      "Daniel Augusto de Souza",
      "Zhengyan Shi",
      "Mengyue Yang",
      "Pasquale Minervini",
      "Alexander D'Amour",
      "Matt J. Kusner"
    ],
    "arxiv_published": "2024-12-21T04:07:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.16475v1",
        "arxiv_url": "http://arxiv.org/abs/2412.16475v1",
        "arxiv_title": "When Can Proxies Improve the Sample Complexity of Preference Learning?",
        "authors": [
          "Yuchen Zhu",
          "Daniel Augusto de Souza",
          "Zhengyan Shi",
          "Mengyue Yang",
          "Pasquale Minervini",
          "Alexander D'Amour",
          "Matt J. Kusner"
        ],
        "published": "2024-12-21T04:07:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
    "url": "https://icml.cc/virtual/2025/poster/43935",
    "abstract": "We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning ten categories across four high-level classes, and find that the majority of disagreements are due to factors such as task Underspecification or response style. Our findings are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, and LLM-as-Judge evaluation methods fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence in evaluations and during LLM training.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2410.14632v2",
    "arxiv_id": "2410.14632v2",
    "arxiv_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
    "arxiv_authors": [
      "Michael JQ Zhang",
      "Zhilin Wang",
      "Jena D. Hwang",
      "Yi Dong",
      "Olivier Delalleau",
      "Yejin Choi",
      "Eunsol Choi",
      "Xiang Ren",
      "Valentina Pyatkin"
    ],
    "arxiv_published": "2024-10-18T17:32:22+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2410.14632v2",
        "arxiv_url": "http://arxiv.org/abs/2410.14632v2",
        "arxiv_title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
        "authors": [
          "Michael JQ Zhang",
          "Zhilin Wang",
          "Jena D. Hwang",
          "Yi Dong",
          "Olivier Delalleau",
          "Yejin Choi",
          "Eunsol Choi",
          "Xiang Ren",
          "Valentina Pyatkin"
        ],
        "published": "2024-10-18T17:32:22+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
    "url": "https://icml.cc/virtual/2025/poster/43915",
    "abstract": "The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16\\% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.09650v2",
    "arxiv_id": "2502.09650v2",
    "arxiv_title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
    "arxiv_authors": [
      "Chengqian Gao",
      "Haonan Li",
      "Liu Liu",
      "Zeke Xie",
      "Peilin Zhao",
      "Zhiqiang Xu"
    ],
    "arxiv_published": "2025-02-11T17:01:11+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.09650v2",
        "arxiv_url": "http://arxiv.org/abs/2502.09650v2",
        "arxiv_title": "Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples",
        "authors": [
          "Chengqian Gao",
          "Haonan Li",
          "Liu Liu",
          "Zeke Xie",
          "Peilin Zhao",
          "Zhiqiang Xu"
        ],
        "published": "2025-02-11T17:01:11+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/45505",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Our framework addresses two critical questions: (1) how to generate high-quality reasoning processes during inference automatically, and (2) how to integrate these processes into post-training. We propose the \\emph{Bootstrapping Reinforced Thinking Process} (BRiTE) algorithm and demonstrate its theoretical convergence at a rate of $1/T$, where $T$ is the number of iterations. The algorithm operates in two steps. First, it generates high-quality rationales by approximating the desired posterior distribution using a reinforcement learning approach with a novel reward shaping mechanism. Second, it fine-tunes the base LLM by maximizing the joint probability of rationale generation with respect to LLM parameters. Empirical evaluation on GSM8K and MATH benchmarks demonstrates that our approach consistently improves performance across different model sizes without requiring human-annotated thinking processes, outperforming standard chain-of-thought prompting while enhancing existing post-training methods.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.18858v1",
    "arxiv_id": "2501.18858v1",
    "arxiv_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
    "arxiv_authors": [
      "Han Zhong",
      "Yutong Yin",
      "Shenao Zhang",
      "Xiaojun Xu",
      "Yuanxin Liu",
      "Yifei Zuo",
      "Zhihan Liu",
      "Boyi Liu",
      "Sirui Zheng",
      "Hongyi Guo",
      "Liwei Wang",
      "Mingyi Hong",
      "Zhaoran Wang"
    ],
    "arxiv_published": "2025-01-31T02:39:07+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.18858v1",
        "arxiv_url": "http://arxiv.org/abs/2501.18858v1",
        "arxiv_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
        "authors": [
          "Han Zhong",
          "Yutong Yin",
          "Shenao Zhang",
          "Xiaojun Xu",
          "Yuanxin Liu",
          "Yifei Zuo",
          "Zhihan Liu",
          "Boyi Liu",
          "Sirui Zheng",
          "Hongyi Guo",
          "Liwei Wang",
          "Mingyi Hong",
          "Zhaoran Wang"
        ],
        "published": "2025-01-31T02:39:07+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reinforced Lifelong Editing for Language Models",
    "url": "https://icml.cc/virtual/2025/poster/46622",
    "abstract": "Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed RLEdit, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a 59.24% improvement while requiring only 2.11% of the time compared to most approaches.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.05759v3",
    "arxiv_id": "2502.05759v3",
    "arxiv_title": "Reinforced Lifelong Editing for Language Models",
    "arxiv_authors": [
      "Zherui Li",
      "Houcheng Jiang",
      "Hao Chen",
      "Baolong Bi",
      "Zhenhong Zhou",
      "Fei Sun",
      "Junfeng Fang",
      "Xiang Wang"
    ],
    "arxiv_published": "2025-02-09T03:37:06+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.05759v3",
        "arxiv_url": "http://arxiv.org/abs/2502.05759v3",
        "arxiv_title": "Reinforced Lifelong Editing for Language Models",
        "authors": [
          "Zherui Li",
          "Houcheng Jiang",
          "Hao Chen",
          "Baolong Bi",
          "Zhenhong Zhou",
          "Fei Sun",
          "Junfeng Fang",
          "Xiang Wang"
        ],
        "published": "2025-02-09T03:37:06+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Conformal Tail Risk Control for Large Language Model Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45795",
    "abstract": "Recent developments in large language models (LLMs) have led to their widespread usage for various tasks. The prevalence of LLMs in society implores the assurance on the reliability of their performance. In particular, risk-sensitive applications demand meticulous attention to unexpectedly poor outcomes, i.e., tail events, for instance,  toxic answers, humiliating language, and offensive outputs. Due to the costly nature of acquiring human annotations, general-purpose scoring models have been created to automate the process of quantifying these tail events. This phenomenon introduces potential human-machine misalignment between the respective scoring mechanisms. In this work, we present a light-weight calibration framework for blackbox models that ensures the alignment of humans and machines with provable guarantees. Our framework provides a rigorous approach to controlling any distortion risk measure that is characterized by a weighted average of quantiles of the loss incurred by the LLM with high confidence. The theoretical foundation of our method rests on the connection between conformal risk control and a traditional family of statistics, i.e., L-statistics. To demonstrate the utility of our framework, we conduct comprehensive experiments that address the issue of human-machine misalignment.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.20285v1",
    "arxiv_id": "2502.20285v1",
    "arxiv_title": "Conformal Tail Risk Control for Large Language Model Alignment",
    "arxiv_authors": [
      "Catherine Yu-Chi Chen",
      "Jingyan Shen",
      "Zhun Deng",
      "Lihua Lei"
    ],
    "arxiv_published": "2025-02-27T17:10:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.20285v1",
        "arxiv_url": "http://arxiv.org/abs/2502.20285v1",
        "arxiv_title": "Conformal Tail Risk Control for Large Language Model Alignment",
        "authors": [
          "Catherine Yu-Chi Chen",
          "Jingyan Shen",
          "Zhun Deng",
          "Lihua Lei"
        ],
        "published": "2025-02-27T17:10:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Reducing Tool Hallucination via Reliability Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45001",
    "abstract": "Large Language Models (LLMs) have expanded their capabilities beyond language generation to interact with external tools, enabling automation and real-world applications. However, tool hallucinations—where models either select inappropriate tools or misuse them—pose significant challenges, leading to erroneous task execution, increased computational costs, and reduced system reliability. To systematically address this issue, we define and categorize tool hallucinations into two main types: tool selection hallucination and tool usage hallucination. To evaluate and mitigate these issues, we introduce RelyToolBench, which integrates specialized test cases and novel metrics to assess hallucination-aware task success and efficiency. Finally, we propose Relign, a reliability alignment framework that expands the tool-use action space to include indecisive actions, allowing LLMs to defer tool use, seek clarification, or adjust tool selection dynamically. Through extensive experiments, we demonstrate that Relign significantly reduces tool hallucinations, improves task reliability, and enhances the efficiency of LLM tool interactions. The code and data will be publicly available.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2412.04141v2",
    "arxiv_id": "2412.04141v2",
    "arxiv_title": "Reducing Tool Hallucination via Reliability Alignment",
    "arxiv_authors": [
      "Hongshen Xu",
      "Zichen Zhu",
      "Lei Pan",
      "Zihan Wang",
      "Su Zhu",
      "Da Ma",
      "Ruisheng Cao",
      "Lu Chen",
      "Kai Yu"
    ],
    "arxiv_published": "2024-12-05T13:10:54+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2412.04141v2",
        "arxiv_url": "http://arxiv.org/abs/2412.04141v2",
        "arxiv_title": "Reducing Tool Hallucination via Reliability Alignment",
        "authors": [
          "Hongshen Xu",
          "Zichen Zhu",
          "Lei Pan",
          "Zihan Wang",
          "Su Zhu",
          "Da Ma",
          "Ruisheng Cao",
          "Lu Chen",
          "Kai Yu"
        ],
        "published": "2024-12-05T13:10:54+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Teaching Language Models to Critique via Reinforcement Learning",
    "url": "https://icml.cc/virtual/2025/poster/45115",
    "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide *accurate judgments* and *actionable suggestions*. In this work, we study LLM critics for code generation and propose $\\texttt{CTRL}$, a framework for $\\texttt{C}$ritic $\\texttt{T}$raining via $\\texttt{R}$einforcement $\\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models.Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1\\% relative improvements across challenging code generation benchmarks.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.03492v1",
    "arxiv_id": "2502.03492v1",
    "arxiv_title": "Teaching Language Models to Critique via Reinforcement Learning",
    "arxiv_authors": [
      "Zhihui Xie",
      "Jie chen",
      "Liyu Chen",
      "Weichao Mao",
      "Jingjing Xu",
      "Lingpeng Kong"
    ],
    "arxiv_published": "2025-02-05T02:18:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.03492v1",
        "arxiv_url": "http://arxiv.org/abs/2502.03492v1",
        "arxiv_title": "Teaching Language Models to Critique via Reinforcement Learning",
        "authors": [
          "Zhihui Xie",
          "Jie chen",
          "Liyu Chen",
          "Weichao Mao",
          "Jingjing Xu",
          "Lingpeng Kong"
        ],
        "published": "2025-02-05T02:18:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "url": "https://icml.cc/virtual/2025/poster/45435",
    "abstract": "A genuinely robust reasoning model should be able to function correctly when the problem statement is modified out-of-training-distribution. Prior work has shown that language models struggle on mathematical benchmarks when questions undergo simple perturbations -- modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycks et. al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.06453v2",
    "arxiv_id": "2502.06453v2",
    "arxiv_title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
    "arxiv_authors": [
      "Kaixuan Huang",
      "Jiacheng Guo",
      "Zihao Li",
      "Xiang Ji",
      "Jiawei Ge",
      "Wenzhe Li",
      "Yingqing Guo",
      "Tianle Cai",
      "Hui Yuan",
      "Runzhe Wang",
      "Yue Wu",
      "Ming Yin",
      "Shange Tang",
      "Yangsibo Huang",
      "Chi Jin",
      "Xinyun Chen",
      "Chiyuan Zhang",
      "Mengdi Wang"
    ],
    "arxiv_published": "2025-02-10T13:31:46+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.06453v2",
        "arxiv_url": "http://arxiv.org/abs/2502.06453v2",
        "arxiv_title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
        "authors": [
          "Kaixuan Huang",
          "Jiacheng Guo",
          "Zihao Li",
          "Xiang Ji",
          "Jiawei Ge",
          "Wenzhe Li",
          "Yingqing Guo",
          "Tianle Cai",
          "Hui Yuan",
          "Runzhe Wang",
          "Yue Wu",
          "Ming Yin",
          "Shange Tang",
          "Yangsibo Huang",
          "Chi Jin",
          "Xinyun Chen",
          "Chiyuan Zhang",
          "Mengdi Wang"
        ],
        "published": "2025-02-10T13:31:46+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "url": "https://icml.cc/virtual/2025/poster/44282",
    "abstract": "Large Language Models (LLMs) have been observed to process non-human-readable text sequences, such as jailbreak prompts, often viewed as a bug for aligned LLMs. In this work, we present a systematic investigation challenging this perception, demonstrating that unnatural languages - strings that appear incomprehensible to humans but maintain semantic meanings for LLMs - contain latent features usable by models. Notably, unnatural languages possess latent features that can be generalized across different models and tasks during inference. Furthermore, models fine-tuned on unnatural versions of instruction datasets perform on-par with those trained on natural language, achieving (49.71) win rates in Length-controlled AlpacaEval 2.0 in average across various base models. In addition, through comprehensive analysis,  we demonstrate that LLMs process unnatural languages by filtering noise and inferring contextual meaning from filtered words.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2503.01926v1",
    "arxiv_id": "2503.01926v1",
    "arxiv_title": "Unnatural Languages Are Not Bugs but Features for LLMs",
    "arxiv_authors": [
      "Keyu Duan",
      "Yiran Zhao",
      "Zhili Feng",
      "Jinjie Ni",
      "Tianyu Pang",
      "Qian Liu",
      "Tianle Cai",
      "Longxu Dou",
      "Kenji Kawaguchi",
      "Anirudh Goyal",
      "J. Zico Kolter",
      "Michael Qizhe Shieh"
    ],
    "arxiv_published": "2025-03-02T12:10:17+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2503.01926v1",
        "arxiv_url": "http://arxiv.org/abs/2503.01926v1",
        "arxiv_title": "Unnatural Languages Are Not Bugs but Features for LLMs",
        "authors": [
          "Keyu Duan",
          "Yiran Zhao",
          "Zhili Feng",
          "Jinjie Ni",
          "Tianyu Pang",
          "Qian Liu",
          "Tianle Cai",
          "Longxu Dou",
          "Kenji Kawaguchi",
          "Anirudh Goyal",
          "J. Zico Kolter",
          "Michael Qizhe Shieh"
        ],
        "published": "2025-03-02T12:10:17+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "url": "https://icml.cc/virtual/2025/poster/45726",
    "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards---a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. We conduct extensive experiments to evaluate \\texttt{RTO} against PPO and other direct preference learning algorithms. The results highlight the effectiveness of RTO, with the algorithm outperforming PPO by 7.5 points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2404.18922v4",
    "arxiv_id": "2404.18922v4",
    "arxiv_title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
    "arxiv_authors": [
      "Han Zhong",
      "Zikang Shan",
      "Guhao Feng",
      "Wei Xiong",
      "Xinle Cheng",
      "Li Zhao",
      "Di He",
      "Jiang Bian",
      "Liwei Wang"
    ],
    "arxiv_published": "2024-04-29T17:58:30+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2404.18922v4",
        "arxiv_url": "http://arxiv.org/abs/2404.18922v4",
        "arxiv_title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
        "authors": [
          "Han Zhong",
          "Zikang Shan",
          "Guhao Feng",
          "Wei Xiong",
          "Xinle Cheng",
          "Li Zhao",
          "Di He",
          "Jiang Bian",
          "Liwei Wang"
        ],
        "published": "2024-04-29T17:58:30+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "url": "https://icml.cc/virtual/2025/poster/44213",
    "abstract": "Test-time scaling, which is also often referred to as slow-thinking, has been demonstrated to enhance multi-step reasoning in large language models (LLMs). However, despite its widespread utilization, the mechanisms underlying slow-thinking methods remain poorly understood. This paper explores the mechanisms of external slow-thinking from a theoretical standpoint. We begin by examining the snowball error effect within the LLM reasoning process and connect it to the likelihood of correct reasoning using information theory. Building on this, we show that external slow-thinking methods can be interpreted as strategies to mitigate the error probability. We further provide a comparative analysis of popular external slow-thinking approaches, ranging from simple to complex, highlighting their differences and interrelationships. Our findings suggest that the efficacy of these methods is not primarily determined by the specific framework employed, and that expanding the search scope or the model's internal reasoning capacity may yield more sustained improvements in the long term. We open-source our code through an anonymous repository https://anonymous.4open.science/r/Snowball-Errors-and-Probability.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2501.15602v2",
    "arxiv_id": "2501.15602v2",
    "arxiv_title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
    "arxiv_authors": [
      "Zeyu Gan",
      "Yun Liao",
      "Yong Liu"
    ],
    "arxiv_published": "2025-01-26T17:05:16+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2501.15602v2",
        "arxiv_url": "http://arxiv.org/abs/2501.15602v2",
        "arxiv_title": "Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning",
        "authors": [
          "Zeyu Gan",
          "Yun Liao",
          "Yong Liu"
        ],
        "published": "2025-01-26T17:05:16+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "url": "https://icml.cc/virtual/2025/poster/45124",
    "abstract": "Existing efforts to align multimodal large language models (MLLMs) with human preferences have only achieved progress in narrow areas, such as hallucination reduction, but remain limited in practical applicability and generalizability. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality.  Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce the Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms.  Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs.  Our approach is rigorously evaluated across 10 distinct dimensions, encompassing 27 benchmarks, with results demonstrating significant and consistent improvements in model performance (Figure.1).",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2502.10391v1",
    "arxiv_id": "2502.10391v1",
    "arxiv_title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
    "arxiv_authors": [
      "Yi-Fan Zhang",
      "Tao Yu",
      "Haochen Tian",
      "Chaoyou Fu",
      "Peiyan Li",
      "Jianshu Zeng",
      "Wulin Xie",
      "Yang Shi",
      "Huanyu Zhang",
      "Junkang Wu",
      "Xue Wang",
      "Yibo Hu",
      "Bin Wen",
      "Fan Yang",
      "Zhang Zhang",
      "Tingting Gao",
      "Di Zhang",
      "Liang Wang",
      "Rong Jin",
      "Tieniu Tan"
    ],
    "arxiv_published": "2025-02-14T18:59:51+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2502.10391v1",
        "arxiv_url": "http://arxiv.org/abs/2502.10391v1",
        "arxiv_title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
        "authors": [
          "Yi-Fan Zhang",
          "Tao Yu",
          "Haochen Tian",
          "Chaoyou Fu",
          "Peiyan Li",
          "Jianshu Zeng",
          "Wulin Xie",
          "Yang Shi",
          "Huanyu Zhang",
          "Junkang Wu",
          "Xue Wang",
          "Yibo Hu",
          "Bin Wen",
          "Fan Yang",
          "Zhang Zhang",
          "Tingting Gao",
          "Di Zhang",
          "Liang Wang",
          "Rong Jin",
          "Tieniu Tan"
        ],
        "published": "2025-02-14T18:59:51+00:00",
        "similarity_score": 1.0
      }
    ]
  },
  {
    "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
    "url": "https://icml.cc/virtual/2025/poster/43951",
    "abstract": "Given how large parts of the publicly available text are crawled to pretrain large language models (LLMs), creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership—i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves generating multiple watermarked rephrases such that a distinct watermark is embedded in each rephrasing. One version is released publicly while others are kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that our approach preserves both the semantic meaning and the utility of benchmarks in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.",
    "is_llm_safety": true,
    "arxiv_url": "http://arxiv.org/abs/2504.13416v1",
    "arxiv_id": "2504.13416v1",
    "arxiv_title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
    "arxiv_authors": [
      "Saksham Rastogi",
      "Pratyush Maini",
      "Danish Pruthi"
    ],
    "arxiv_published": "2025-04-18T02:25:08+00:00",
    "similarity_score": 1.0,
    "arxiv_all_matches": [
      {
        "arxiv_id": "2504.13416v1",
        "arxiv_url": "http://arxiv.org/abs/2504.13416v1",
        "arxiv_title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings",
        "authors": [
          "Saksham Rastogi",
          "Pratyush Maini",
          "Danish Pruthi"
        ],
        "published": "2025-04-18T02:25:08+00:00",
        "similarity_score": 1.0
      }
    ]
  }
]