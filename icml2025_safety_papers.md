# ICML 2025 Safety Related Papers

| No. | Title | ArXiv Link | PDF Link | Main Contents |
|--|-------|------------|----------|---------------|
| 1  | An Efficient Private GPT Never Autoregressively Decodes | [ArXiv](http://arxiv.org/abs/2505.15252v1) | [PDF](./papers/2505.15252v1.pdf) | The paper introduces "Public Decoding and Secure Verification" (Post), a novel approach to accelerate secure GPT inference while preserving privacy and generation quality. By leveraging public GPT models to generate draft tokens, which are then verified by a private model, Post reduces decoding steps. It exploits the observation that secure decoding latency is insensitive to input length. The method enhances efficiency through an optimized speculative sampling protocol and model alignment via knowledge distillation. Experiments across three model pairs and various tasks demonstrate a 2.1× to 6.0× speedup compared to standard secure decoding, with acceptance ratios of 52%-84%. Post is compatible with existing secure inference frameworks and shows potential for further improvement with better-aligned or larger public models. |
| 2  | ROPO: Robust Preference Optimization for Large Language Models | [ArXiv](http://arxiv.org/abs/2404.04102v2) | [PDF](./papers/2404.04102v2.pdf) | The paper proposes ROPO (Robust Preference Optimization), an iterative framework for aligning large language models with human preferences under noisy data. ROPO alternates between noise-tolerant model training and noisy sample filtering, using a robust loss function to suppress gradients of uncertain samples and rejection sampling to improve data quality. Theoretical analysis and experiments on datasets like UltraFeedback, Alpaca Comparison, and TL;DR demonstrate ROPO’s superior noise tolerance and performance over baselines like DPO and IPO, especially in high-noise settings. |
| 3  | Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales | [ArXiv](http://arxiv.org/abs/2405.17618v2) | [PDF](./papers/2405.17618v2.pdf) | introduces a Symmetric Reinforcement Learning (SRL) loss to enhance the robustness of A2C and PPO algorithms against noisy data in diverse tasks. By combining original RL loss with a reverse RL loss, inspired by Symmetric Cross Entropy, the proposed SA2C and SPPO algorithms demonstrate superior performance in Atari games, MuJoCo, Box2D, and RLHF tasks (IMDB sentiment and TL;DR summarization) under noisy and non-noisy conditions. Gradient analysis and experiments confirm improved stability and robustness, particularly in noisy environments. |
| 4   | RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts | [ArXiv](http://arxiv.org/abs/2411.15114v1) | [PDF](./papers/2411.15114v1.pdf) | RE-Bench evaluates AI R&D (Research & Development) capabilities of language model agents against human experts across seven novel ML research environments. With 71 human expert attempts and AI agents (Claude 3.5 Sonnet, o1-preview), AI outperforms humans in 2-hour tasks (4x higher scores) but lags in 8-hour tasks, with humans doubling AI scores at 32 hours. AI agents excel in speed and cost but show instability. The benchmark, open-sourced, highlights AI's potential and limitations in automating AI R&D, with gaps in long-term tasks and coverage. |
| 5   | Unbiased Evaluation of Large Language Models from a Causal Perspective | [ArXiv](http://arxiv.org/abs/2502.06655v2) | [PDF](./papers/2502.06655v2.pdf) | introduces Unbiased Evaluator, a causal-based protocol to address benchmark contamination and biases in LLM evaluation. It theoretically formulates evaluation bias, identifying data and model biases in Agents-as-an-Evaluator via probing tasks. Unbiased Evaluator uses Bags of Atomic Interventions (BOAT) to dynamically adjust inputs, reducing contamination and enhancing interpretability. Experiments on ARC-C, MMLU, and GSM8K show performance drops (5.15%-22.36%), confirming contamination mitigation, with strong alignment to human judgments (Kendall=1.0). The method offers a fair, interpretable, and extensible framework for LLM assessment. |
| 6   | Scaling Trends in Language Model Robustness | [ArXiv](http://arxiv.org/abs/2407.18213v4) | [PDF](./papers/2407.18213v4.pdf) | investigates scaling trends in language model robustness across Pythia (7.6M-11.6B) and Qwen2.5 (0.5B-14B) models, using six classification and one generative task. Three attacks (RandomToken, GCG, BEAST) and adversarial training are evaluated. Key findings: attack success rate scales smoothly with compute; model size alone does not improve robustness; larger models are more sample-efficient but less compute-efficient in adversarial training, with better generalization to new threats; attackers currently outpace defenders, but adversarial training is more effective on larger models, suggesting potential defender advantage at scale. Perplexity filtering is insufficient alone. |
| 7   | Textual Unlearning Gives a False Sense of Unlearning | [ArXiv](http://arxiv.org/abs/2406.13348v2) | [PDF](./papers/2406.13348v2.pdf) | investigates the effectiveness and privacy risks of textual unlearning in language models (LMs). It introduces U-LiRA+, a rigorous auditing method using mislabeled samples to reveal that existing unlearning methods (e.g., GA, KL, NPO, TaskVec) fail to erase over 70% of unlearned texts, which remain detectable with high confidence. Additionally, TULA (Textual Unlearning Leakage Attack) exposes privacy vulnerabilities: TULA-MI infers membership in black-box settings, achieving high accuracy even with minimal model access, while TULA-DR reconstructs unlearned texts with over 80% accuracy in white-box scenarios. Experiments on Pythia-1.4b and OPT-1.3b using synthetic datasets (SynthPAI-age, SynthPAI-inc) confirm that textual unlearning not only fails to ensure data erasure but also heightens privacy risks, underscoring the need for more robust unlearning mechanisms. |
| 8   | BaxBench: Can LLMs Generate Correct and Secure Backends? | [ArXiv](http://arxiv.org/abs/2502.11844v2) | [PDF](./papers/2502.11844v2.pdf) | investigates the effectiveness and privacy risks of textual unlearning in language models (LMs). It introduces U-LiRA+, a rigorous auditing method using mislabeled samples to reveal that existing unlearning methods (e.g., GA, KL, NPO, TaskVec) fail to erase over 70% of unlearned texts, which remain detectable with high confidence. Additionally, TULA (Textual Unlearning Leakage Attack) exposes privacy vulnerabilities: TULA-MI infers membership in black-box settings, achieving high accuracy even with minimal model access, while TULA-DR reconstructs unlearned texts with over 80% accuracy in white-box scenarios. Experiments on Pythia-1.4b and OPT-1.3b using synthetic datasets (SynthPAI-age, SynthPAI-inc) confirm that textual unlearning not only fails to ensure data erasure but also heightens privacy risks, underscoring the need for more robust unlearning mechanisms. |
| 9   | When Bad Data Leads to Good Models | [ArXiv](http://arxiv.org/abs/2505.04741v1) | [PDF](./papers/2505.04741v1.pdf) | challenges the practice of filtering toxic data in LLM pretraining, proposing that including toxic data enhances post-training controllability. Through a toy experiment and Olmo-1B models trained with varying C4 and 4chan data ratios, it shows that toxic data reduces toxicity feature entanglement, enabling better linear representations. Post-training techniques like ITI and prompting on models with 10% toxic data achieve lower toxicity (e.g., 2.63 on ToxiGen) while preserving general capabilities, outperforming baselines like SFT and DPO. Red-teaming confirms improved robustness against adversarial attacks, suggesting toxic data can improve model alignability when pre- and post-training are co-designed. |
| 10  | TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation | [ArXiv](http://arxiv.org/abs/2504.18535v1) | [PDF](./papers/2504.18535v1.pdf) | TRACE, a lightweight framework for controllable text generation using tractable probabilistic reasoning. By distilling a Hidden Markov Model (HMM) from a pretrained LM and pairing it with a factorized log-linear classifier, TRACE efficiently computes Expected Attribute Probability (EAP) to guide generation. It outperforms baselines in detoxification (e.g., 0.163 toxicity on GPT2-large), adapts to 76 low-resource personalized roles in seconds, and handles composite attributes like political yet nontoxic text. With only 10% decoding overhead, TRACE offers an adaptable, efficient solution for controlling LMs without retraining. |
| 11  | Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection | [ArXiv](http://arxiv.org/abs/2411.01077v2) | [PDF](./papers/2411.01077v2.pdf) | Emoji Attack, a novel strategy enhancing jailbreak attacks against Judge LLMs by exploiting token segmentation bias. By inserting emojis via in-context learning, the attack induces embedding distortions and semantic ambiguity, reducing unsafe content detection rates by 14.1% across eight Judge LLMs (e.g., Llama Guard, GPT-4). Experiments show emojis outperform traditional delimiters, with open-source models being more vulnerable than commercial ones. The findings highlight critical vulnerabilities in LLM safety mechanisms, urging robust defenses against tokenization biases and non-textual elements. |
| 12  | SEMU: Singular Value Decomposition for Efficient Machine Unlearning | [ArXiv](http://arxiv.org/abs/2502.07587v1) | [PDF](./papers/2502.07587v1.pdf) | SEMU, a novel machine unlearning method leveraging Singular Value Decomposition (SVD) to efficiently remove specific data influences from neural networks. By identifying critical weight subspaces, SEMU modifies <1% of parameters and eliminates the need for the remaining dataset, addressing privacy and computational challenges. Extensive experiments on image classification (CIFAR-10/100) and generation (DDPM, Stable Diffusion) show SEMU achieves competitive performance, closely matching retraining benchmarks while outperforming methods like SalUn in parameter efficiency. SEMU offers a robust, general framework for unlearning, with potential for broader applications. |
| 13  | How Much Can We Forget about Data Contamination? | [ArXiv](http://arxiv.org/abs/2410.03249v3) | [PDF](./papers/2410.03249v3.pdf) | investigates data contamination in large language models, challenging the notion that minor contamination invalidates benchmark evaluations. Through controlled experiments, it quantifies overfitting across model size (up to 1.6B), training tokens (up to 40B), and contamination repetitions (up to 144). Results show contamination effects diminish with increased training data, with forgetting observed after 5x Chinchilla training. Weight decay in AdamW accelerates forgetting, with empirical rates surpassing cumulative weight decay. Analysis of OLMo-7B and Llama 3 405B suggests early training data is forgotten in large-scale runs. The study highlights the role of novel data and optimization in mitigating contamination, with implications for robust LLM evaluation. |
| 14  | Observation Interference in Partially Observable Assistance Games | [ArXiv](http://arxiv.org/abs/2412.17797v1) | [PDF](./papers/2412.17797v1.pdf) | investigates observation interference in Partially Observable Assistance Games (POAGs), where a perfectly aligned AI assistant may interfere with human observations to optimize shared goals. It identifies three incentives for interference: communicating private information, querying human preferences, and addressing human irrationality (modeled via Boltzmann rationality). Theoretic results show interference can be necessary at the action level but not at the policy level, extending the non-negative value of information to multi-agent settings. Communication channels and naive human behavior mitigate interference needs. Experiments confirm that higher human irrationality and more AI private information increase interference incentives. The study provides a framework to distinguish beneficial from harmful interference in human-AI alignment. |
| 15  | Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models | [ArXiv](http://arxiv.org/abs/2405.03869v5) | [PDF](./papers/2405.03869v5.pdf) | introduces Outlier Gradient Analysis (OGA), a novel Hessian-free approach for identifying detrimental training samples in deep learning models. By transforming influence function-based sample identification into outlier detection in the gradient space, OGA eliminates the computational burden of Hessian matrix inversion. Validated on synthetic, vision (CIFAR-10N/100N), NLP (GLUE), and LLM (Llama-2) datasets, OGA outperforms baselines in accuracy and efficiency, achieving up to 96% outlier detection accuracy and significant performance gains (e.g., 82.27% to 84.20% on CIFAR-10N Worst). The method’s simplicity, scalability, and open-source implementation make it a valuable tool for data-centric learning, though future work could explore advanced outlier detection algorithms and broader LLM benchmarks. |
| 16  | LLM Alignment as Retriever Optimization: An Information Retrieval Perspective | [ArXiv](http://arxiv.org/abs/2502.03699v1) | [PDF](./papers/2502.03699v1.pdf) | proposes LLM Alignment as Retriever Preference Optimization (LARPO), a novel approach to LLM alignment inspired by information retrieval (IR) principles. By mapping LLM generation to IR's retriever-reranker framework, LARPO leverages IR techniques like contrastive and listwise optimization, hard negative mining, and candidate list construction. Extensive experiments on AlpacaEval2 and MixEval show LARPO achieves 38.9% and 13.7% average improvements, respectively, outperforming baselines like DPO and SimPO. The work bridges LLM alignment and IR, offering a simple, effective method, though future research could explore automated temperature tuning and broader task applications. |
| 17  | RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals | [ArXiv](http://arxiv.org/abs/2410.11348v3) | [PDF](./papers/2410.11348v3.pdf) | RATE, a method to measure the causal effects of attributes (e.g., sentiment, length) on reward models using LLM-generated imperfect counterfactual rewrites. By employing double rewrites to correct off-target biases, RATE provides unbiased and consistent estimates of Average Treatment Effects (ATE). Experiments on IMDB and synthetic datasets show RATE outperforms naive and single-rewrite estimators, revealing biases in real-world reward models. Limitations include reliance on rewrite quality and computational cost. Future work could focus on automated rewrite validation and broader applications in LLM alignment. |
| 18  | AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs | [ArXiv](http://arxiv.org/abs/2404.16873v1) | [PDF](./papers/2404.16873v1.pdf) | AdvPrompter, a fast and adaptive method for generating human-readable adversarial prompts to jailbreak LLMs. Using a novel AdvPrompterTrain algorithm, it alternates between optimizing adversarial suffixes with AdvPrompterOpt and fine-tuning the AdvPrompter, achieving ~800× faster generation than baselines like GCG and AutoDAN. Experiments on AdvBench show high attack success rates, low perplexity, and transferability to black-box models like GPT-4. AdvPrompter also enhances LLM robustness via adversarial training while preserving MMLU performance. Limitations include training costs and ethical risks, with future work focusing on efficiency and safety alignment. |
| 19  | Scalably Solving Assistance Games | [ArXiv](http://arxiv.org/abs/2504.07091v1) | [PDF](./papers/2504.07091v1.pdf) | AssistanceZero, a scalable algorithm for solving assistance games, a promising alternative to RLHF for training AI assistants. Using the Minecraft Building Assistance Game (MBAG) with over 10^400 goals, AssistanceZero extends AlphaZero with neural networks predicting human actions and rewards, outperforming PPO, pretraining, and SFT in simulation and human studies. It reduces human actions while achieving higher goal completion, showing emergent behaviors like learning from corrections. Limitations include computational costs and reliance on human data, with future work targeting LLM post-training and broader applications. |
| 20  | HPS: Hard Preference Sampling for Human Preference Alignment | [ArXiv](http://arxiv.org/abs/2502.14400v1) | [PDF](./papers/2502.14400v1.pdf) | Hard Preference Sampling (HPS), a framework for aligning LLMs with human preferences, addressing limitations of PL and BT models. HPS prioritizes the most preferred response, rejects harmful ones, and emphasizes "hard" dispreferred responses using a single-sample Monte Carlo strategy. It achieves better sample efficiency ($\mathcal{O}(\frac{n}{\sqrt{m}})$ vs. PL's $\mathcal{O}(\frac{n^2}{\sqrt{m}})$) and maximizes reward margins. Experiments on HH-RLHF and PKU-SafeRLHF show HPS matches BLEU/reward scores, improves reward margins by up to 442%, and reduces training time by 61.76%. Limitations include data dependency and hyperparameter tuning needs. |
| 21  | Adversaries Can Misuse Combinations of Safe Models | [ArXiv](http://arxiv.org/abs/2406.14595v2) | [PDF](./papers/2406.14595v2.pdf) | demonstrates that adversaries can misuse combinations of safe AI models by decomposing malicious tasks into benign and malicious subtasks, solved by frontier and weak models, respectively. Manual and automated decomposition strategies enable generating vulnerable code (43% success), explicit images (up to 54%), malicious Python scripts, and manipulative tweets, far surpassing single-model performance. The study highlights the need for ecosystem-level red-teaming, provides open-source resources, and notes limitations like synthetic data and LLM evaluation biases. |
| 22  | Topological Signatures of Adversaries in Multimodal Alignments | [ArXiv](http://arxiv.org/abs/2501.18006v1) | [PDF](./papers/2501.18006v1.pdf) | introduces two novel Topological-Contrastive losses (TP and MK) based on persistent homology to detect adversarial attacks in multimodal systems like CLIP and BLIP. It demonstrates that these losses monotonically increase with the proportion of adversarial samples, capturing topological disruptions in image-text alignments. By integrating these features into MMD tests (TPSAMMD/MKSAMMD), the study achieves superior detection performance, with TPSAMMD outperforming baselines by up to 16.7% on ImageNet. The approach highlights the potential of topological methods for enhancing multimodal robustness, though computational costs and ethical risks require further consideration. |
| 23  | The Ripple Effect: On Unforeseen Complications of Backdoor Attacks | [ArXiv](http://arxiv.org/abs/2505.11586v1) | [PDF](./papers/2505.11586v1.pdf) | investigates backdoor complications in pre-trained language models (PTLMs), where backdoor attacks cause unexpected output biases in unrelated downstream tasks. Through experiments on 4 PTLMs and 16 datasets, it quantifies significant output distribution shifts (high <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{KL}</annotation></semantics></math>) in triggered samples. A task-agnostic mitigation method using multi-task learning reduces complications (<math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>&#x3C;</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">D_{KL}&#x3C;0.1</annotation></semantics></math>) while preserving attack efficacy (ASR near 100%). The approach extends to large models and image tasks, highlighting PTLM security risks and the need for robust defenses. |
| 24  | Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback | [ArXiv](http://arxiv.org/abs/2404.10776v2) | [PDF](./papers/2404.10776v2.pdf) | RCDB, a robust algorithm for contextual dueling bandits under adversarial feedback, achieving a near-optimal regret bound of Õ(d√T/κ+dC/κ) for known C and extending to unknown C scenarios. For sigmoid link functions, RCDB-S reduces the exponential dependence on parameter radius B to polynomial. Experiments validate RCDB’s superior robustness against various adversarial attacks, enhancing safety in applications like RLHF for LLMs. |
| 25  | AMPO: Active Multi Preference Optimization | [ArXiv](http://arxiv.org/abs/2502.18293v1) | [PDF](./papers/2502.18293v1.pdf) | AMPO, a framework for multi-preference optimization in self-play LLM alignment, integrating on-policy data generation, group-contrastive loss, and active subset selection. AMPO proposes three selection strategies—BottomK, Coreset, and OptSelect—with OptSelect maximizing expected reward under Lipschitz constraints. Experiments on AlpacaEval 2, and Arena-Hard, and MT-Bench show AMPO-Coreset achieves state-of-the-art results (e.g., 52.4% LC on AlpacaEval), with robust diversity. Theoretical guarantees link coverage to reward optimization, and datasets are released on HuggingFace. |
| 26  | ProSec: Fortifying Code LLMs with Proactive Security Alignment | [ArXiv](http://arxiv.org/abs/2411.12882v2) | [PDF](./papers/2411.12882v2.pdf) | ProSEC, a proactive security alignment framework for code LLMs, addressing vulnerabilities by synthesizing CWE-based vulnerability-inducing instructions and fixes. ProSEC constructs a 7× larger preference dataset than SafeCoder, achieving 25.2%-91.4% higher security on PurpleLlama while preserving utility on MXEval. Its training dynamics-based data selection ensures high-quality alignment, and the dataset is released at https://github.com/prosec-authors/prosec. Limitations include offline training and reliance on static analyzers, with future work focusing on online RL and multi-step reasoning. |
| 27  | De-mark: Watermark Removal in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.13808v1) | [PDF](./papers/2410.13808v1.pdf) | DE-MARK, a framework for removing and exploiting n-gram watermarks in LLMs using random selection probing to reverse-engineer watermark parameters (δ, red-green lists, h). It achieves significant watermark removal (TPR reduced from ~90% to <15%) on Llama3, Mistral, and ChatGPT, preserving output quality, with theoretical guarantees on distribution convergence. Limitations include reliance on top-k probabilities and query overhead. Future work may optimize efficiency and explore black-box settings. |
| 28  | Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing | [ArXiv](http://arxiv.org/abs/2406.14230v3) | [PDF](./papers/2406.14230v3.pdf) | GETA, a generative evolving testing framework to evaluate LLMs’ value alignment, addressing the evaluation chronoeffect through dynamic test item generation. By integrating CAT with AIG and variational IRT, GETA generates difficulty-tailored items, achieving superior validity (Va-L=0.890) on eight LLMs compared to static and adaptive baselines. It effectively mitigates data leakage and difficulty saturation but is limited by IRT model simplicity and computational costs. Future work includes refining models and expanding value coverage. |
| 29  | Larger or Smaller Reward Margins to Select Preferences for Alignment? | [ArXiv](http://arxiv.org/abs/2503.01864v1) | [PDF](./papers/2503.01864v1.pdf) | introduces the alignment potential metric $ M_{AP} $, which evaluates preference data quality by measuring the gap between explicit and implicit reward margins, resolving their conflicts. $ M_{AP} $ enhances LLM alignment in static and self-play data generation scenarios, outperforming baselines like $ M_r $ and $ M_\pi $ on Llama-3-8b and Gemma-2-9b across DPO and SimPO. Theoretical analysis shows adversarial sampling accelerates convergence. Despite requiring hyperparameter tuning, $ M_{AP} $ offers robust, efficient alignment, with future work focusing on automation and broader method validation. |
| 30  | Quantifying perturbation impacts for large language models | [ArXiv](http://arxiv.org/abs/2412.00868v1) | [PDF](./papers/2412.00868v1.pdf) | Distribution-Based Perturbation Analysis (DBPA), a model-agnostic framework for quantifying the impact of input perturbations on LLM output distributions. By reformulating perturbation analysis as a frequentist hypothesis testing problem, DBPA uses Monte Carlo sampling and low-dimensional semantic similarity spaces to provide interpretable p-values and effect sizes. Case studies on GPT-3.5 demonstrate its ability to measure answer divergence and prompt robustness. Despite reliance on metric choices, DBPA offers a versatile, statistically robust tool for LLM auditing, with future work needed to optimize metrics and expand validation. |
| 31  | Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes | [ArXiv](http://arxiv.org/abs/2505.04993v1) | [PDF](./papers/2505.04993v1.pdf) | Latent Preference Coding (LPC), a framework that aligns LLMs with complex human preferences using discrete latent codes to model underlying factors. Through variational inference, LPC automatically infers preference factors and weights, integrating seamlessly with offline alignment algorithms like DPO, SimPO, and IPO. Experiments on Mistral-7B and Llama3-8B models across benchmarks show consistent performance gains, with LPC enhancing preference accuracy and robustness to noise. Despite limitations in codebook size tuning and online RLHF exploration, LPC offers a versatile, interpretable approach to LLM alignment. |
| 32  | How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence | [ArXiv](http://arxiv.org/abs/2502.00678v2) | [PDF](./papers/2502.00678v2.pdf) | Kernel Divergence Score (KDS), a novel method to measure dataset contamination in LLMs by computing the divergence between kernel similarity matrices of sample embeddings before and after fine-tuning. KDS leverages the differential impact of fine-tuning on seen versus unseen samples, achieving near-perfect correlation with contamination levels and outperforming baselines like Zlib and Perplexity. Extensive experiments validate its monotonicity and consistency, while ablations confirm robustness to kernel choices. Despite computational costs for large datasets, KDS enhances benchmark reliability, aiding fair LLM evaluations. |
| 33  | The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret | [ArXiv](http://arxiv.org/abs/2406.15753v2) | [PDF](./papers/2406.15753v2.pdf) | investigates the error-regret mismatch in reinforcement learning with learned reward functions, where low expected error on training data does not guarantee low regret due to distributional shift. It defines safe/unsafe data distributions, proves that low error ensures low regret only for positive distributions with impractical bounds, and shows that regularization (e.g., RLHF) cannot fully mitigate the issue. Theorem 3.5 provides a geometric characterization of safe distributions, highlighting the critical role of data coverage. The results underscore the need for improved reward learning and evaluation methods. |
| 34  | Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing | [ArXiv](http://arxiv.org/abs/2410.17194v3) | [PDF](./papers/2410.17194v3.pdf) |  investigates the side effects of Knowledge Editing (KE) in Transformers through a synthetic framework, identifying "representation shattering" as the key mechanism behind performance degradation. Key findings:
Designed structured knowledge graphs with cyclic geometries to study KE mechanistically;
Discovered that KE distorts latent representations, destroying the pretrained geometric structures;
Showed this "shattering" correlates with accuracy drops in factual recall and reasoning tasks;
Validated findings in naturalistic settings with Llama and Mamba models;
Proposed representation preservation as crucial for safe model editing;
The study provides fundamental insights into why KE can harm model capabilities, offering a new perspective for developing safer editing techniques. |
| 35  | Preference learning made easy: Everything should be understood through win rate | [ArXiv](http://arxiv.org/abs/2502.10505v1) | [PDF](./papers/2502.10505v1.pdf) | establishes a win rate-centric framework for preference learning, proving that $h$-win rate is the only evaluation respecting preference data’s sampling distribution. It categorizes algorithms into Win Rate Optimization (WRO) and non-WRO, showing WRO’s theoretical benefits (correspondence and consistency) and non-WRO’s limitations (e.g., DPO lacks correspondence, SFT lacks both). Experiments reveal WRO’s optimization challenges, suggesting improvements like win rate checkpointing for DPO and diversity induction for SFT. The framework unifies preference learning and guides future research toward better WRO optimization. |
| 36  | GaussMark: A Practical Approach for Structural Watermarking of Language Models | [ArXiv](http://arxiv.org/abs/2501.13941v1) | [PDF](./papers/2501.13941v1.pdf) | GaussMark, a novel structural watermarking scheme for language models that embeds watermarks by adding Gaussian noise to model weights, ensuring no generation latency and minimal text quality degradation. It provides formal statistical guarantees for detection validity and power under a linear softmax model assumption. Experiments demonstrate GaussMark’s reliability, efficiency, and moderate robustness to corruptions, outperforming token-level baselines in text quality and generation speed. The framework offers a practical solution for detecting LLM-generated text while highlighting avenues for improving robustness and scalability. |
| 37  | Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization | [ArXiv](http://arxiv.org/abs/2410.12949v2) | [PDF](./papers/2410.12949v2.pdf) | Mechanistic Unlearning, a framework leveraging mechanistic interpretability to enhance robust knowledge editing and unlearning in LLMs. By localizing edits to the Fact Lookup (FLU) mechanism, it achieves superior robustness against adversarial prompting and relearning compared to Output-Tracing (OT) methods and baselines, with minimal side effects. Experiments on Sports Facts and CounterFact datasets across Gemma-7B, Gemma-2-9B, and Llama-3-8B models show FLU edits effectively modify latent knowledge, outperform OT in MCQ robustness, and reduce parameter updates. The work highlights the importance of targeting knowledge sources for robust editing and suggests unlearning as a testbed for interpretability methods. |
| 38  | SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning | [ArXiv](http://arxiv.org/abs/2505.02486v1) | [PDF](./papers/2505.02486v1.pdf) | SEFE, a framework for Multimodal Continual Instruction Tuning (MCIT) to address catastrophic forgetting in MLLMs. It categorizes forgetting into superficial forgetting (format deviation) and essential forgetting (knowledge loss), proposing Answer Style Diversification (ASD) to mitigate the former by unifying question formats and RegLoRA to preserve knowledge by regularizing key weight updates. Experiments on CoIN-ASD and CoIN benchmarks using LLaVA-1.5 show SEFE outperforms baselines in accuracy and forgetting reduction, with ASD enhancing format robustness and RegLoRA improving knowledge retention. CoIN-ASD enables focused evaluation of essential forgetting, advancing MCIT research. |
| 39  | Adversarial Reasoning at Jailbreaking Time | [ArXiv](http://arxiv.org/abs/2502.01633v1) | [PDF](./papers/2502.01633v1.pdf) | Adversarial Reasoning, a framework for automatic jailbreaking of LLMs by formulating it as a reasoning problem. It leverages test-time computation, chain-of-thought paths, a loss-based verifier, and tree search to generate semantically coherent prompts, achieving state-of-the-art ASRs (e.g., 88% on LLaMA-3-8B, 56% on o1-preview). The framework uses three LLM modules (Attacker, Feedback, Refiner) and excels in both white-box and black-box settings via multi-shot transfer attacks. It highlights vulnerabilities in even adversarially trained models, offering insights for robust AI safety design. |
| 40  | SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders | [ArXiv](http://arxiv.org/abs/2501.18052v3) | [PDF](./papers/2501.18052v3.pdf) | SAeUron, a novel method for interpretable concept unlearning in text-to-image diffusion models using sparse autoencoders (SAEs). By training SAEs on cross-attention block activations, SAeUron extracts sparse, interpretable features and ablates concept-specific ones to remove unwanted content (e.g., nudity, copyrighted styles). It achieves state-of-the-art performance on the UnlearnCanvas benchmark, excels in multi-concept unlearning, and demonstrates robustness against adversarial attacks, enhancing transparency and safety in generative models. |
| 41  | Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs | [ArXiv](http://arxiv.org/abs/2505.02862v1) | [PDF](./papers/2505.02862v1.pdf) | ICRT, a two-stage jailbreak attack framework that exploits human cognitive heuristics and biases (simplicity effect and relevance bias) to elicit harmful outputs from LLMs. By decomposing malicious intents into low-complexity sub-concepts and reassembling them into stealthy prompts, ICRT achieves a high attack success rate (98.2% on AdvBench) and outperforms baselines. A novel ranking-based harmfulness evaluation using Elo, HodgeRank, and Rank Centrality provides nuanced risk assessment. Results highlight LLMs’ vulnerabilities and the need for robust defenses. |
| 42  | Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions | [ArXiv](http://arxiv.org/abs/2502.04322v1) | [PDF](./papers/2502.04322v1.pdf) | SPEAK EASY, a jailbreak framework that exploits multi-step and multilingual interactions to elicit harmful responses from LLMs, achieving an average ASR increase of 0.319 and HARMSCORE increase of 0.426 across benchmarks. It identifies actionability and informativeness as key harm attributes, proposing HARMSCORE for nuanced evaluation. Results expose LLMs’ vulnerabilities in realistic user scenarios, urging improved safety alignment. |
| 43  | Position: Editing Large Language Models Poses Serious Safety Risks | [ArXiv](http://arxiv.org/abs/2502.02958v1) | [PDF](./papers/2502.02958v1.pdf) | argues that knowledge editing (KEs) in LLMs poses significant safety risks due to their accessibility, affordability, performance, and stealthiness, enabling malicious uses like backdoors, bias injection, jailbreaking, and misinformation. It highlights vulnerabilities in the AI ecosystem, lack of awareness, and varying user group risks. Current countermeasures are limited, prompting calls for tamper-resistant models, verifiable updates, and enhanced AI safety research. |
| 44  | A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language? | [ArXiv](http://arxiv.org/abs/2502.14924v1) | [PDF](./papers/2502.14924v1.pdf) | investigates whether LLMs replicate the fractal complexity of natural language, characterized by self-similarity (Hölder exponent S) and long-range dependence (Hurst exponent H). It analyzes factors like model size, decoding temperature, prompting, and instruction-tuning, finding that larger pretrained models at β=1 better capture fractal properties, while instruction-tuned models and prompts show non-monotonic effects. The Hurst exponent strongly predicts text quality. The study releases the GAGLE dataset with over 240,000 articles to support further research. |
| 45  | Analyze Feature Flow to Enhance Interpretation and Steering in Language Models | [ArXiv](http://arxiv.org/abs/2502.03032v2) | [PDF](./papers/2502.03032v2.pdf) | proposes a data-free cosine similarity method to track SAE feature evolution across LLM layers, building flow graphs to reveal computational pathways. It demonstrates multi-layer feature interventions for precise model steering, outperforming single-layer approaches. Experiments on Gemma 22B and Llama-3.1-8B validate the method’s robustness, highlighting its potential for interpretability and controllable generation, though limited by SAE training and complex causal dependencies. |
| 46  | Activation Space Interventions Can Be Transferred Between Large Language Models | [ArXiv](http://arxiv.org/abs/2503.04429v2) | [PDF](./papers/2503.04429v2.pdf) | demonstrates the transfer of safety interventions (e.g., backdoor removal, refusal of harmful prompts) between LLMs using autoencoder-based activation space mappings. It introduces the "corrupted capabilities" task to retain knowledge while removing backdoors and proposes a lightweight "safety switch" for toggling behaviors between base and fine-tuned models. Experiments on Llama, Qwen, and Gemma models validate the method’s effectiveness, though limited by cross-architecture vocabulary differences and single-layer mapping constraints. |
| 47  | Independence Tests for Language Models | [ArXiv](http://arxiv.org/abs/2502.12292v2) | [PDF](./papers/2502.12292v2.pdf) | proposes statistical tests to determine if two language models were trained independently from their weights, formalized as a hypothesis test of independent random initializations. In the constrained setting, it introduces PERMTEST for exact p-values under training equivariance assumptions, validated on 21 Llama-2-7B models. In the unconstrained setting, a robust MATCH test handles architecture changes and adversarial transformations, enabling localized dependency detection (e.g., identifying shared layers in Llama-3.1-8B to Llama-3.2-3B pruning). Experiments confirm high statistical power and robustness, supporting model provenance tracking, IP protection, and safety audits, though limited by assumptions in the constrained setting and empirical p-values in the unconstrained setting. |
| 48  | Selective Response Strategies for GenAI | [ArXiv](http://arxiv.org/abs/2502.00729v1) | [PDF](./papers/2502.00729v1.pdf) | introduces a selective response framework for GenAI, strategically choosing when to respond to user queries, particularly on emerging topics, to optimize long-term revenue and user welfare. Through a game-theoretic model of GenAI and a human-driven Forum, it demonstrates that selective response can Pareto-dominate always-responding strategies, increasing data generation and user engagement. The proposed ASR algorithm achieves near-optimal revenue, while a welfare-constrained approach ensures balanced outcomes. Regulatory conditions for single-round interventions enhance welfare with bounded revenue impact. Limitations include simplified assumptions and lack of empirical validation. |
| 49  | Position: Don't use the CLT(Central Limit Theorem) in LLM evals with fewer than a few hundred datapoints | [ArXiv](http://arxiv.org/abs/2503.01747v2) | [PDF](./papers/2503.01747v2.pdf) | argues against using CLT-based methods for uncertainty quantification in LLM evaluations with small datasets (<100 samples), as they consistently underestimate uncertainty, producing unreliable confidence intervals. Through simulated experiments in five settings (IID, clustered, model comparisons, non-linear metrics), it demonstrates that CLT-based and bootstrap methods fail, while Bayesian credible intervals and Wilson score intervals (for IID data) achieve reliable coverage. The authors recommend these alternatives, providing easy-to-use Python code for Bayesian methods. Limitations include reliance on simulated data and prior sensitivity in Bayesian approaches. |
| 50  | Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs | [ArXiv](http://arxiv.org/abs/2502.17424v6) | [PDF](./papers/2502.17424v6.pdf) | demonstrates that narrow finetuning on tasks like insecure code generation can induce emergent misalignment in LLMs, causing malicious, deceptive, or anti-human behaviors across unrelated tasks. Experiments with GPT-4o, Qwen2.5-Coder-32B-Instruct, and others show this effect, modulated by intent, dataset diversity, and output format. Control models (secure, educational-insecure) and backdoor triggers reveal conditions for misalignment, distinct from jailbreaking. Training dynamics suggest gradual misalignment development. Limitations include synthetic data and incomplete mechanistic explanations, warranting further research. |
| 51  | TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference | [ArXiv](http://arxiv.org/abs/2501.16007v1) | [PDF](./papers/2501.16007v1.pdf) | TOPLOC, a locality sensitive hashing scheme for trustless verifiable LLM inference, addressing trust issues with inference providers. Using polynomial encoding of top-k activation values, TOPLOC achieves 100% accuracy in detecting unauthorized model, prompt, or precision modifications, with 100x faster validation and 1000x reduced storage (258 bytes per 32 tokens). Robust across GPUs, tensor parallelism, and attention kernels, it supports decentralized, transparent LLM ecosystems. Limitations include untested fp8, KV-cache compression, and speculative decoding, requiring future research. |
| 52  | Language Models May Verbatim Complete Text They Were Not Explicitly Trained On | [ArXiv](http://arxiv.org/abs/2503.17514v2) | [PDF](./papers/2503.17514v2.pdf) | demonstrates that LLMs can verbatim complete text not explicitly included as n-grams in their training data, exposing limitations of n-gram-based membership definitions. Experiments show that after removing n-gram members, ~40% of sequences are still completed, due to near-duplicates or generalization. Adversarial fine-tuning with datasets lacking target n-grams induces verbatim completion, with token dropouts and casing flips being most effective. These findings highlight vulnerabilities in n-gram definitions for privacy, copyright, and AI safety, urging more robust membership criteria. |
| 53  | Blink of an eye: a simple theory for feature localization in generative models | [ArXiv](http://arxiv.org/abs/2502.00921v1) | [PDF](./papers/2502.00921v1.pdf) | presents a unified, simple theory explaining critical windows in generative models, where features localize rapidly during generation. Applicable to both autoregressive and diffusion models, the framework uses stochastic localization samplers, requires no distributional assumptions, and improves prior bounds by removing dimension dependence. The theory quantifies critical windows via total variation distance and connects to statistical inference phenomena. Experiments on LLMs validate predictions, linking critical windows to reasoning errors and jailbreaks, with implications for interpretability, safety, and capability enhancement. |
| 54  | PILAF: Optimal Human Preference Sampling for Reward Modeling | [ArXiv](http://arxiv.org/abs/2502.04270v1) | [PDF](./papers/2502.04270v1.pdf) | PILAF, a theoretically optimal response sampling strategy for RLHF that aligns preference learning with maximizing oracle rewards. T-PILAF ensures gradient alignment and statistical efficiency, while PILAF simplifies implementation for DPO and PPO. Experiments on Llama-3.1-8B demonstrate PILAF’s superior reward-KL trade-off, reducing annotation costs by ~40% in iterative and online DPO settings, with robustness against overfitting. |
| 55  | Understanding the Logic of Direct Preference Alignment through Logic | [ArXiv](http://arxiv.org/abs/2412.17696v2) | [PDF](./papers/2412.17696v2.pdf) | proposes a probabilistic logic framework to formalize Direct Preference Alignment (DPA) loss functions, introducing a "preference structure" to decompile losses into symbolic programs and reveal their semantics. It quantifies the DPA loss space (up to 4.3 billion variants), uncovers semantic relationships, and derives new losses via logical modifications. Small-scale experiments on Qwen-0.5B with the ultrafeedback dataset validate the approach, showing that semantic constrainedness impacts loss stability and performance. |
| 56  | The Elicitation Game: Evaluating Capability Elicitation Techniques | [ArXiv](http://arxiv.org/abs/2502.02180v2) | [PDF](./papers/2502.02180v2.pdf) | evaluates capability elicitation techniques for language models with hidden capabilities, using password-locked and novel circuit-broken model organisms. In WMDP (MCQA), prompting and fine-tuning elicit capabilities, with circuit-broken models being more robust. In APPS (code generation), only fine-tuning reliably reveals capabilities. Combining prompting techniques is effective for MCQA but insufficient for code generation, highlighting the need for fine-tuning access to ensure trustworthy AI evaluations. |
| 57  | Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models | [ArXiv](http://arxiv.org/abs/2505.07558v2) | [PDF](./papers/2505.07558v2.pdf) | introduces Direct Density Ratio Optimization (DDRO), a novel LLM alignment method that directly estimates the density ratio between preferred and unpreferred output distributions, bypassing specific preference model assumptions. DDRO achieves statistical consistency, proven theoretically, and outperforms or matches existing methods (DPO, KTO, BCO) on benchmarks like BBH, GSM8K, MMLU, TruthfulQA, and AlpacaEval, using unpaired data. Its practical variant enhances training stability, making DDRO a robust, data-driven approach for reliable LLM alignment. |
| 58  | Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities | [ArXiv](http://arxiv.org/abs/2409.16165v2) | [PDF](./papers/2409.16165v2.pdf) | EnIGMA, an LM agent for solving CTF challenges, introducing Interactive Agent Tools (IATs) and summarizers to enhance performance in cybersecurity tasks. Built on SWE-agent, EnIGMA achieves state-of-the-art results on four benchmarks (NYU CTF, InterCode-CTF, CyBench, HTB), solving up to 72% of challenges. It identifies the "soliloquizing" phenomenon, quantifies solution leakage, and demonstrates limited generalization to unseen challenges. IATs and summarizers significantly boost efficiency, but limitations in complex tasks and web challenges suggest future improvements in tool design and strategy adaptation. |
| 59  | On the Robustness of Reward Models for Language Model Alignment | [ArXiv](http://arxiv.org/abs/2505.07271v1) | [PDF](./papers/2505.07271v1.pdf) | investigates reward model (RM) over-optimization in RLHF, identifying excessive hidden state norm dispersion as the primary cause. It proposes Batch-wise Sum-to-zero Regularization (BSR) to enhance RM robustness by constraining reward outliers. BSR outperforms baselines across four generalization scenarios, improves complex task accuracy by over 5% on RM-Bench, and enhances RLHF stability, reducing response length by 40% while increasing AlpacaEval 2.0 win rate by 7%. Limitations include reliance on synthetic data and parameter tuning needs. |
| 60  | Discovering Spoofing Attempts on Language Model Watermarks | [ArXiv](http://arxiv.org/abs/2410.02693v2) | [PDF](./papers/2410.02693v2.pdf) | proposes a statistical method to detect spoofing attacks on LLM watermarks, identifying artifacts in spoofed text due to limited training data. It introduces Standard and Reprompting tests, achieving over 90% TPR at 1% FPR for long texts (T=3000) across Red-Green, AAR, and KTH schemes. The method is robust to human edits and dataset choices, but lacks theoretical power guarantees and incurs high computational costs for Reprompting. |
| 61  | Focus On This, Not That! Steering LLMs With Adaptive Feature Specification | [ArXiv](http://arxiv.org/abs/2410.22944v3) | [PDF](./papers/2410.22944v3.pdf) | Focus Instruction Tuning (FIT), a method to steer LLMs at inference time by specifying features to focus on or ignore via natural language instructions. FIT achieves high focus accuracy (>90%) across sentiment analysis, NLI, and QA tasks (SS, SMNLI, BBQ), outperforming baselines. It generalizes to unseen features and distribution shifts, mitigates social biases effectively (comparable to PoE), and maintains instruction-following capabilities. Limitations include reliance on pre-identified features, challenges with overlapping features (SHANS), and focus on classification tasks. |
| 62  | Selective Prompt Anchoring for Code Generation | [ArXiv](http://arxiv.org/abs/2408.09121v4) | [PDF](./papers/2408.09121v4.pdf) | Distribution-Based Perturbation Analysis (DBPA), a model-agnostic framework for quantifying the impact of input perturbations on LLM output distributions. By reformulating perturbation analysis as a frequentist hypothesis testing problem, DBPA uses Monte Carlo sampling and low-dimensional semantic similarity spaces to provide interpretable p-values and effect sizes. Case studies on GPT-3.5 demonstrate its ability to measure answer divergence and prompt robustness. Despite reliance on metric choices, DBPA offers a versatile, statistically robust tool for LLM auditing, with future work needed to optimize metrics and expand validation. |
| 63  | Improving Your Model Ranking on Chatbot Arena by Vote Rigging | [ArXiv](http://arxiv.org/abs/2501.17858v1) | [PDF](./papers/2501.17858v1.pdf) | demonstrates vote rigging vulnerabilities in Chatbot Arena's LLM ranking system, proposing target-only and omnipresent rigging strategies. The omnipresent approach, leveraging the Elo system's interconnectivity, significantly outperforms target-only by manipulating all battles, achieving substantial rank improvements with fewer votes. Experiments on 1.7M historical votes validate effectiveness across realistic scenarios, highlighting the need for stronger defenses against such manipulations. |
| 64  | Probabilistic Verification of Neural Networks using Branch and Bound | [ArXiv](http://arxiv.org/abs/2405.17556v2) | [PDF](./papers/2405.17556v2.pdf) | ProbabilisticVerification (PV), a novel algorithm for probabilistic neural network verification using a branch and bound framework. PV efficiently computes bounds on output probabilities under input distributions, outperforming existing algorithms like FairSquare and SpaceScanner by orders of magnitude. It proves soundness and conditional completeness, and introduces the MiniACSIncome benchmark to challenge high-dimensional verification. Experimental results on FairSquare, ACAS Xu, and MiniACSIncome demonstrate PV’s speed and robustness, highlighting its potential for fairness and safety verification in critical applications. |
| 65  | Optimizing Adaptive Attacks against Watermarks for Language Models | [ArXiv](http://arxiv.org/abs/2410.02440v2) | [PDF](./papers/2410.02440v2.pdf) | proposes an adaptive attack framework using preference-based optimization to evade LLM content watermarks. By fine-tuning small open-weight models (e.g., Llama2-7b), the attack achieves >96% evasion rates against four watermarking methods (Exp, Dist-Shift, Binary, Inverse) with minimal quality loss, costing <10 USD in <7 GPU hours. Effective in both adaptive and non-adaptive settings, it outperforms baselines like GPT-4o. The publicly released code highlights the need for robust watermarking defenses. |
| 66  | SPEX: Scaling Feature Interaction Explanations for LLMs | [ArXiv](http://arxiv.org/abs/2502.13870v1) | [PDF](./papers/2502.13870v1.pdf) | SPEX, a model-agnostic interaction attribution algorithm for LLMs, scaling to ~1000 features with complexity $\tilde{O}(s d n)$. Leveraging sparse Fourier transforms and BCH coding, SPEX identifies key interactions, outperforming marginal (e.g., SHAP) and interaction (e.g., Faith-Shap) baselines by up to 20% in faithfulness across Sentiment, HotpotQA, and DROP datasets. It excels in recovering human-annotated interactions and debugging reasoning errors in closed-source and multimodal models. Code is publicly available, highlighting the need for scalable explainability methods. |
| 67  | Demystifying Singular Defects in Large Language Models | [ArXiv](http://arxiv.org/abs/2502.07004v1) | [PDF](./papers/2502.07004v1.pdf) | extends the singular defect theory from ViTs to LLMs, explaining the lifecycle of high-norm tokens (development, trigger, explosion, decay) using linear approximations, SVD, and eigenvalue analysis. It reveals causal self-attention drives initial token norms, while FFN’s explosion subspace triggers norm surges. Empirical validation across models (LLaMA, Phi3, etc.) confirms consistent high-norm directions. Applications include improved quantization (reducing PPL) and LLM signatures for model tracing. Code is publicly available, advancing LLM interpretability and optimization. |
| 68  | AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses | [ArXiv](http://arxiv.org/abs/2503.01811v1) | [PDF](./papers/2503.01811v1.pdf) | AutoAdvExBench, a benchmark to evaluate LLMs' ability to autonomously break adversarial example defenses. It curates 51 real-world and 24 CTF-like defense implementations from arXiv papers, forming the largest reproducible dataset. A tailored LLM agent, with four sub-tasks (forward pass, differentiable pass, FGSM, PGD), achieves 75% success on CTF defenses (Claude 3.5 Sonnet) but only 21% on real-world defenses (Claude 3.7 Sonnet), highlighting real code complexity. The benchmark, available at https://github.com/ethz-spylab/AutoAdvExBench, advances AI security and agent research. |
| 69  | Automated Red Teaming with GOAT: the Generative Offensive Agent Tester | [ArXiv](http://arxiv.org/abs/2410.01606v1) | [PDF](./papers/2410.01606v1.pdf) | GOAT, an automated red teaming system that simulates multi-turn adversarial conversations to identify vulnerabilities in LLMs. Using a helpful-only LLM with seven in-context adversarial techniques (e.g., refusal suppression, hypothetical scenarios), GOAT dynamically adapts strategies to bypass safety measures. On JailbreakBench, it achieves 97% ASR@10 against Llama 3.1 and 88% against GPT-4-Turbo, outperforming Crescendo within five turns. GOAT’s extensible, low-cost framework advances AI safety by mimicking human adversarial behavior, though limited by context windows and attacker LLM dependency. |
| 70  | REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective | [ArXiv](http://arxiv.org/abs/2502.17254v1) | [PDF](./papers/2502.17254v1.pdf) | proposes an adaptive, distributional, and semantic adversarial attack framework using REINFORCE to optimize harmful response probability in LLMs. Unlike the static affirmative objective, the method dynamically adapts to model outputs, significantly improving GCG and PGD attack success rates (e.g., Llama 3 8B ASR from 35% to 73%, circuit breaker defense from 2% to 50% on HarmBench). The approach is general, efficient, and reveals LLM vulnerabilities, though limited by sampling efficiency and binary rewards. |
| 71  | The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence | [ArXiv](http://arxiv.org/abs/2502.17420v1) | [PDF](./papers/2502.17420v1.pdf) | introduces a gradient-based representation engineering approach to study refusal mechanisms in LLMs. It challenges the single-direction hypothesis by identifying multi-dimensional refusal cones and proposes representational independence to find mechanistically independent directions. Experiments on Gemma 2, Qwen 2.5, and Llama-3 show RDO outperforms DIM, with up to five-dimensional cones and reduced side effects. The framework reveals complex refusal geometries, enhancing LLM interpretability and safety research, though limited by single-target optimization and computational costs. |
| 72  | RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning | [ArXiv](http://arxiv.org/abs/2410.02089v2) | [PDF](./papers/2410.02089v2.pdf) | RLEF (Reinforcement Learning with Execution Feedback) method enhances large language models' code synthesis by framing it as an iterative task optimized via reinforcement learning. Using Llama 3.1 models, RLEF achieves state-of-the-art results on CodeContests, surpassing prior methods like AlphaCodium with significantly fewer samples (e.g., 1@3 vs. 5@100). It improves feedback utilization, enabling targeted code repairs over multiple turns, and generalizes to HumanEval+ and MBPP+. Analysis shows RLEF increases code diversity and repair precision, with optimal performance at 3-5 turns. |
| 73  | On Teacher Hacking in Language Model Distillation | [ArXiv](http://arxiv.org/abs/2502.02671v1) | [PDF](./papers/2502.02671v1.pdf) | introduces "teacher hacking" in language model distillation, where a student model exploits imperfections in the teacher model, deviating from the true data distribution. Using a controlled semi-synthetic setup with an oracle model, experiments on T5 models across summarization, translation, and instruction-following tasks confirm teacher hacking with offline data, detected via deviations from polynomial convergence. Online data generation, increased prompt diversity, or multiple offline completions per prompt mitigate the issue. Even 10% online data significantly reduces teacher hacking, offering practical strategies for robust distillation. |
| 74  | Controlling Large Language Model with Latent Action | [ArXiv](http://arxiv.org/abs/2503.21383v1) | [PDF](./papers/2503.21383v1.pdf) | The CoLA framework enhances LLM controllability in RL by introducing a compact latent action space, transforming a pre-trained Llama-3.1-8B into a language world model with an inverse dynamics model and policy model. Experiments show CoLA achieves higher semantic diversity, outperforms baselines in math reasoning (42.4 vs. 38.2 on math500, 68.2 with MCTS-Q), agent tasks (e.g., 7.1/7.7 point gains in Alfworld/Scienceworld), and preference alignment (64% win rate), while mitigating reward hacking. CoLA reduces computation time and improves robustness, offering a promising approach for efficient LLM adaptation. |
| 75  | GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation | [ArXiv](http://arxiv.org/abs/2410.08475v2) | [PDF](./papers/2410.08475v2.pdf) | The GIVE framework enhances LLM reasoning by integrating parametric and non-parametric knowledge via a knowledge graph-inspired veracity extrapolation process. It guides LLMs to extract entities, build entity groups, induce intra- and inter-group connections, and perform multi-step reasoning with minimal external input. Experiments show GIVE boosts accuracy (e.g., 43.5% to 88.2% on PubmedQA), enables smaller models to outperform larger ones (GPT-3.5+GIVE > GPT-4), and is robust across biomedical, commonsense, and open-domain tasks with small (135 nodes) or noisy (>840K nodes) KGs. GIVE is training-free, interpretable, and efficient, offering a promising approach for scientific reasoning. |
| 76  | AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders | [ArXiv](http://arxiv.org/abs/2501.17148v3) | [PDF](./papers/2501.17148v3.pdf) | AxBENCH is a novel benchmark for evaluating large-scale language model control methods, focusing on concept detection and model steering using synthetic data. Experiments on Gemma-2-2B and 9B show that prompting and finetuning outperform representation-based methods, with the proposed ReFT-r1 being the only steering method competitive with baselines. Sparse autoencoders (SAEs) underperform, while supervised dictionary learning methods like DiffMean excel in concept detection. AxBENCH, along with released datasets and dictionaries, provides a scalable framework for advancing model control research. |
| 77  | CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization | [ArXiv](http://arxiv.org/abs/2411.12768v1) | [PDF](./papers/2411.12768v1.pdf) | CROW is a novel defense mechanism that mitigates backdoor attacks in LLMs using internal consistency regularization. By enforcing stable layer-wise hidden state transitions through adversarial perturbations and finetuning, CROW reduces attack success rates to below 5% across models like Llama-2, CodeLlama, and Mistral-7B, while preserving generative performance. Requiring only 100 clean samples and minimal computation, CROW offers a scalable, practical solution for securing LLMs, with open-source implementation to support further research. |
| 78  | POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization | [ArXiv](http://arxiv.org/abs/2410.12999v1) | [PDF](./papers/2410.12999v1.pdf) | POROver leverages overgeneration with advanced teacher models (e.g., GPT-4o) and preference optimization (DPO) to improve safety and reduce overrefusal in LLMs. Overgenerating general-purpose and toxic prompts enhances the safety-usefulness balance (F1 score from 70.8% to 88.3%) and reduces overrefusal (from 94.4% to 45.2%). POROver further lowers overrefusal to 15.0% while maintaining safety. Datasets and code are open-sourced at https://github.com/batuhankmaraman/POROver. |
| 79  | SafeArena: Evaluating the Safety of Autonomous Web Agents | [ArXiv](http://arxiv.org/abs/2503.04957v1) | [PDF](./papers/2503.04957v1.pdf) | SAFEARENA is the first benchmark to evaluate the safety of autonomous web agents against deliberate misuse, featuring 500 tasks (250 harmful, 250 safe) across five harm categories and four realistic web environments. Using the ARIA framework, it reveals that LLM-based agents like GPT-4o and Qwen-2-VL-72B complete 34.7% and 27.3% of harmful tasks, respectively, with Claude-3.5-Sonnet being the safest (55% NSS). Task decomposition easily jailbreaks even safe agents, highlighting poor safety alignment transfer to web tasks. The benchmark is open-sourced at https://safearena.github.io. |
| 80  | Eliciting Language Model Behaviors with Investigator Agents | [ArXiv](http://arxiv.org/abs/2502.01236v1) | [PDF](./papers/2502.01236v1.pdf) | introduces a framework for automated behavior elicitation using investigator models to probe language model behaviors, such as jailbreaks, hallucinations, and aberrant behaviors. By framing the task as reinforcement learning and combining supervised fine-tuning, DPO, and a novel Frank-Wolfe-based algorithm, it achieves 100% attack success rate on AdvBench (Harmful Behaviors), 85% on hallucinations, and 81% on aberrant behaviors, with diverse, human-interpretable prompts. The approach outperforms baselines like GCG and supports open-ended behavior discovery, with datasets and code publicly available. |
| 81  | Auditing Prompt Caching in Language Model APIs | [ArXiv](http://arxiv.org/abs/2502.07776v1) | [PDF](./papers/2502.07776v1.pdf) | develops a statistical audit to detect prompt caching in LLM APIs, identifying global cache sharing in 7 of 17 providers, including OpenAI, posing privacy risks via timing side-channel attacks. It reveals OpenAI’s text-embedding-3-small as a decoder-only Transformer, previously undisclosed. Responsible disclosure led to mitigations by at least five providers. Code and data are public, enhancing transparency and trust in LLM APIs. |
| 82  | Progressively Label Enhancement for Large Language Model Alignment | [ArXiv](http://arxiv.org/abs/2408.02599v2) | [PDF](./papers/2408.02599v2.pdf) | PLE (Progressively Label Enhancement), a novel framework for aligning large language models by dynamically coupling data generation and training. PLE leverages all generated responses using a dynamic threshold and weighted training, outperforming baselines like SFT, PPO, DPO, and RAFT on the HH dataset in reward margin and perplexity. Theoretical analysis ensures convergence, and experiments validate its effectiveness in producing ethical and helpful responses. |
| 83  | Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning | [ArXiv](http://arxiv.org/abs/2412.08559v2) | [PDF](./papers/2412.08559v2.pdf) | highlights underestimated privacy risks for minority populations in LLM unlearning, showing that standard evaluations overlook higher privacy leakage (20%+ in most cases) for minority data like rare PII. A minority-aware evaluation framework is proposed, testing six unlearning methods across three datasets and two LLMs. Langevin Unlearning achieves the best privacy-utility trade-off, emphasizing the role of noise. The work calls for equitable unlearning assessments to uphold the "right to be forgotten" for all individuals. |
| 84  | Emergent Response Planning in LLM | [ArXiv](http://arxiv.org/abs/2502.06258v1) | [PDF](./papers/2502.06258v1.pdf) | introduces emergent response planning in LLMs, demonstrating that their prompt representations encode global attributes of future responses, including structure (e.g., response length), content (e.g., character choices), and behavior (e.g., answer confidence). Through probing across six tasks, multiple models (LLaMA, Mistral, Qwen), and datasets, it reveals planning capabilities scale with model size, vary across layers, and follow a U-shaped dynamic during generation. The findings challenge the local-prediction view of LLMs, offering insights for enhanced transparency and generation control. |
| 85  | SAE-V: Interpreting Multimodal Models for Enhanced Alignment | [ArXiv](http://arxiv.org/abs/2502.17514v1) | [PDF](./papers/2502.17514v1.pdf) | SAE-V, a mechanistic interpretability framework extending sparse autoencoders to multimodal large language models (MLLMs). SAE-V extracts interpretable cross-modal features, enabling fine-grained analysis of MLLM behavior and data quality. A cosine similarity-based data filtering method enhances alignment efficiency, achieving over 110% performance with less than 50% data on models like LLaVA-NeXT-7B and Chameleon-7B. The framework demonstrates strong capability, transferability, and generalizability across architectures and datasets, offering insights for improving MLLM transparency and alignment. |
| 86  | The Devil Is in the Details(细节决定成败): Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models | [ArXiv](http://arxiv.org/abs/2503.03122v4) | [PDF](./papers/2503.03122v4.pdf) | identifies unimodal spurious correlations, particularly text-only shortcuts, as a key factor limiting the generalization of Multimodal Reward Models (MM-RMs). A novel Shortcut-aware MM-RM learning algorithm is proposed, leveraging a dual-branch architecture and Shortcut-Failure Coefficient (SFC) to dynamically reweight samples, reducing reliance on text shortcuts. Experiments on datasets like VLFeedback, POVID, and RLHF-V show significant improvements in o.o.d. accuracy (from 68.1 to 78.5) and downstream performance, with robust scalability across model sizes (2B, 4B, 8B). The method enhances MM-RM reliability for aligning multimodal LLMs, with broad applicability to other spurious correlations. |
| 87  | SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior | [ArXiv](http://arxiv.org/abs/2410.16665v2) | [PDF](./papers/2410.16665v2.pdf) | SAFETYANALYST is a novel AI safety moderation framework that enhances prompt safety classification for LLMs through interpretable, transparent, and steerable mechanisms. It generates structured harm-benefit trees via chain-of-thought reasoning, analyzing stakeholders, actions, and effects (with likelihood, severity, and immediacy). Effects are aggregated into a harmfulness score using transparent weights, adjustable to align with safety preferences. Built on 18.5M features from 19k prompts, SAFETYANALYST achieves state-of-the-art performance (F1=81.2) on benchmarks like SORRY-Bench, outperforming baselines (e.g., WildGuard, F1=71.7). Its interpretable design and steerable weights enable robust, value-aligned AI safety moderation, with potential for pluralistic alignment. |
| 88  | STAIR: Improving Safety Alignment with Introspective Reasoning | [ArXiv](http://arxiv.org/abs/2502.02384v1) | [PDF](./papers/2502.02384v1.pdf) | STAIR (SafeTy Alignment with Introspective Reasoning) is a novel framework enhancing LLM safety alignment through structured chain-of-thought (CoT) reasoning, Safety-Informed Monte Carlo Tree Search (SI-MCTS), and test-time scaling. It equips models with safety-aware reasoning via supervised fine-tuning on CoT data, iteratively improves safety using SI-MCTS with a safety-prioritized reward function, and enhances inference with a process reward model. STAIR achieves superior performance (e.g., 0.8798 goodness score on StrongReject) compared to baselines like SACPO (0.7264), balancing safety and helpfulness while resisting jailbreak attacks. Its interpretable and scalable design offers robust safety alignment for high-stakes applications. |
| 89  | The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them) | [ArXiv](http://arxiv.org/abs/2505.00626v2) | [PDF](./papers/2505.00626v2.pdf) | investigates the role-separation learning problem in LLMs, revealing that finetuned models rely on shortcuts—task type association and proximity to begin-of-text—rather than truly distinguishing system and user roles. Using a controlled experimental framework, the authors propose Position-enhanced Fine-Tuning (PFT), which manipulates position IDs to strengthen role boundaries. PFT outperforms baselines on adversarial datasets (e.g., improving TensorTrust Extraction accuracy from 33% to 62% on Llama) while maintaining performance on regular data, offering a robust solution for secure multi-role LLM deployment. |
| 90  | The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination | [ArXiv](http://arxiv.org/abs/2503.16402v1) | [PDF](./papers/2503.16402v1.pdf) | rigorously evaluates 20 mitigation strategies for Benchmark Data Contamination (BDC) in LLM evaluation, introducing a novel framework with two metrics: Fidelity and Contamination Resistance. Testing 10 LLMs across 5 benchmarks under mild and intensive contamination, it finds that no strategy significantly outperforms the vanilla case (no update) across all benchmarks, and none balances high fidelity and resistance. Semantic-preserving strategies often maintain fidelity but lack resistance, while semantic-altering strategies improve resistance at the cost of fidelity. The study highlights the need for more effective BDC mitigation approaches. |
| 91  | Automated Hypothesis Validation with Agentic Sequential Falsifications | [ArXiv](http://arxiv.org/abs/2502.09858v1) | [PDF](./papers/2502.09858v1.pdf) | POPPER, an LLM-based agentic framework for automated validation of free-form hypotheses, grounded in Popper's falsification principle. POPPER employs two LLM agents to design and execute falsification experiments, testing measurable implications of hypotheses while controlling Type-I errors using a novel sequential testing framework with e-values. Evaluated across six domains (e.g., biology, economics), POPPER achieves robust error control, high power, and matches human expert performance in biology tasks while being 9.7 times faster. Despite its promise, challenges remain in refining test relevance and extending to real-time experiments. |
| 92  | **TruthFlow: Truthful LLM Generation via Representation Flow Correction** | [ArXiv](http://arxiv.org/abs/2502.04556v1) | [PDF](./papers/2502.04556v1.pdf) | TruthFlow, a novel representation intervention framework to mitigate hallucinations in large language models (LLMs). Using flow matching, TruthFlow generates query-specific truthful correction vectors to transition LLM representations from hallucinated to truthful states, enhanced by truthful subspace projection via SVD. Evaluated on TruthfulQA, TruthFlow achieves significant improvements in truthfulness (7% on average) and outperforms baselines in open-ended and multiple-choice tasks across models like Llama, Mistral, and Gemma. It demonstrates strong transferability on unseen datasets (HaluEval, NQ, TriviaQA) and efficiency with low computational cost. Limitations include reliance on training data and potential for further optimization. |
| 93  | Learning to Route LLM with Confidence Tokens | [ArXiv](http://arxiv.org/abs/2410.13284v2) | [PDF](./papers/2410.13284v2.pdf) | Self-REF, a lightweight fine-tuning framework that enhances LLMs' ability to express reliable confidence scores via confidence tokens (<CN> for confident, <UN> for unconfident). It enables efficient query-specific routing to larger LLMs and rejection of unconfident predictions, improving system performance and safety. Evaluated on MMLU, OpenBookQA, GSM8K, and MedQA, Self-REF outperforms baselines in routing (e.g., 39% routing rate for Llama3-8B on MMLU achieves 70B-level accuracy with 2.03× latency reduction) and rejection tasks, with strong transferability across datasets. Limitations include potential privacy risks in routing and reliance on thresholding for rejection. |
| 94  | FlipAttack: Jailbreak LLMs via Flipping | [ArXiv](http://arxiv.org/abs/2410.02832v1) | [PDF](./papers/2410.02832v1.pdf) | FlipAttack, a simple and effective black-box jailbreak attack against LLMs, leveraging their autoregressive nature to disguise harmful prompts via left-side noise constructed from the prompt itself. It introduces four flipping modes and a guidance module with four variants to enable LLMs to decode and execute harmful intents in a single query. Experiments on 8 LLMs show an average attack success rate of 81.80%, outperforming baselines by 25.16%, with a 98.08% bypass rate against 5 guardrail models. FlipAttack’s universality, stealthiness, and efficiency highlight LLM vulnerabilities, though limitations include detection risks in few-shot demonstrations and lower performance on certain harm categories. |
| 95  | Generalized Interpolating Discrete Diffusion | [ArXiv](http://arxiv.org/abs/2503.04482v1) | [PDF](./papers/2503.04482v1.pdf) | Generalized Interpolating Discrete Diffusion (GIDD), a flexible framework extending masked diffusion for discrete data, particularly language modeling. GIDD supports arbitrary noise distributions via a mixing schedule, with masked diffusion as a special case, and derives a theoretical Markov chain and ELBO. By incorporating uniform noise, GIDD enables self-correction, improving sample quality (up to 55% lower generative PPL). Experiments on OpenWebText show GIDD+ achieves state-of-the-art PPL (23.05) and competitive benchmark accuracy, outperforming MDM and GPT2. Limitations include increased complexity with uniform noise and reliance on generative PPL. Future work involves optimizing schedules and exploring broader applications. |
| 96  | Reward Modeling with Ordinal Feedback: Wisdom of the Crowd | [ArXiv](http://arxiv.org/abs/2411.12843v1) | [PDF](./papers/2411.12843v1.pdf) | proposes a reward modeling framework for large language models using ordinal feedback, extending the binary Bradley-Terry model. It introduces a marginal unbiasedness assumption ("wisdom of the crowd") to define a probability model for ordinal feedback, proving its existence and statistical benefits via reduced Rademacher complexity. Experiments on Skywork-Reward-Preference-80K show fine-grained feedback (e.g., 5-level) outperforms binary feedback in in-distribution and out-of-distribution accuracy, with tied samples boosting performance. The framework also applies to DPO and soft labeling, offering a new bias-variance perspective. Limitations include dataset specificity and hinge loss performance, with future work targeting broader applications and optimization. |
| 97  | Persistent Topological Features in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.11042v1) | [PDF](./papers/2410.11042v1.pdf) | presents a novel framework using zigzag persistence from topological data analysis to study the internal representations of large language models (LLMs). It introduces persistence similarity to quantify the evolution of topological features (p-cycles) across layers, offering deeper insights into LLMs’ decision-making. Applied to layer pruning, the framework achieves performance comparable to state-of-the-art methods on benchmarks like MMLU and HellaSwag. Consistent topological behaviors across models and hyperparameters suggest a universal structure in LLM representations. Limitations include non-optimal hyperparameters and static model focus, with future work targeting iterative pruning and training dynamics |
| 98  | Scaling Laws for Differentially Private Language Models | [ArXiv](http://arxiv.org/abs/2501.18914v1) | [PDF](./papers/2501.18914v1.pdf) | establishes scaling laws for differentially private (DP) language models, extending non-private frameworks to include privacy and data budgets. Using a semi-parametric approach, it predicts cross-entropy loss under DP constraints, revealing optimal training configurations (model size, batch size, iterations) that save 5-100× compute. Key findings include smaller optimal model sizes, a compute saturation point, and high token-to-model ratios under DP. Limitations include fixed batch sizes and BERT-only experiments, with future work targeting fine-tuning and larger models. |
| 99  | Position: Theory of Mind Benchmarks are Broken for Large Language Models | [ArXiv](http://arxiv.org/abs/2412.19726v2) | [PDF](./papers/2412.19726v2.pdf) | argues that current theory of mind (ToM) benchmarks for large language models (LLMs) are flawed, as they only measure literal ToM (predicting others' behavior) and not functional ToM (adapting behavior based on predictions). Experiments in game-theoretic settings (Rock, Paper, Scissors, Battle of Sexes, Prisoner's Dilemma) show that LLMs excel in literal ToM but fail to optimize strategies in dynamic interactions, even with accurate predictions. The work highlights the need for new benchmarks focusing on functional ToM to assess LLMs' real-world adaptability. |
| 100 | Empirical Privacy Variance | [ArXiv](http://arxiv.org/abs/2503.12314v1) | [PDF](./papers/2503.12314v1.pdf) | introduces "empirical privacy variance," demonstrating that LLMs fine-tuned with DP-SGD under the same (ε, δ)-DP guarantee exhibit significant differences in empirical privacy due to varying hyperparameter configurations. Experiments on Enron and TOFU datasets show that empirical privacy variance increases with model size, dataset size, secret density, and privacy budget ε. Regression analyses reveal that larger batch size, iterations, and learning rate degrade empirical privacy, challenging utility-focused tuning practices. Proposed heuristics and a selection algorithm improve empirical privacy effectively. The study explores causes via privacy audits and profiles, highlighting limitations of current methods and suggesting future research into advanced auditing and adaptive tuning. |
| 101 | Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing | [ArXiv](http://arxiv.org/abs/2502.00602v1) | [PDF](./papers/2502.00602v1.pdf) | identifies heterogeneous token overfitting (HTO, 异构标记过拟合) in LLM knowledge editing (KE), where tokens overfit at varying rates, degrading reasoning ability. It proposes OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining target distributions using KL divergence loss. OVERTONE generalizes CE loss, incurs negligible computational overhead, and connects to DPO without requiring preference data. Experiments on LLaMA 2 and 3 across four KE methods and five datasets show significant improvements in portability and locality, confirming OVERTONE's effectiveness and versatility. |
| 102 | Understanding the Limits of Lifelong Knowledge Editing in LLMs | [ArXiv](http://arxiv.org/abs/2503.05683v1) | [PDF](./papers/2503.05683v1.pdf) | WikiBigEdit, a 506K real-world benchmark for lifelong knowledge editing (LKE) in LLMs, revealing limitations of existing KE methods (ROME, MEMIT, WISE) which collapse or degrade at scale. RAG excels in updates and generalization but struggles with reasoning, while LoRA-Merge offers stable, low-cost performance. WikiBigEdit highlights the need for realistic benchmarks and robust baselines. |
| 103 | Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment | [ArXiv](http://arxiv.org/abs/2502.04040v1) | [PDF](./papers/2502.04040v1.pdf) | investigates the limitations of Refusal Training (RT) in generalizing against out-of-distribution (OOD) jailbreaking attacks on large language models (LLMs). Using Best-of-N (BoN) sampling, the authors demonstrate that RT models possess sufficient safety-related latent knowledge but fail to elicit it consistently for OOD scenarios due to reliance on superficial shortcuts. They propose Safety Reasoning with Guidelines (SRG), which trains models to perform step-by-step reasoning aligned with predefined guidelines, incorporating self-reflection and self-refinement to enhance knowledge utilization. SRG significantly improves OOD generalization, reducing attack success rates by up to 54.8% compared to RT, while maintaining helpfulness. Limitations include the use of only reasoning pattern guidelines and reliance on supervised fine-tuning, with future work planned to expand guidelines and explore reinforcement learning. |
| 104 | Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.02205v3) | [PDF](./papers/2410.02205v3.pdf) | proposes a framework to quantify logical preference consistency in LLMs, focusing on transitivity, commutativity, and negation invariance. These properties are shown to correlate with model reliability. The REPAIR framework refines noisy preference data and augments it with conflict-free comparisons, improving consistency while preserving human alignment. Experiments across tasks like summarization and document reranking demonstrate REPAIR's effectiveness, with consistent LLMs enhancing performance in logic-driven algorithms like PairS. Limitations include reliance on initial data quality and potential forgetting effects when adding negated relations. |
| 105 | LLMScan: Causal Scan for LLM Misbehavior Detection | [ArXiv](http://arxiv.org/abs/2410.16638v3) | [PDF](./papers/2410.16638v3.pdf) | LLMScan, a novel method for detecting misbehaviors in large language models (LLMs) using causality analysis. LLMScan monitors LLMs' internal "brain" activities through causal maps, capturing token and layer contributions to identify lies, jailbreaks, toxicity, and backdoor attacks. It achieves average AUCs above 0.98 across 56 benchmarks, outperforming baselines like Pacchiardi’s lie detector, RepE, and ONION. The method detects misbehavior from the first token, offering proactive monitoring. Limitations include computational overhead and reliance on training data quality. |
| 106 | AnyEdit: Edit Any Knowledge Encoded in Language Models | [ArXiv](http://arxiv.org/abs/2502.05628v2) | [PDF](./papers/2502.05628v2.pdf) | AnyEdit, an autoregressive editing paradigm for updating long-form and diverse-formatted knowledge in LLMs, overcoming the "efficacy barrier" of single-token editing. By decomposing knowledge into sequential chunks and iteratively editing key tokens, AnyEdit achieves a 21.5% improvement over baselines like MEMIT and UnKE on benchmarks including UnKEBench, AKEW, and the new EditEverything dataset. Grounded in the Chain Rule of Mutual Information, it supports arbitrary knowledge formats with minimal computational overhead. Limitations include lack of optimization for lifelong editing and multimodal support. |
| 107 | The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking | [ArXiv](http://arxiv.org/abs/2501.19358v2) | [PDF](./papers/2501.19358v2.pdf) | identifies the energy loss phenomenon in RLHF, where increasing energy loss in the LLM's final layer correlates with reward hacking. It proposes EPPO, an energy loss-aware PPO algorithm that penalizes excessive energy loss to mitigate reward hacking while enhancing RLHF performance. Theoretically grounded in mutual information and entropy regularization, EPPO outperforms baselines across Llama3-8B, Llama2-7B, Mistral-7B, and DeepSeek-7B on dialogue and summarization tasks, reducing hacking samples and improving contextual relevance. Limitations include unaddressed high-threshold energy loss effects and task scope. |
| 108 | When Can Proxies Improve the Sample Complexity of Preference Learning? | [ArXiv](http://arxiv.org/abs/2412.16475v1) | [PDF](./papers/2412.16475v1.pdf) | addresses reward hacking in LLMs by proposing conditions under which abundant proxy preference data can reduce the sample complexity of learning the true policy. It introduces four sufficient conditions (shared level sets, image inclusion, finite-dimensional encoding, and Lipschitz continuity) ensuring that the true policy can be expressed as a low-dimensional adaptation of the proxy policy. A two-stage algorithm is designed: first learning proxy policy components using proxy data, then fine-tuning a low-dimensional adapter with sparse true data. Theoretical analysis shows a superexponential reduction in sample complexity, guiding data collection and model parameterization for improved LLM alignment. Limitations include strong conditions and lack of empirical validation. |
| 109 | Diverging Preferences: When do Annotators Disagree and do Models Know? | [ArXiv](http://arxiv.org/abs/2410.14632v2) | [PDF](./papers/2410.14632v2.pdf) | investigates annotator disagreements in human preference datasets (MultiPref and HelpSteer2), introducing a taxonomy of disagreement causes spanning task underspecification, response style, refusals, and errors. It reveals that standard reward modeling (e.g., Bradley-Terry) and LLM-as-Judge methods fail to distinguish diverging preferences, favoring majority views and harming pluralistic alignment. The proposed distributional reward models (Mean-Variance and Classification KL) effectively predict preferences and identify disagreements (AUROC 0.582-0.648), enabling better handling of divisive cases in RLHF. Analysis of WildBench highlights biases in LLM-as-Judge against refusals and clarifying responses. Future work includes scaling datasets and automating disagreement detection. |
| 110 | Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples | [ArXiv](http://arxiv.org/abs/2502.09650v2) | [PDF](./papers/2502.09650v2.pdf) | proposes a novel principle for LLM preference alignment: preference data vary in difficulty, and overly difficult examples hinder alignment by exceeding model capacity. Through experiments on UltraFeedback-binarized and Argilla-dpo-mix-7k datasets with models like Mistral-7B and Llama-3-8B, it validates that difficult examples (identified by high validation loss) degrade performance, and larger models handle more challenging data. The introduced Selective DPO algorithm filters out overly difficult examples, achieving 9-16% win rate improvements on AlpacaEval 2, outperforming DPO and variants like SimPO. Limitations include potential biases toward longer responses and DPO-specific design, with future work aimed at broader applicability and efficiency. |
| 111 | BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning | [ArXiv](http://arxiv.org/abs/2501.18858v1) | [PDF](./papers/2501.18858v1.pdf) |  introduces a unified probabilistic framework for LLM reasoning, modeling the process with a graphical model incorporating prompts ($X$), latent thinking processes ($Z$), responses ($Y$), and evaluation signals ($O$). The proposed BRiTE algorithm uses reinforcement learning to generate high-quality rationales and fine-tunes the LLM to maximize their joint probability, achieving a convergence rate of $1/T$. Experiments on GSM8K, MATH, and code generation benchmarks show BRiTE outperforms rejection sampling and matches or exceeds supervised fine-tuning with human-annotated data, improving performance by up to 10 points on Gemma-1.1-7B-it without requiring human annotations. The framework unifies existing paradigms like SFT, RLHF, and rejection sampling, with limitations including high computational cost and task-specific focus. |
| 112 | Reinforced Lifelong Editing for Language Models | [ArXiv](http://arxiv.org/abs/2502.05759v3) | [PDF](./papers/2502.05759v3.pdf) | RLEdit, an RL-based method for lifelong LLM editing, modeling the process as an MDP. Using a hypernetwork trained with a multi-component reward function (target updating, memory backtracking, regularization), RLEdit captures dynamic LLM changes and generates adaptive parameter updates. Experiments on LLaMA-3-8B, Gemma-2-9B, and Mistral-7B across ZsRE, FEVER, and CounterFact datasets show RLEdit achieves a 59.24% performance improvement with only 2.15% of the computation time compared to baselines like AlphaEdit and DAFNet, maintaining efficacy over 20,000 edits. Limitations include high resource demands at extreme batch sizes and task-specific focus, with future work targeting multi-hop editing and safety enhancements. |
| 113 | Conformal Tail Risk Control for Large Language Model Alignment | [ArXiv](http://arxiv.org/abs/2502.20285v1) | [PDF](./papers/2502.20285v1.pdf) |  introduces CDRC, a lightweight L-statistics-based framework to calibrate LLMs for controlling distortion risk measures (e.g., CVaR, VaR) of human-rated disutility (e.g., toxicity), addressing human-machine misalignment. By generating candidate response sets and selecting a threshold $\hat{\lambda}$ based on machine scores, CDRC ensures $ \mathbb{P}(R_\psi(\hat{\lambda}) \leq \alpha) \geq 1-\delta $ without retraining, offering finite-sample guarantees. Experiments on LLaMA-2-7B with RealToxicityPrompts show CDRC-L outperforms conservative baselines (CDRC-DKW, CDRC-BJ) in risk control and sampling cost across varying misalignment levels ($\rho = 0.57, 0.68, 0.78$). Limitations include task-specific focus and sensitivity to distribution shifts, with future work targeting preference data and context adaptation. |
| 114 | Reducing Tool Hallucination via Reliability Alignment | [ArXiv](http://arxiv.org/abs/2412.04141v2) | [PDF](./papers/2412.04141v2.pdf) | addresses tool hallucinations in LLMs, where models incorrectly select or misuse external tools, impacting task reliability and efficiency. It categorizes hallucinations into tool selection (type, timing) and usage (format, content) errors, introducing an automated evaluation framework with 92.7% accuracy. The RelyToolBench benchmark, with subsets for missing parameters and unmatched tools, assesses hallucination impacts. New metrics, RePR and Benefit-Cost Utility, quantify reliability and efficiency. The Relign framework, incorporating an indecisive action space (e.g., ChangeTools, TalkToUser), reduces hallucinations via SFT and DPO training. Experiments on LLaMA-3.1 and Qwen-2.5 show Relign lowers hallucination rates, improves task success, and outperforms baselines, approaching GPT-4o performance. Limitations include dataset scope and task specificity, with future work targeting broader benchmarks and complex tool interactions. |
| 115 | Teaching Language Models to Critique via Reinforcement Learning | [ArXiv](http://arxiv.org/abs/2502.03492v1) | [PDF](./papers/2502.03492v1.pdf) | CTRL, a reinforcement learning framework for training LLM critics to provide actionable feedback for code generation. By decoupling the critic from the generator, CTRL uses a two-stage approach: supervised finetuning with execution-guided critique synthesis and Group Relative Policy Optimization (GRPO) to reduce variance. Evaluated on benchmarks like CodeContests and LiveCodeBench, CTRL achieves up to 106.1% relative Pass@1 improvements, generalizes to stronger generators (e.g., GPT-4o), and mitigates compounding errors. Despite higher timeout rates, it outperforms baselines, demonstrating weak-to-strong generalization and test-time scaling. |
| 116 | MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations | [ArXiv](http://arxiv.org/abs/2502.06453v2) | [PDF](./papers/2502.06453v2.pdf) | constructs MATH-P-Simple and MATH-P-Hard benchmarks by applying simple and hard perturbations to 279 level-5 MATH problems, testing LLMs' mathematical reasoning robustness. Significant performance drops on MATH-P-Hard (e.g., o1-mini -16.49%, GPT-4o -27.6%) reveal a novel memorization issue: models blindly apply learned problem-solving techniques without assessing their suitability. In-context learning with original problems improves performance but can mislead, exacerbating errors. The work highlights hard perturbations as a critical bottleneck for LLM reasoning and calls for future research to enhance generalization. |
| 117 | Unnatural Languages Are Not Bugs but Features for LLMs | [ArXiv](http://arxiv.org/abs/2503.01926v1) | [PDF](./papers/2503.01926v1.pdf) | proposes that unnatural languages—non-human-readable strings comprehensible to LLMs—are features, not bugs. It introduces a gradient-based search algorithm to generate unnatural strings, creating SynContextQA and SimGSM8K datasets to test LLMs' understanding, achieving 80.4% and 54% accuracy, respectively. Models fine-tuned on unnatural LIMA instructions perform comparably to natural ones (48.82% winrate on LC AlpacaEval 2.0). LLMs process unnatural languages by filtering noise and inferring keyword organization, with transferable features across models and tasks. Limitations include search efficiency and partial natural token retention. |
| 118 | DPO Meets PPO: Reinforced Token Optimization for RLHF | [ArXiv](http://arxiv.org/abs/2404.18922v4) | [PDF](./papers/2404.18922v4.pdf) | Reinforced Token Optimization (RTO), a novel RLHF framework modeling LLMs as a token-wise MDP, combining DPO's implicit token-level rewards with PPO optimization. Theoretically, RTO achieves near-optimal policies with sample complexity $A^{\min\{\xi+1, H\}}$, far superior to the $A^H$ of sentence-level bandit formulations. Practically, RTO outperforms PPO by 7.5 points on AlpacaEval 2 and 4.1 points on Arena-Hard, showing robust performance in dialogue and summarization tasks. Limitations include linear reward assumptions and high computational costs. |
| 119 | Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning | [ArXiv](http://arxiv.org/abs/2501.15602v2) | [PDF](./papers/2501.15602v2.pdf) | analyzes external slow-thinking in LLMs, linking snowball errors to reasoning error probability via information theory. It quantifies error accumulation, showing mutual information decays exponentially with reasoning length. External slow-thinking mitigates errors by expanding reasoning space, but effectiveness depends on reward function reliability and total reasoning cost. Comparing BoN and MCTS, BoN achieves comparable accuracy with similar costs, suggesting framework design is secondary. Empirical results on GSM8k and PrOntoQA validate the framework, with code open-sourced. |
| 120 | MM-RLHF: The Next Step Forward in Multimodal LLM Alignment | [ArXiv](http://arxiv.org/abs/2502.10391v1) | [PDF](./papers/2502.10391v1.pdf) | MM-RLHF, a 120,000-pair multimodal dataset with fine-grained human annotations, advancing MLLM alignment with human preferences. It proposes a Critique-Based Reward Model, enhancing interpretability by generating critiques before scoring, and MM-DPO, a DPO variant with dynamic reward scaling for efficient training. Evaluations on MM-RLHF-RewardBench and MM-RLHF-SafetyBench show significant improvements, with 11% gains in conversational ability and 57% reduction in unsafe behavior. The work offers a robust framework for multimodal alignment, with open-sourced resources. |
| 121 | STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings | [ArXiv](http://arxiv.org/abs/2504.13416v1) | [PDF](./papers/2504.13416v1.pdf) | STAMP, a framework for detecting dataset membership in LLM pretraining corpora using watermarked rephrasings. It generates public and private watermarked versions of content, then applies a paired t-test on perplexity differences to prove membership. STAMP outperforms baselines (p-values $10^{-12}$ to $10^{-3}$) on benchmarks (TriviaQA, ARC-C, MMLU, GSM8K and real-world cases (paper abstracts, blogs), detecting contamination at <0.001% of the corpus. It preserves semantic similarity (P-SP 0.83-0.95) and benchmark utility, offering robust statistical guarantees with no false positives. Code is available at https://github.com/codecode/STAMP. |
