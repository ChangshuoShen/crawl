# ICML 2025 Safety Related Papers

| Title | ArXiv Link | PDF Link |
|-------|------------|----------|
| An Efficient Private GPT Never Autoregressively Decodes | [ArXiv](http://arxiv.org/abs/2505.15252v1) | [PDF](./papers/2505.15252v1.pdf) |
| ROPO: Robust Preference Optimization for Large Language Models | [ArXiv](http://arxiv.org/abs/2404.04102v2) | [PDF](./papers/2404.04102v2.pdf) |
| Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales | [ArXiv](http://arxiv.org/abs/2405.17618v2) | [PDF](./papers/2405.17618v2.pdf) |
| RE-Bench: Evaluating Frontier AI R&D Capabilities of Language Model Agents against Human Experts | [ArXiv](http://arxiv.org/abs/2411.15114v1) | [PDF](./papers/2411.15114v1.pdf) |
| Unbiased Evaluation of Large Language Models from a Causal Perspective | [ArXiv](http://arxiv.org/abs/2502.06655v2) | [PDF](./papers/2502.06655v2.pdf) |
| Scaling Trends in Language Model Robustness | [ArXiv](http://arxiv.org/abs/2407.18213v4) | [PDF](./papers/2407.18213v4.pdf) |
| Textual Unlearning Gives a False Sense of Unlearning | [ArXiv](http://arxiv.org/abs/2406.13348v2) | [PDF](./papers/2406.13348v2.pdf) |
| BaxBench: Can LLMs Generate Correct and Secure Backends? | [ArXiv](http://arxiv.org/abs/2502.11844v2) | [PDF](./papers/2502.11844v2.pdf) |
| When Bad Data Leads to Good Models | [ArXiv](http://arxiv.org/abs/2505.04741v1) | [PDF](./papers/2505.04741v1.pdf) |
| TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation | [ArXiv](http://arxiv.org/abs/2504.18535v1) | [PDF](./papers/2504.18535v1.pdf) |
| Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection | [ArXiv](http://arxiv.org/abs/2411.01077v2) | [PDF](./papers/2411.01077v2.pdf) |
| SEMU: Singular Value Decomposition for Efficient Machine Unlearning | [ArXiv](http://arxiv.org/abs/2502.07587v1) | [PDF](./papers/2502.07587v1.pdf) |
| How Much Can We Forget about Data Contamination? | [ArXiv](http://arxiv.org/abs/2410.03249v3) | [PDF](./papers/2410.03249v3.pdf) |
| Observation Interference in Partially Observable Assistance Games | [ArXiv](http://arxiv.org/abs/2412.17797v1) | [PDF](./papers/2412.17797v1.pdf) |
| Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models | [ArXiv](http://arxiv.org/abs/2405.03869v5) | [PDF](./papers/2405.03869v5.pdf) |
| LLM Alignment as Retriever Optimization: An Information Retrieval Perspective | [ArXiv](http://arxiv.org/abs/2502.03699v1) | [PDF](./papers/2502.03699v1.pdf) |
| RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals | [ArXiv](http://arxiv.org/abs/2410.11348v3) | [PDF](./papers/2410.11348v3.pdf) |
| AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs | [ArXiv](http://arxiv.org/abs/2404.16873v1) | [PDF](./papers/2404.16873v1.pdf) |
| Scalably Solving Assistance Games | [ArXiv](http://arxiv.org/abs/2504.07091v1) | [PDF](./papers/2504.07091v1.pdf) |
| HPS: Hard Preference Sampling for Human Preference Alignment | [ArXiv](http://arxiv.org/abs/2502.14400v1) | [PDF](./papers/2502.14400v1.pdf) |
| Adversaries Can Misuse Combinations of Safe Models | [ArXiv](http://arxiv.org/abs/2406.14595v2) | [PDF](./papers/2406.14595v2.pdf) |
| Topological Signatures of Adversaries in Multimodal Alignments | [ArXiv](http://arxiv.org/abs/2501.18006v1) | [PDF](./papers/2501.18006v1.pdf) |
| The Ripple Effect: On Unforeseen Complications of Backdoor Attacks | [ArXiv](http://arxiv.org/abs/2505.11586v1) | [PDF](./papers/2505.11586v1.pdf) |
| Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback | [ArXiv](http://arxiv.org/abs/2404.10776v2) | [PDF](./papers/2404.10776v2.pdf) |
| AMPO: Active Multi Preference Optimization | [ArXiv](http://arxiv.org/abs/2502.18293v1) | [PDF](./papers/2502.18293v1.pdf) |
| ProSec: Fortifying Code LLMs with Proactive Security Alignment | [ArXiv](http://arxiv.org/abs/2411.12882v2) | [PDF](./papers/2411.12882v2.pdf) |
| De-mark: Watermark Removal in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.13808v1) | [PDF](./papers/2410.13808v1.pdf) |
| Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing | [ArXiv](http://arxiv.org/abs/2406.14230v3) | [PDF](./papers/2406.14230v3.pdf) |
| Larger or Smaller Reward Margins to Select Preferences for Alignment? | [ArXiv](http://arxiv.org/abs/2503.01864v1) | [PDF](./papers/2503.01864v1.pdf) |
| Quantifying perturbation impacts for large language models | [ArXiv](http://arxiv.org/abs/2412.00868v1) | [PDF](./papers/2412.00868v1.pdf) |
| Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes | [ArXiv](http://arxiv.org/abs/2505.04993v1) | [PDF](./papers/2505.04993v1.pdf) |
| How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence | [ArXiv](http://arxiv.org/abs/2502.00678v2) | [PDF](./papers/2502.00678v2.pdf) |
| The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret | [ArXiv](http://arxiv.org/abs/2406.15753v2) | [PDF](./papers/2406.15753v2.pdf) |
| Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing | [ArXiv](http://arxiv.org/abs/2410.17194v3) | [PDF](./papers/2410.17194v3.pdf) |
| Preference learning made easy: Everything should be understood through win rate | [ArXiv](http://arxiv.org/abs/2502.10505v1) | [PDF](./papers/2502.10505v1.pdf) |
| GaussMark: A Practical Approach for Structural Watermarking of Language Models | [ArXiv](http://arxiv.org/abs/2501.13941v1) | [PDF](./papers/2501.13941v1.pdf) |
| Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization | [ArXiv](http://arxiv.org/abs/2410.12949v2) | [PDF](./papers/2410.12949v2.pdf) |
| SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning | [ArXiv](http://arxiv.org/abs/2505.02486v1) | [PDF](./papers/2505.02486v1.pdf) |
| Adversarial Reasoning at Jailbreaking Time | [ArXiv](http://arxiv.org/abs/2502.01633v1) | [PDF](./papers/2502.01633v1.pdf) |
| SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders | [ArXiv](http://arxiv.org/abs/2501.18052v3) | [PDF](./papers/2501.18052v3.pdf) |
| Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs | [ArXiv](http://arxiv.org/abs/2505.02862v1) | [PDF](./papers/2505.02862v1.pdf) |
| Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions | [ArXiv](http://arxiv.org/abs/2502.04322v1) | [PDF](./papers/2502.04322v1.pdf) |
| Position: Editing Large Language Models Poses Serious Safety Risks | [ArXiv](http://arxiv.org/abs/2502.02958v1) | [PDF](./papers/2502.02958v1.pdf) |
| A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language? | [ArXiv](http://arxiv.org/abs/2502.14924v1) | [PDF](./papers/2502.14924v1.pdf) |
| Analyze Feature Flow to Enhance Interpretation and Steering in Language Models | [ArXiv](http://arxiv.org/abs/2502.03032v2) | [PDF](./papers/2502.03032v2.pdf) |
| Activation Space Interventions Can Be Transferred Between Large Language Models | [ArXiv](http://arxiv.org/abs/2503.04429v2) | [PDF](./papers/2503.04429v2.pdf) |
| Independence Tests for Language Models | [ArXiv](http://arxiv.org/abs/2502.12292v2) | [PDF](./papers/2502.12292v2.pdf) |
| Selective Response Strategies for GenAI | [ArXiv](http://arxiv.org/abs/2502.00729v1) | [PDF](./papers/2502.00729v1.pdf) |
| Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints | [ArXiv](http://arxiv.org/abs/2503.01747v2) | [PDF](./papers/2503.01747v2.pdf) |
| Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs | [ArXiv](http://arxiv.org/abs/2502.17424v6) | [PDF](./papers/2502.17424v6.pdf) |
| TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference | [ArXiv](http://arxiv.org/abs/2501.16007v1) | [PDF](./papers/2501.16007v1.pdf) |
| Language Models May Verbatim Complete Text They Were Not Explicitly Trained On | [ArXiv](http://arxiv.org/abs/2503.17514v2) | [PDF](./papers/2503.17514v2.pdf) |
| Blink of an eye: a simple theory for feature localization in generative models | [ArXiv](http://arxiv.org/abs/2502.00921v1) | [PDF](./papers/2502.00921v1.pdf) |
| PILAF: Optimal Human Preference Sampling for Reward Modeling | [ArXiv](http://arxiv.org/abs/2502.04270v1) | [PDF](./papers/2502.04270v1.pdf) |
| Understanding the Logic of Direct Preference Alignment through Logic | [ArXiv](http://arxiv.org/abs/2412.17696v2) | [PDF](./papers/2412.17696v2.pdf) |
| The Elicitation Game: Evaluating Capability Elicitation Techniques | [ArXiv](http://arxiv.org/abs/2502.02180v2) | [PDF](./papers/2502.02180v2.pdf) |
| Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models | [ArXiv](http://arxiv.org/abs/2505.07558v2) | [PDF](./papers/2505.07558v2.pdf) |
| Interactive Tools Substantially Assist LM Agents in Finding Security Vulnerabilities | [ArXiv](http://arxiv.org/abs/2409.16165v2) | [PDF](./papers/2409.16165v2.pdf) |
| On the Robustness of Reward Models for Language Model Alignment | [ArXiv](http://arxiv.org/abs/2505.07271v1) | [PDF](./papers/2505.07271v1.pdf) |
| Discovering Spoofing Attempts on Language Model Watermarks | [ArXiv](http://arxiv.org/abs/2410.02693v2) | [PDF](./papers/2410.02693v2.pdf) |
| Focus On This, Not That! Steering LLMs With Adaptive Feature Specification | [ArXiv](http://arxiv.org/abs/2410.22944v3) | [PDF](./papers/2410.22944v3.pdf) |
| Selective Prompt Anchoring for Code Generation | [ArXiv](http://arxiv.org/abs/2408.09121v4) | [PDF](./papers/2408.09121v4.pdf) |
| Improving Your Model Ranking on Chatbot Arena by Vote Rigging | [ArXiv](http://arxiv.org/abs/2501.17858v1) | [PDF](./papers/2501.17858v1.pdf) |
| Probabilistic Verification of Neural Networks using Branch and Bound | [ArXiv](http://arxiv.org/abs/2405.17556v2) | [PDF](./papers/2405.17556v2.pdf) |
| Optimizing Adaptive Attacks against Watermarks for Language Models | [ArXiv](http://arxiv.org/abs/2410.02440v2) | [PDF](./papers/2410.02440v2.pdf) |
| SPEX: Scaling Feature Interaction Explanations for LLMs | [ArXiv](http://arxiv.org/abs/2502.13870v1) | [PDF](./papers/2502.13870v1.pdf) |
| Demystifying Singular Defects in Large Language Models | [ArXiv](http://arxiv.org/abs/2502.07004v1) | [PDF](./papers/2502.07004v1.pdf) |
| AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses | [ArXiv](http://arxiv.org/abs/2503.01811v1) | [PDF](./papers/2503.01811v1.pdf) |
| Automated Red Teaming with GOAT: the Generative Offensive Agent Tester | [ArXiv](http://arxiv.org/abs/2410.01606v1) | [PDF](./papers/2410.01606v1.pdf) |
| REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective | [ArXiv](http://arxiv.org/abs/2502.17254v1) | [PDF](./papers/2502.17254v1.pdf) |
| The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence | [ArXiv](http://arxiv.org/abs/2502.17420v1) | [PDF](./papers/2502.17420v1.pdf) |
| RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning | [ArXiv](http://arxiv.org/abs/2410.02089v2) | [PDF](./papers/2410.02089v2.pdf) |
| On Teacher Hacking in Language Model Distillation | [ArXiv](http://arxiv.org/abs/2502.02671v1) | [PDF](./papers/2502.02671v1.pdf) |
| Controlling Large Language Model with Latent Action | [ArXiv](http://arxiv.org/abs/2503.21383v1) | [PDF](./papers/2503.21383v1.pdf) |
| GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation | [ArXiv](http://arxiv.org/abs/2410.08475v2) | [PDF](./papers/2410.08475v2.pdf) |
| AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders | [ArXiv](http://arxiv.org/abs/2501.17148v3) | [PDF](./papers/2501.17148v3.pdf) |
| CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization | [ArXiv](http://arxiv.org/abs/2411.12768v1) | [PDF](./papers/2411.12768v1.pdf) |
| POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization | [ArXiv](http://arxiv.org/abs/2410.12999v1) | [PDF](./papers/2410.12999v1.pdf) |
| SafeArena: Evaluating the Safety of Autonomous Web Agents | [ArXiv](http://arxiv.org/abs/2503.04957v1) | [PDF](./papers/2503.04957v1.pdf) |
| Eliciting Language Model Behaviors with Investigator Agents | [ArXiv](http://arxiv.org/abs/2502.01236v1) | [PDF](./papers/2502.01236v1.pdf) |
| Auditing Prompt Caching in Language Model APIs | [ArXiv](http://arxiv.org/abs/2502.07776v1) | [PDF](./papers/2502.07776v1.pdf) |
| Progressively Label Enhancement for Large Language Model Alignment | [ArXiv](http://arxiv.org/abs/2408.02599v2) | [PDF](./papers/2408.02599v2.pdf) |
| Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning | [ArXiv](http://arxiv.org/abs/2412.08559v2) | [PDF](./papers/2412.08559v2.pdf) |
| Emergent Response Planning in LLM | [ArXiv](http://arxiv.org/abs/2502.06258v1) | [PDF](./papers/2502.06258v1.pdf) |
| SAE-V: Interpreting Multimodal Models for Enhanced Alignment | [ArXiv](http://arxiv.org/abs/2502.17514v1) | [PDF](./papers/2502.17514v1.pdf) |
| The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models | [ArXiv](http://arxiv.org/abs/2503.03122v4) | [PDF](./papers/2503.03122v4.pdf) |
| SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior | [ArXiv](http://arxiv.org/abs/2410.16665v2) | [PDF](./papers/2410.16665v2.pdf) |
| STAIR: Improving Safety Alignment with Introspective Reasoning | [ArXiv](http://arxiv.org/abs/2502.02384v1) | [PDF](./papers/2502.02384v1.pdf) |
| The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them) | [ArXiv](http://arxiv.org/abs/2505.00626v2) | [PDF](./papers/2505.00626v2.pdf) |
| The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination | [ArXiv](http://arxiv.org/abs/2503.16402v1) | [PDF](./papers/2503.16402v1.pdf) |
| Automated Hypothesis Validation with Agentic Sequential Falsifications | [ArXiv](http://arxiv.org/abs/2502.09858v1) | [PDF](./papers/2502.09858v1.pdf) |
| TruthFlow: Truthful LLM Generation via Representation Flow Correction | [ArXiv](http://arxiv.org/abs/2502.04556v1) | [PDF](./papers/2502.04556v1.pdf) |
| Learning to Route LLM with Confidence Tokens | [ArXiv](http://arxiv.org/abs/2410.13284v2) | [PDF](./papers/2410.13284v2.pdf) |
| FlipAttack: Jailbreak LLMs via Flipping | [ArXiv](http://arxiv.org/abs/2410.02832v1) | [PDF](./papers/2410.02832v1.pdf) |
| Generalized Interpolating Discrete Diffusion | [ArXiv](http://arxiv.org/abs/2503.04482v1) | [PDF](./papers/2503.04482v1.pdf) |
| Reward Modeling with Ordinal Feedback: Wisdom of the Crowd | [ArXiv](http://arxiv.org/abs/2411.12843v1) | [PDF](./papers/2411.12843v1.pdf) |
| Persistent Topological Features in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.11042v1) | [PDF](./papers/2410.11042v1.pdf) |
| Scaling Laws for Differentially Private Language Models | [ArXiv](http://arxiv.org/abs/2501.18914v1) | [PDF](./papers/2501.18914v1.pdf) |
| Position: Theory of Mind Benchmarks are Broken for Large Language Models | [ArXiv](http://arxiv.org/abs/2412.19726v2) | [PDF](./papers/2412.19726v2.pdf) |
| Empirical Privacy Variance | [ArXiv](http://arxiv.org/abs/2503.12314v1) | [PDF](./papers/2503.12314v1.pdf) |
| Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing | [ArXiv](http://arxiv.org/abs/2502.00602v1) | [PDF](./papers/2502.00602v1.pdf) |
| Understanding the Limits of Lifelong Knowledge Editing in LLMs | [ArXiv](http://arxiv.org/abs/2503.05683v1) | [PDF](./papers/2503.05683v1.pdf) |
| Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment | [ArXiv](http://arxiv.org/abs/2502.04040v1) | [PDF](./papers/2502.04040v1.pdf) |
| Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models | [ArXiv](http://arxiv.org/abs/2410.02205v3) | [PDF](./papers/2410.02205v3.pdf) |
| LLMScan: Causal Scan for LLM Misbehavior Detection | [ArXiv](http://arxiv.org/abs/2410.16638v3) | [PDF](./papers/2410.16638v3.pdf) |
| AnyEdit: Edit Any Knowledge Encoded in Language Models | [ArXiv](http://arxiv.org/abs/2502.05628v2) | [PDF](./papers/2502.05628v2.pdf) |
| The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking | [ArXiv](http://arxiv.org/abs/2501.19358v2) | [PDF](./papers/2501.19358v2.pdf) |
| When Can Proxies Improve the Sample Complexity of Preference Learning? | [ArXiv](http://arxiv.org/abs/2412.16475v1) | [PDF](./papers/2412.16475v1.pdf) |
| Diverging Preferences: When do Annotators Disagree and do Models Know? | [ArXiv](http://arxiv.org/abs/2410.14632v2) | [PDF](./papers/2410.14632v2.pdf) |
| Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples | [ArXiv](http://arxiv.org/abs/2502.09650v2) | [PDF](./papers/2502.09650v2.pdf) |
| BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning | [ArXiv](http://arxiv.org/abs/2501.18858v1) | [PDF](./papers/2501.18858v1.pdf) |
| Reinforced Lifelong Editing for Language Models | [ArXiv](http://arxiv.org/abs/2502.05759v3) | [PDF](./papers/2502.05759v3.pdf) |
| Conformal Tail Risk Control for Large Language Model Alignment | [ArXiv](http://arxiv.org/abs/2502.20285v1) | [PDF](./papers/2502.20285v1.pdf) |
| Reducing Tool Hallucination via Reliability Alignment | [ArXiv](http://arxiv.org/abs/2412.04141v2) | [PDF](./papers/2412.04141v2.pdf) |
| Teaching Language Models to Critique via Reinforcement Learning | [ArXiv](http://arxiv.org/abs/2502.03492v1) | [PDF](./papers/2502.03492v1.pdf) |
| MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations | [ArXiv](http://arxiv.org/abs/2502.06453v2) | [PDF](./papers/2502.06453v2.pdf) |
| Unnatural Languages Are Not Bugs but Features for LLMs | [ArXiv](http://arxiv.org/abs/2503.01926v1) | [PDF](./papers/2503.01926v1.pdf) |
| DPO Meets PPO: Reinforced Token Optimization for RLHF | [ArXiv](http://arxiv.org/abs/2404.18922v4) | [PDF](./papers/2404.18922v4.pdf) |
| Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning | [ArXiv](http://arxiv.org/abs/2501.15602v2) | [PDF](./papers/2501.15602v2.pdf) |
| MM-RLHF: The Next Step Forward in Multimodal LLM Alignment | [ArXiv](http://arxiv.org/abs/2502.10391v1) | [PDF](./papers/2502.10391v1.pdf) |
| STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings | [ArXiv](http://arxiv.org/abs/2504.13416v1) | [PDF](./papers/2504.13416v1.pdf) |
